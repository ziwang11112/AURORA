Index,Citation,Summary
1,"[Kuo-Kai Lee, Wing-Kai Hon, Chung-Shou Liao, Kunihiko Sadakane, and Meng-Tsung Tsai,  ""Fully Dynamic No-Back-Edge-Traversal Forest via 2D-Range Queries,""  International Journal of Computational Geometry & Applications,  vol. 33, no. 1&2, pp. 43–54, 2023.  Available: https://dblp.org/rec/journals/ijcga/LeeHLST23]","Orthogonal range search, widely used in applications such as databases, data mining, and text indexing, has recently found use in maintaining a DFS forest in dynamic graphs. This paper extends that line of work by applying orthogonal range search to efficiently maintain a BFS-like no-back-edge-traversal (NBET) forest. An NBET forest, which arises from a traversal avoiding the creation of back edges, serves as a robust certificate for the 2-connectivity of undirected graphsa property that is more general than what is established by scan-first search spanning forests."
2,"[Gregory Bint, Anil Maheshwari, Michiel H. M. Smid, and Subhas C. Nandy,  ""Partial Enclosure Range Searching,""  International Journal of Computational Geometry & Applications,  vol. 29, no. 1, pp. 73–93, 2019.  Available: https://dblp.org/rec/journals/ijcga/BintMSN19]","This paper introduces the partial enclosure range searching problem, a generalization of classical range searching where, given a set of geometric objects (points, line segments, rectangles) in the plane and a query axis-aligned rectangle $Q$ with an integer parameter $k$ or fraction $\alpha$, the goal is to preprocess the set so that all objects partially enclosed by $Q$having at least $k$ points or $\alpha$-fraction of length/area inside $Q$can be efficiently reported. Motivated by applications in graphics and spatial databases, the authors define several variants of this problem and develop efficient data structures based on range, segment, and interval trees, adapting and extending these structures for partial, measure-theoretic enclosure. For points, existing range structures suffice, while segments and rectangles require more advanced combination of endpoint queries, segment intersection detection, and measure-based decomposition. Their results, which achieve fast polylogarithmic query times per object after near-linear preprocessing and space, generalize existing range searching techniques and demonstrate the tractability of partial enclosure queries. Key challenges addressed include handling the complexity of partial rather than total containment and unifying different range searching paradigms. The study lays groundwork for further research into broader classes of objects, higher-dimensional queries, dynamic updates, and approximate or probabilistic methods in geometric data analysis."
3,"[Haitao Wang and Wuzhou Zhang,  ""On Top-k Weighted Sum Aggregate Nearest and Farthest Neighbors in the L1 Plane,""  International Journal of Computational Geometry & Applications,  vol. 29, no. 3, pp. 189–218, 2019.  Available: https://dblp.org/rec/journals/ijcga/0001Z19]","In this paper, the authors investigate top-$k$ aggregate nearest neighbor queries using the weighted SUM operator under the $L_1$ (Manhattan) metric in the plane. Given a dataset $P$ of $n$ points and a query set $Q$ of $m$ weighted points, the goal is to find the $k$ points in $P$ with the smallest aggregate (weighted) $L_1$ distance to $Q$, where the aggregate distance for $p \in P$ is the sum of weighted distances from $p$ to all $q \in Q$. The authors design a data structure of size $O(n\log n \log\log n)$ that can be built in $O(n\log n \log\log n)$ time, supporting each top-$k$ query in $O(m\log m + (k+m)\log^2 n)$ time. Trade-offs between preprocessing and querying are also presented. Notably, even for $k=1$, their method improves upon previous results by reducing preprocessing space and query time (previously $O(n\log^2 n)$ space and $O(m^2\log^3 n)$ query, now improved). They also address the one-dimensional case with an $O(n)$-size structure supporting $O(\min\{k,\log m\}\cdot m + k + \log n)$-time queries, and extend their approach to top-$k$ aggregate farthest neighbor queries under similar performance bounds."
4,"[R. H. Güting, S. K. Das, F. Valdés, and S. Ray, ""Exact Trajectory Similarity Search With N-tree: An Efficient Metric Index for kNN and Range Queries,"" ACM Transactions on Spatial Algorithms and Systems, vol. 11, no. 1, pp. 5:1–5:54, 2025. [Online]. Available: https://dblp.org/rec/journals/tsas/GutingDVR25]","Similarity search involves identifying objects from a collection that are similar to a given query and is fundamental in applications spanning from text documents to moving object trajectories. Motivated by trajectory analysisrecorded paths of entities like vehicles or humansthis paper introduces a novel metric distance function, DistanceAvg, designed to capture movement similarity and to permit indexing for efficient search. Leveraging the metric space framework (where objects are organized solely by a distance function), the authors note that while traditional metric indexes exist, they are suboptimal for trajectory data. The proposed index improves performance over existing methods for $k$-nearest neighbor ($k$NN) queries and matches the best for range queries, especially for computationally expensive distance measures typical in trajectory analysis. The index also provides exact answers for the specified distance, making $k$NN queries practical when choosing an appropriate search radius is difficult."
5,"[H. Wang and Q. Zeng, ""Unit-disk range searching and applications,"" Journal of Computational Geometry, vol. 12, no. 1, pp. 381-417, 2021. [Online]. Available: https://jocg.org/index.php/jocg/article/view/4015]","This paper focuses on efficient algorithms and data structures for unit-disk range counting, where the goal is to preprocess a set $P$ of $n$ points in the plane so that, given any unit disk, one can quickly count how many points of $P$ it contains. By adapting simplex range searching techniques to exploit the uniformity of unit-disk queries, the authors construct data structures with $O(n)$ space and $O(\sqrt{n})$ query time, improving upon prior results. Core methods include partitioning the plane into grid cells of length $\Theta(1)$, associating points to cells, and constructing pseudo-trapezoidal partitions and hierarchical cuttings for disk arrangements. These foundations yield improvements for several geometric problems: batched range counting for $n$ unit disks and $n$ points is solved in $O(n^{4/3})$ time (improving the previous $O(n^{4/3}\log n)$ bound), intersecting pairs of circles are counted in $O(n^{4/3})$ time, the $k$-th smallest distance among $P$ is found in $O(n\log n + n^{2/3}k^{1/3}\log n)$ expected time, and the discrete $2$-center problem admits subquadratic deterministic and randomized solutions. Besides algorithmic advances, the work introduces new structural insights for unit disk arrangements and suggests open questions in dynamic settings and higher dimensions, laying a foundation for further advances in uniform-range geometric searching."
6,"[S. Rahul, ""Approximate range counting revisited,"" Journal of Computational Geometry, vol. 12, no. 1, pp. 183-212, 2021. [Online]. Available: https://jocg.org/index.php/jocg/article/view/3153]","This paper investigates range-searching for colored objects, focusing on approximately counting the number of distinct colors within a query range, primarily involving orthogonal range queries in two and three dimensions and rectangle stabbing by points. The authors provide optimal and near-optimal algorithms for these tasks, mainly achieved through reductions to the uncolored version of the problems and the development of improved data structures. A notable contribution is the introduction of the ""nested shallow cuttings"" technique."
7,"[J. Xue and H. Wang, ""Searching for the closest-pair in a query translate,"" Journal of Computational Geometry, vol. 11, no. 1, pp. 104–139, 2020. [Online]. Available: https://jocg.org/index.php/jocg/article/view/3118]","We consider a range-search variant of the closest-pair problem, where for a fixed shape $\varGamma$ in the plane, the goal is to preprocess $n$ points so that for any translate of $\varGamma$, the closest pair of contained points can be efficiently reported. For the case where $\varGamma$ is a polygon (possibly with holes), we present a data structure using $O(n)$ space and $O(\log n)$ query time, achieving asymptotically optimal bounds. When $\varGamma$ is a general convex body with a smooth boundary, we provide a near-optimal data structure requiring $O(n \log n)$ space and $O(\log^2 n)$ query time. These contributions resolve certain open questions from Xue et al. [SoCG 2018]."
8,"[H. Yepdjio Nkouanga and S. Vajda, ""Optimization Strategies for the k-Nearest Neighbor Classifier,"" SN Computer Science, vol. 4, no. 47, 2023. [Online]. Available: https://doi.org/10.1007/s42979-022-01469-3]","This paper introduces six fast and efficient schemes to accelerate k-nearest neighbor (kNN) classification for various image types by either reducing the training set or compressing data dimensionality. Prototype reduction is achieved through convex hull selectionwhere only the boundary points of each class are retained as representativesand stratified sampling, both supported by dimensionality reduction techniques such as Principal Component Analysis (PCA) and Feature Agglomeration (FA) to handle high-dimensional data. Additionally, data is re-represented via auto-encoder neural networks, which learn compact encodings, or by embedding samples into a dissimilarity space using centroids from k-means clustering as prototypes. Experiments on MNIST, Fashion-MNIST, and Lampung character datasets demonstrate that these methods deliver classification speed-ups up to $32\times$, often without sacrificing, and sometimes even improving, accuracyfor example, combining encoder and embedding yielded a +0.72% accuracy gain and $23.7\times$ speedup on MNIST. While convex hull and stratified sampling achieved the greatest acceleration, they could reduce accuracy on complex or overlapping data. In contrast, auto-encoder-based and embedding approaches afforded robust, highly compact representations, providing the optimal balance of speed and performance. The results emphasize that a combination of prototype reduction and learned feature compression can dramatically accelerate kNN classification with minimal loss in predictive power."
9,"[N. G. Siddappa and T. Kampalappa, ""Imbalance Data Classification Using Local Mahalanobis Distance Learning Based on Nearest Neighbor,"" SN Computer Science, vol. 1, no. 76, 2020. [Online]. Available: https://doi.org/10.1007/s42979-020-0085-x]","This paper addresses the challenge of class imbalance in datasets, where standard machine learning algorithms often perform poorly due to the disproportionate representation of classes. The authors propose a method utilizing Local Mahalanobis Distance Learning (LMDL) with the nearest neighbor (NN) classifier, which preserves the original data distribution and minimizes information loss compared to traditional techniques like sampling or cost-sensitive learning. The LMDL approach adaptively learns Mahalanobis distance metrics for localized prototypes, enhancing discrimination between classes by optimizing an objective function derived from the NN error rate. Experimental analysis across varied UCI datasetsincluding Iris, breast cancer, and E-colidemonstrates superior classification results, with LMDL achieving up to 98% accuracy, high precision, recall, F-measure, and robust G-mean and AUC metrics (e.g., for Iris: $97\%$ accuracy, $98\%$ precision, $97\%$ recall and F-measure, $97.36\%$ G-mean, $90.62\%$ AUC). Comparative discussion shows LMDL outperforming methods such as Random Forest, SMOTE-based ensembles, and fuzzy adaptive K-NN, though it is less effective for highly nonlinear data and small sample sizes. The study concludes that LMDL effectively mitigates class imbalance issues while preserving critical data properties, and suggests extending the framework to feature-based and multiclass nonlinear classification in future work."
10,"[D. Obraczka and E. Rahm, ""Fast Hubness-Reduced Nearest Neighbor Search for Entity Alignment in Knowledge Graphs,"" SN Computer Science, vol. 3, issue 6, Oct. 2022. [Direct access unavailable—see journal issue: https://link.springer.com/journal/42979/volumes-and-issues/3-6]]","The paper addresses the challenge of representing heterogeneous entities and relations in knowledge graphs (KGs) for data integration, focusing on Knowledge Graph Embeddings (KGEs) which map these entities into a common low-dimensional space. However, KGEs are shown to suffer from the hubness phenomenon, where certain entities ('hubs') dominate nearest neighbor searchesan integral step in entity alignmentthus degrading alignment quality. The authors systematically investigate hubness reduction techniques and (approximate) nearest neighbor libraries, demonstrating that hubness-reduced nearest neighbor search can be conducted with little to no speed loss and significantly better alignment results. Statistical significance is validated via Bayesian analysis. For practical application and further research, an open-source Python library is provided at https://github.com/dobraczka/kiez."
11,"[Magdalen Dobson Manohar, Taekseung Kim, and Guy E. Blelloch, ""Range Retrieval with Graph-Based Indices,"" arXiv preprint arXiv:2502.13245, 2025. [Online]. Available: https://arxiv.org/abs/2502.13245]","Retrieving points within a specific radius in high-dimensional vector spacesknown as range retrievalis vital for applications such as duplicate detection, plagiarism checking, and facial recognition, but has received less focus compared to approximate nearest neighbor search (ANNS). This paper presents algorithms for efficient range retrieval on graph-based vector indices, enabling both rapid termination for queries with few or no results and efficient exploration for queries returning many matches. The proposed methods modify standard graph search with techniques like early-stopping heuristics and introduce two approachesgreedy search and doubling beam searchtailored to the query's match density and dataset characteristics. Comprehensive benchmarking was performed on nine datasets (including eight with up to 100 million points), showing up to 100x improvement in query throughput over naive adaptations, with typical 510x gains, especially as dataset size and density increase. Experimental analysis reveals that the optimal algorithm varies based on dataset properties, and that adaptive computation and early stopping are key to efficiency. This work establishes new benchmarks for range retrieval, identifies open challenges in early termination and benchmarking radius selection, and outlines future directions such as machine-learned heuristics and weighted range scoring. Overall, these innovations significantly advance scalable, high-performance range search on massive, real-world embedding datasets."
12,"[Anqi Liang, Pengcheng Zhang, Bin Yao, Zhongpu Chen, Yitong Song, and Guangxu Cheng, ""UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search,"" arXiv preprint arXiv:2412.02448, 2024. [Online]. Available: https://arxiv.org/abs/2412.02448]","This paper introduces UNIFY, an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors coupled with attribute values, addressing scenarios where queries require nearest neighbors within specified attribute ranges (e.g., date or price). Existing strategiespre-, post-, and hybrid filteringare hampered by performance drops as query ranges vary and face practical issues like index maintenance. UNIFY overcomes these via a unified proximity graph-based index that supports all three strategies. At its core, UNIFY proposes the Segmented Inclusive Graph (SIG), enabling efficient graph-based hybrid filtering by ensuring that the proximity graph for any attribute range segment is a subgraph of the global SIG. It further presents the Hierarchical Segmented Inclusive Graph (HSIG), introducing a hierarchical design inspired by HNSW, which embeds skip list and compressed HNSW structures for logarithmic time range filtering and quick, range-aware pre- and post-filtering. The framework automatically selects the optimal search strategy based on the query ranges cardinality versus dataset-derived thresholds. Extensive experiments on six large real-world datasets show that HSIG significantly outperforms prior approaches, achieving up to $2.29\times$ faster query speed, higher recall, and robust incremental insertion support, with scalability to hundreds of millions of objects. The researchers highlight broad applicability in real-world ANNS, database, and retrieval-augmented generation (RAG) systems, and suggest future extensions for multi-attribute constraints and adaptive strategy calibration."
13,"[Dimitris Fotakis, Andreas Kalavas, and Ioannis Psarros, ""A Query-Driven Approach to Space-Efficient Range Searching,"" arXiv preprint arXiv:2502.13653, 2025. [Online]. Available: https://arxiv.org/abs/2502.13653]","We initiate a study of a query-driven approach to designing partition trees for range-searching problems. Our model assumes that a data structure is to be built for an unknown query distribution that we can access through a sampling oracle, and must be selected such that it optimizes a meaningful performance parameter on expectation. Our first contribution is to show that a near-linear sample of queries allows the construction of a partition tree with a near-optimal expected number of nodes visited during querying. We enhance this approach by treating node processing as a classification problem, leveraging fast classifiers like shallow neural networks to obtain experimentally efficient query times. Our second contribution is to develop partition trees using sparse geometric separators. Our preprocessing algorithm, based on a sample of queries, builds a balanced tree with nodes associated with separators that minimize query stabs on expectation; this yields both fast processing of each node and a small number of visited nodes, significantly reducing query time."
14,"[G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B. Micenková, E. Schubert, I. Assent, and M. E. Houle, ""On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study,"" Data Mining and Knowledge Discovery, vol. 30, no. 4, pp. 891–927, 2016. [Online]. Available: https://doi.org/10.1007/s10618-015-0444-8]","This paper delivers an extensive experimental evaluation of unsupervised outlier detection algorithms, focusing primarily on a representative suite of k nearest neighbor (kNN)-based models (including kNN, kNN-weight, LOF, SimplifiedLOF, and others) across nearly 1000 processed datasets with thorough consideration of preprocessing (such as normalization, duplicate handling, and downsampling). The study demonstrates that the scarcity and inconsistent preparation of benchmark datasets, along with limited understanding and bias in popular evaluation metrics (notably precision at $n$, average precision, and ROC AUC), hinder fair comparative assessment; thus, the authors propose adjusted metrics for P@$n$ and AP to facilitate chance-corrected benchmarking. Their systematic experiments reveal that classic methodskNN, kNNW, and LOFremain robustly competitive, with no recent methods showing universal improvements, and that claims of algorithmic supremacy based on selective parameterization and restricted datasets are largely unsupported. Key challenges discussed include sensitivity to the neighborhood parameter $k$, instability from dataset creation practices (especially downsampling), and the urgent need for a transparent, well-documented repository of benchmark datasets. The conclusion calls for comprehensive evaluation protocols spanning diverse data and parameter settings, finding that performance claims for new methods should be tempered by these robust, multi-dimensional comparisons, and the authors provide a public web repository to promote reproducible, transparent research in outlier detection."
15,"[Yaru Chen, Jie Zhou, Xinglong Luo, ""An improved density peaks clustering based on sparrow search algorithm,"" Cluster Computing, vol. 27, no. 8, pp. 11017–11037, 2024. https://doi.org/10.1007/s10586-024-04384-9]","The paper proposes SSA-DPC, an improved Density Peaks Clustering (DPC) algorithm that addresses DPC's shortcomings of sensitivity to cut-off distance and poor fault tolerance in cluster assignment. SSA-DPC leverages the Sparrow Search Algorithm (SSA) to automatically and optimally determine the cut-off distance by maximizing an ACC index, removing the need for manual parameter tuning and reducing the impact of this value on results. The method also introduces mutual nearest neighbor concepts to divide the dataset into high- and low-density regions, assigning different cluster allocation strategies for each to enhance robustness. Experimental results on both synthetic and real-world datasets, evaluated via Adjusted Mutual Information (AMI), Adjusted Rand Index (ARI), and Fowlkes-Mallows Index (FMI), demonstrate that SSA-DPC achieves superior clustering accuracy compared to DPC, SNN-DPC, DBSCAN, K-means, KNN-DPC, and DPCSA methods, as shown in compact tabular results in the paper. Statistical tests, such as Friedman's test, confirm the significant improvements of SSA-DPC, and order sensitivity analysis highlights its stability. However, challenges remain, including pre-specification of parameter $k$ and manual setting of the number of clusters, as well as scalability for large or high-dimensional datasets. The authors suggest future work will target automatic parameter selection and applications combining SSA-DPC with superpixel techniques."
16,"[Luyao Chang, Fan Li, Xinzheng Niu, Jiahui Zhu, ""On an improved clustering algorithm based on node density for WSN routing protocol,"" Cluster Computing, vol. 25, no. 4, pp. 3005–3017, 2022. https://doi.org/10.1007/s10586-022-03544-z]","This paper presents the MKN-DPC algorithm for clustering in wireless sensor networks (WSN), addressing flaws in traditional protocols like LEACH and classical density peak clustering (DPC). Unlike fixed global parameters or random cluster head selection, MKN-DPC assigns an adaptive, local cut-off distance $d_c$ for each node using the mutual $k$-nearest neighbor approach, then computes the objective function $\gamma_i = \delta_i \cdot \rho_i$ (density and distance, both normalized) to automatically select cluster heads. Boundary nodes within the global mean $d_c$ are merged with a checking mechanism to prevent excessive merging. Experiments on 10 artificial and 6 real datasets (UCI: Iris, Seed, Glass, Haberman, Hayes-roth, WDbc), evaluated with Purity, NMI, and Rand Index (all in $[0,1]$), show that MKN-DPC outperforms DBSCAN, K-means, DPC, and DPC-DBFN, particularly excelling on datasets with uneven densities and nested clusters. The proposed method offers enhanced clustering quality and robustness crucial for balancing energy consumption and prolonging system lifecycle in WSNs, though it incurs higher computational complexity when selecting adaptive parameters, a limitation noted for future research."
17,"[Zainab Iftikhar, Adeel Anjum, Abid Khan, Munam Ali Shah, Gwanggil Jeon, ""Privacy preservation in the internet of vehicles using local differential privacy and IOTA ledger,"" Cluster Computing, vol. 26, pp. 3361–3377, 2023. https://doi.org/10.1007/s10586-023-04002-0]","This paper addresses privacy preservation in Vehicular Ad Hoc Networks (VANETs), where vehicle devices communicate extensively, generating vast sensitive data. Traditional privacy solutions often rely on trusted third parties for data anonymization, which poses security risks, especially with potentially untrusted edge nodes or roadside units (RSUs). The authors propose a two-tier model: Tier 1 applies Local Differential Privacy (LDP), allowing each vehicle to perturb its sensitive data with frequency oracle protocols (OLH, OUE, THE, HR, CMS) locallyensuring that no raw data is exposed during transmission. Tier 2 implements security via the IOTA distributed ledger, where RSUs concatenate, hash, and store data immutably, distributing computational/storage loads and providing quantum resistance. Experimental results confirm the schemes scalability and efficiency, maintaining sub-second computation at the vehicle-end and under-5-second processing at RSUs for up to 10,000 vehicles. Compared to group leader-based DP schemes (e.g., DPDS), this model achieves lower computation costs and higher accuracy as validated by reduced Mean Relative Error (MRE). The approach eliminates the need for a Trusted Third Party (TTP), mitigates Single-Point-of-Failure (SPoF) at RSUs, and is proven secure and lightweight. Future work aims to integrate machine learning for automatic frequency oracle and privacy budget $\epsilon$ selection. The proposed solution improves both privacy and utility for VANETs, leveraging local anonymization and decentralized quantum-safe storage."
18,"[A. Rachwał, A. Popławska, and M. Borys, ""Determining the Quality of a Dataset in Clustering Terms,"" Applied Sciences, vol. 13, no. 5, art. 2942, pp. 1–22, 2023. [Online]. Available: https://doi.org/10.3390/app13052942]","The paper introduces a novel methodology for handling mixed datasets comprising both categorical and continuous variables within customer segmentation tasks. Its primary focus is to provide instruments and methodologies for assessing the quality of such datasets specifically in the context of clustering applications, aiming to enhance data evaluation processes where mixed-type attributes are involved."
19,"[Y. Liu, J. Ding, H. Wang, and Y. Du, ""A Clustering Algorithm Based on the Detection of Density Peaks and the Interaction Degree Between Clusters,"" Applied Sciences, vol. 15, no. 7, art. 3612, pp. 1–19, 2025. [Online]. Available: https://doi.org/10.3390/app15073612]","In response to challenges posed by data with irregular shapes and uneven density, this paper introduces a two-phase clustering algorithm, CPDD-ID, which uses peaks in dimensional density curves and the degree of interaction between clusters to guide clustering. Initially, kernel density estimation is applied to compute local densities in all dimensions, constructing density curves and identifying their peaks as benchmarks for partitioning via a Kd-Tree, generating initial sub-clusters. The intersection of these results across all dimensions yields the final sub-clusters, enabling the method to effectively identify clusters with differing densities and irregular forms. In the merging phase, a novel similarity measure based on the interaction degree of shared k-nearest neighbors is used to iteratively merge subclusters exhibiting maximum similarity, addressing issues with highly overlapping clusters and unclear boundaries. Experimental evaluation on 10 synthetic and 10 UCI real datasets demonstrates the CPDD-ID algorithms superior ability to detect potential cluster structures and deliver high clustering accuracy compared to state-of-the-art approaches."
20,"[S. Song and X. Liang, ""Federated Pseudo-Sample Clustering Algorithm: A Label-Personalized Federated Learning Scheme Based on Image Clustering,"" Applied Sciences, vol. 14, no. 6, art. 2345, pp. 1–18, 2024. [Online]. Available: https://doi.org/10.3390/app14062345]","Federated learning (FL) enables collaborative model training while preserving privacy, but conventional approaches struggle with non-IID data and client-specific label distributions. This paper proposes the Federated Pseudo-Sample Clustering Algorithm (FPSCA), where each client clusters its local image data and generates cluster centroids (pseudo-samples) to represent the underlying data distribution. These pseudo-samples are sent to a central server, which aggregates them at the cluster or label level to update global or cluster-specific models, allowing for personalized and label-aware model aggregation. Experimental results on benchmarks such as MNIST and CIFAR-10 show that FPSCA outperforms standard techniques like FedAvg and FedProx in client-specific accuracy under high label heterogeneity and non-IID splits, while also improving communication efficiency by transmitting only cluster prototypes rather than entire models. FPSCA maintains privacy and robustness but may require extra client-side computation for clustering and parameter tuning. Future directions include adapting FPSCA for multimodal data and exploring dynamic clustering approaches."
21,"[H. Yin, A. Aryani, S. Petrie, A. Nambissan, A. Astudillo, and S. Cao, ""A Rapid Review of Clustering Algorithms,"" arXiv preprint arXiv:2401.07389, 2024. [Online]. Available: https://arxiv.org/abs/2401.07389]","Clustering algorithms, essential in areas such as marketing, healthcare, and social media, group data based on inherent patterns, yet no universal algorithm fits all tasks. This comprehensive review classifies mainstream clustering methods along five axes: their underlying principles (partition, hierarchical, density, grid, model-based), strategies for assigning data points to clusters (hard/soft), scalability to dataset size, dependence on predefined cluster counts, and domain-specific applications. For example, hard clustering (e.g., K-Means) assigns each data point to a single cluster, while soft clustering (e.g., Fuzzy C-Means, GMM) allows multiple memberships. Optimal cluster count selection remains challenging, with common approaches including the elbow method, silhouette score, and gap statistics. Evaluation relies on metrics like silhouette score, Davies-Bouldin, Dunns index (internal), ARI, and NMI (external). Research focus is shifting toward domain-specific customizations, integration with deep learning, and hybrid approaches for complex data. Challenges persist in determining cluster numbers and ensuring interpretability, with trends indicating increasing use of deep and hybrid clustering for improved adaptability and performance across diverse data types. There is no unequivocal best method; the optimal choice is context- and data-dependent."
22,"[S. Zhou, H. Xu, Z. Zheng, J. Chen, Z. Li, J. Bu, J. Wu, X. Wang, W. Zhu, and M. Ester, ""A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and Future Directions,"" arXiv preprint arXiv:2206.07579, 2022. [Online]. Available: https://arxiv.org/abs/2206.07579]","Clustering is a core machine learning task that traditionally assumes vectorized feature representations, but as data complexity has grown, conventional shallow clustering approaches falter on high-dimensional data. This survey reviews the advent of deep clustering, which jointly optimizes feature representation and clustering using deep architectures. It establishes a taxonomy of state-of-the-art deep clustering methods based on how they integrate representation learning and clustering modules: multi-stage deep clustering (where representation and clustering are sequential), iterative deep clustering (alternating between them), generative deep clustering, and simultaneous deep clustering (end-to-end learning). Key components and evaluation metrics (such as ACC, NMI, and ARI) are detailed, alongside summarized benchmark datasets covering images, text, graphs, and video, as shown in the survey tables. Real-world applications encompass community detection, anomaly detection, image segmentation, and medical data analysis. Challenges and open issues discussed include initialization effects, overlapping clusters, degeneracy, representation choices, explainability, transfer learning, anomaly detection, and computational efficiency. The survey concludes by mapping promising future directions and unifying principles in deep clustering, serving as a systematic resource and guide for ongoing research in the field."
23,"[V. Cohen-Addad, B. Guedj, V. Kanade, and G. Rom, ""Online k-means Clustering,"" in Proc. 24th International Conference on Artificial Intelligence and Statistics (AISTATS), Proceedings of Machine Learning Research, vol. 130, pp. 1126-1134, 2021. [Online]. Available: https://arxiv.org/abs/1909.06861]","We study the problem of learning a clustering for an online set of points under the $k$-means objective, where at each time step, the algorithm maintains $k$ candidate centers and incurs a loss equal to the squared distance from the new point to its closest center, aiming to minimize regret relative to the optimal $k$-means solution in hindsight. Assuming data lies within a bounded region, we show that a discretized Multiplicative Weights Update Algorithm (MWUA) achieves a regret bound of $\tilde{O}(\sqrt{T})$ in expectation. We further present an online-to-offline reduction implying that an efficient no-regret online algorithm would resolve the offline $k$-means problem, which is NP-hard. Therefore, we consider the relaxed objective of achieving regret compared to $(1+\epsilon)OPT$ and provide a no-regret algorithm with runtime $O\left(T\,\mathrm{poly}(\log(T),k,d,1/\epsilon)^{O(kd)}\right)$, leveraging a bounded-size coreset to focus MWUA on relevant regions of the space. We show that simpler methods like Follow The Leader fail to guarantee sublinear regret in worst cases, and we report preliminary experimental results on synthetic and real datasets. These findings answer an open question posed by Dasgupta (2008)."
24,"[Naveen Donthu, Satish Kumar, Nitesh Pandey, Prashant Gupta, ""Forty years of the International Journal of Information Management: A bibliometric analysis"", International Journal of Information Management, vol. 57, Article 102307, 2021. https://doi.org/10.1016/j.ijinfomgt.2020.102307]","This paper presents a bibliometric analysis of forty years of the International Journal of Information Management (IJIM), highlighting the journal's substantial growth in both publication volume and citation impact. The analysis reveals a shift in thematic focus away from traditional information sciences, with increasing emphasis on topics such as managing information systems, technology adoption, and social perspectives on knowledge management. Empirical research, particularly using quantitative methods, dominates the published work, and the culture of collaboration among authors has intensified, especially among those from Europe and the United States. Key article attributesincluding article order, methodology, European authorship, the number of references and keywords, and abstract lengthare significantly associated with citation counts; notably, conceptual and review articles also show a positive relationship with citations. Overall, IJIM has established itself as a unique and influential journal within its field."
25,"[Imran Ali, Maria Balta, Thanos Papadopoulos, ""Social media platforms and social enterprise: Bibliometric analysis and systematic review"", International Journal of Information Management, vol. 69, Article 102510, April 2023. https://doi.org/10.1016/j.ijinfomgt.2022.102510]","This paper presents the first comprehensive review of the intersection between social media platforms and social entrepreneurship (SE), employing both bibliometric analysis and systematic review of 258 articles published from 2013 to 2021. The study reveals a rising trajectory of publications, with research organized into three main clusters: (1) digital networks for collaborative innovation and co-creation, (2) social capital and legitimacy building, and (3) technological affordances for social value creation. While digital platforms are shown to foster collaboration, resource mobilization, and legitimacy for SE, the literature is thematically fragmented, heavily reliant on resource-based and social capital theories, and methodologically homogeneous, typically consisting of case studies or conceptual work. Major challenges include limited theoretical integration, insufficient comparative/cross-cultural analyses, and underexplored negative or unintended effects of social media use by SEs. The review develops a taxonomy of research themes, highlights empirical and conceptual gaps, and proposes future research directions such as adopting critical and multi-level theories, examining diverse platforms and technologies, and investigating the ethical, negative, or unintended consequences of digitalization. The article thus provides a holistic roadmap for academics and practitioners interested in leveraging social media for social innovation and entrepreneurship."
26,"[N. Tonellotto, C. Macdonald, and I. Ounis, ""Efficient Query Processing for Scalable Web Search,"" Foundations and Trends® in Information Retrieval, vol. 12, no. 4–5, pp. 319–500, 2018. [Online]. Available: https://www.nowpublishers.com/article/Details/INR-057]","Search engines play a vital role in modern information access, balancing the competing goals of effectiveness and efficiency as user demands grow and retrieval technologies become more complex. This survey offers a comprehensive review of foundational aspects such as index layouts and query processing strategiesincluding term-at-a-time (TAAT) and document-at-a-time (DAAT) approachesand discusses advanced methods like dynamic pruning (e.g., WAND and BMW algorithms) and impact-sorted posting lists, using illustrative figures to clarify algorithmic progressions. Recent advances such as the integration of learning-to-rank models in cascading infrastructures, as well as selective application of processing techniques via query efficiency prediction, are covered to highlight per-query efficiency-effectiveness trade-offs. The review concludes by outlining open research directions, including the adoption of signatures, and the impacts of real-time, energy-efficient, and modern hardware/software architectures on efficient search infrastructure."
27,"[R. Zanibbi, B. Mansouri, and A. Agarwal, ""Mathematical Information Retrieval: Search and Question Answering,"" Foundations and Trends® in Information Retrieval, vol. 19, no. 1–2, pp. 1–190, 2025. [Online]. Available: https://www.nowpublishers.com/article/Details/INR-095]","This book addresses the challenges of creating, interpreting, and searching mathematical information by introducing a unifying frameworkthe Source Jar Frameworkfor understanding mathematical information tasks spanning source identification, retrieval, annotation, and synthesis. It comprehensively reviews the representation of math formulas (including Symbol Layout Trees (SLTs) and Operator Trees (OPTs)), explains the design and evaluation of formula indexing and dense embedding approaches, and discusses the construction of interfaces supporting multimodal input and autocomplete. Throughout, it analyzes the performance of retrieval and question answering models (text-based, tree-based, embedding, hybrid) on major benchmarks such as NTCIR and ARQMath, presenting comparative results using metrics like $nDCG'$, $mAP'$, Bpref, and $P'@10$, and reports trade-offs uncovered through real-world system evaluations. The book emphasizes persistent challenges such as the vocabulary problem for formulae, annotation difficulties, polysemous notation, and the need for scalable test collections, while proposing future research into richer linking (e.g., AMR), novel formula representations, ensemble retrieval models, compact indices, and improved interfaces. Ultimately, the authors recognize that mathematical information retrieval is inherently interdisciplinary, noting the progress enabled by advances in machine learning and LLMs, but stress that interpretability, expert assessment, and user understanding remain central to building truly effective math information systems."
28,"[R. Reinanda, E. Meij, and M. de Rijke, ""Knowledge Graphs: An Information Retrieval Perspective,"" Foundations and Trends® in Information Retrieval, vol. 14, no. 4, pp. 289–444, 2020. [Online]. Available: https://www.nowpublishers.com/article/Details/INR-063]","This survey provides a comprehensive overview of the intersection between knowledge graphs (KGs) and information retrieval (IR), highlighting how modern IR systems can utilize information from both public and proprietary KGs to enhance tasks such as document and entity retrieval. The authors review the key components needed to build IR systems that leverage KGs, organizing the literature from two perspectives: using KGs for IR, and enriching KGs via IR techniques like entity recognition, typing, and relation extraction. They discuss common challenges encountered in integrating these technologies, outline future research directions, and offer resources such as datasets useful for both newcomers and experienced researchers in the field. The work aims to serve as an introduction and reference for those looking to understand or innovate at the interface of IR and knowledge graphs."
29,"[Y. Qiao, S. Ji, C. Wang, J. Shao, and T. Yang, ""Privacy-aware document retrieval with two-level inverted indexing,"" Information Retrieval Journal, vol. 26, Art. no. 12, 2023. [Online]. Available: https://link.springer.com/article/10.1007/s10791-023-09428-z]","This paper introduces QDT, a privacy-aware document retrieval and ranking system that employs a two-level inverted index structure to limit information leakage during the process of securely matching and ranking documents in a cloud environment. By grouping document IDs and utilizing group- and member-based bucket tags, the design enables retrieval of encoded feature vectors for ranking while minimizing risks such as unauthorized query processing and cross-posting-list document sharing inference. The query protocol uses randomization and query-specific tagging, while guided padding with fake documentscarefully matched to feature distributions and member ID obfuscationconceals posting list lengths and document co-occurrences, thereby thwarting leakage-abuse and plaintext-recovery attacks. Evaluation on TREC datasets demonstrates minimal impact on ranking relevance (e.g., NDCG and precision remain high), sub-second server response times enabled by the two-level structure (with up to 130x speedup), and a marked reduction in attack accuracy (down to 2 out of 130, or zero with increased fake padding). However, these privacy gains incur a storage overhead of 4.8x and increased computation due to cryptographic operations on large integers, evidencing a substantial privacy-efficiency tradeoff. The authors propose integrating transformers and learned document representations (such as BERT) for further efficiency and privacy advances as future work. Key findings are structured below:

\[
\begin{tabular}{|l|c|c|c|}
\hline
Metric & Traditional & QDT (no padding) & QDT (with padding) \\
\hline
Storage Cost & 1x & 4.8x & higher \\
Query Speedup & 1x & 130x & - \\
Attack Accuracy & high & 2/130 & 0 \\
Ranking Relevance (NDCG, Precision) & high & high & high \\
\hline
\end{tabular}
\]

In conclusion, QDT achieves strong privacy protections for document retrieval and ranking with significant improvements in efficiency compared to previous privacy-preserving schemes, albeit at the cost of increased storage and computation."
30,"[T. Gagie, A. Hartikainen, K. Karhu, J. Kärkkäinen, G. Navarro, S. J. Puglisi, and J. Sirén, ""Document retrieval on repetitive string collections,"" Information Retrieval Journal, vol. 20, pp. 273–303, 2017. [Online]. Available: https://doi.org/10.1007/s10791-017-9297-7]","Most large and rapidly growing string collections, such as versioned documents, genomic sequences, or software repositories, are highly repetitive, and exploiting this repetitiveness can drastically reduce space requirements for information retrieval tasks. Classical inverted indexes, effective for natural language documents, are limited when applied to generic string collections where identifying and parsing words is nontrivial or irrelevant. This paper studies indexing methods for such repetitive string collections, introducing two novel approaches: the interleaved LCP (ILCP) arrayan interleaving of per-document longest-common-prefix arrays amenable to run-length compressionand Precomputed Document Lists (PDL), which use grammar-compressed partial answers at sampled suffix tree nodes. ILCP enables efficient document listing and counting using compact encodings; PDL supports fast top-$k$ document retrieval and is especially space- and time-efficient for repetitive data. Theoretical and empirical analysis shows these indexes offer significant advantages in space-time tradeoffs over classic and brute-force approaches. Experimental evaluation on repetitive datasets (e.g., Wikipedia versions, bio-sequences) demonstrates that the choice of index depends on collection repetitiveness and document size: PDL is often the preferred practical solution, while compressed Sadakane-based structures are recommended for document counting. For multi-term ranked queries, a combined index supports tf-idf based ranking, performing competitively in speed with inverted indexes though using more space due to the substring-level indexing. Remaining challenges include scaling index construction, enabling disk-based and incremental updates, and supporting advanced rankingareas identified for future work. Overall, the presented methods extend efficient document retrieval to new domains beyond natural language, where classical word-based methods fail, and set the groundwork for scalable, practical solutions to indexing large repetitive collections."
31,"[J. Lin and A. Trotman, ""The role of index compression in score-at-a-time query evaluation,"" Information Retrieval Journal, vol. 20, pp. 274–314, 2017. [Online]. Available: https://doi.org/10.1007/s10791-016-9291-5]","This paper investigates the efficiency of top $k$ document retrieval using score-at-a-time (SaaT) query evaluation on impact-ordered in-memory indexes, with a focus on how index compression affects query latency within modern processor architectures. By testing different compression schemes (variable byte, Simple-8b, QMX variants, and no compression) on four web test collections, the authors find that leaving postings uncompressed yields the fastest query evaluation speeds, though the margin over state-of-the-art compression (QMX-D4) is minor and comes with a significant increase in index size. Their findings are interpreted through the lens of processor design; high-impact index segments are typically short and benefit from cache locality, while long, low-impact segments are effectively handled by modern memory bandwidth and prefetching. Effectiveness metrics (AP, NDCG@10) remain statistically indistinguishable across methods, confirming correctness. Compression schemes approximately halve the index size with minimal performance sacrificeSimple-8b and vbyte offer similar compression ratios, while QMX-D4 indexes are slightly larger. The study, limited to single-threaded performance and a modest query set, highlights the importance of ""architecture affinity"" for search engine design and suggests continued empirical comparison of retrieval techniques as hardware evolves. The authors note that uncompressed indexes may be preferable when maximum speed is critical, but compression provides significant memory savings; future work should examine multi-threaded performance and compare other evaluation algorithms, such as DaaT, from the perspective of hardware interaction."
32,"[Xingyan Bin, Jianfei Cui, Wujie Yan, Zhichen Zhao, Xintian Han, Chongyang Yan, Feng Zhang, Xun Zhou, Qi Wu, and Zuotao Liu,  ""Real-time Indexing for Large-scale Recommendation by Streaming Vector Quantization Retriever,""  arXiv preprint arXiv:2501.08695, pp. 1–20, 2024.  Available: https://arxiv.org/abs/2501.08695]","Retrievers are crucial components in large-scale recommendation systems, tasked with efficiently filtering massive candidate sets under strict latency constraints. Traditional approaches like HNSW Two-tower rely on index structures such as Product Quantization (PQ), but face limitations: they require periodic reconstruction, struggle with index imbalance (e.g., hot items accumulating in few clusters), and fail to capture dynamic trends due to fixed item-index assignments. To address these issues, the authors introduce a streaming Vector Quantization (streaming VQ) model, which assigns items to clusters in real-time, thus achieving index immediacy and adaptability. This model supports complex ranking architectures and multi-task learning, thanks to mechanisms for balancing and reparability that optimize index effectiveness as well as support complicated models. Streaming VQ is lightweight and easy to deploy; its industry adoption in Douyin and Douyin Lite led to significant increases in user engagement. Experiments demonstrate that streaming VQ outperforms traditional retrievers by generating more precise candidate sets, accommodating the semantic drift of item clusters, and facilitating the adoption of advanced ranking models. The authors conclude that streaming VQs combination of immediacy, reparability, and balancing makes it suitable for broader real-world deployment in recommendation tasks."
33,"[Liyang Sun, Yujing Wang, Zejian Wang, Xinyi Wu, Xiangming Dou, Jinji Li, Yicheng Bai, Xuerui Wang, Weinan Zhang, Yong Yu, and Zhenguo Li,  ""The Disruption Index Measures Displacement Between a Paper and Its Citations,""  arXiv preprint arXiv:2504.04677, 2024.  Available: https://arxiv.org/abs/2504.04677]","Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research. Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias. Here, we show thatcontrary to the common belief that the D-index measures absolute innovationit captures relative innovation: a paper's ability to displace its most-cited reference. In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental questionmuch like light bulbs replacing candles. We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence. To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (18002024) based on OpenAlex."
34,"[Kiyonari Kobayashi, Shusuke Shimbo, and Yuji Matsumoto,  ""Resource-Efficient Index Advisor Utilizing Large Language Model,""  arXiv preprint arXiv:2503.07884, 2024.  Available: https://arxiv.org/abs/2503.07884]","LLMIdxAdvis is a resource-efficient index advisor for database management systems that leverages large language models (LLMs) without requiring extensive fine-tuning. Unlike traditional heuristic or learning-based index recommendation methods, which can be slow, resource-intensive, and lack generalizability, LLMIdxAdvis formulates index recommendation as a sequence-to-sequence task where the model receives the target workload, storage constraints, and database environment, and outputs a set of recommended indexes. Through offline synthesis of diverse SQL workloads using GPT-4-Turbo and heuristic labeling, it builds a demonstration pool, which is then used for in-context learning during recommendation to inject domain expertise. The system further strengthens LLM understanding by extracting detailed workload features, including column statistics. A novel inference scaling mechanism combines vertical scaling (e.g., Index-Guided Major Voting and Best-of-N) and horizontal scaling (iterative self-optimization with live database feedback), making recommendations more robust and reliable. Experimental evaluation on three OLAP and two real-world (ByteDance) benchmarks shows that LLMIdxAdvis matches or beats existing learning and heuristic baselines in recommendation quality, achieves nearly 99.3% reduction in recommendation time compared to RL methods, and generalizes well across different database schemas and workloads. Ablation studies confirm the critical importance of feature extraction, demonstration selection, and scaling strategies for performance, while real-world validation demonstrates practical effectivenessespecially for complex schemas. Despite these advances, challenges remain in handling highly complex analytic workloads and maintaining efficiency for large candidate index sets, pointing to future work in further optimization and possibly domain-adapted LLM fine-tuning."
35,"[H. Wei, P. Li, H. Gao and C. Wang, ""String Similarity Search: A Hash-Based Approach,"" IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 7, pp. 1371-1385, July 2017. [Online]. Available: https://ieeexplore.ieee.org/document/8051071]","This article proposes new hash-based labeling techniques, namely OX label and XX label, to improve string similarity search based on edit distance. Traditional methods require constructing large indices for long strings or big datasets, which prolongs indexing and diminishes query performance. The proposed methods address these limitations by assigning hash labels to strings, enabling efficient pruning of dissimilar candidates through hash label comparison and thereby significantly reducing both index size and construction timeby an order of magnitude compared to existing approacheswhile maintaining high search efficiency."
36,"[H. Wang, J. Zhang, Y. Wei, Y. Wang, X. Zhang and J. Pei, ""Neural Similarity Search on Supergraph Containment,"" IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 11, pp. 11200-11214, Nov. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10135129/]","Supergraph search aims to efficiently identify all data graphs within a query graph using subgraph isomorphism, but existing methods relying on indices and filtering-verification frameworks can be computationally intensive and redundant. Addressing this, the paper introduces Neural Supergraph similarity Search (NSS), the first learning-based approach for supergraph containment similarity search. NSS learns representations for both query and data graphs, enabling supergraph search in representation space with linear complexity relative to the number of data graphs. Key innovations include a Wasserstein discriminator and a reconstruction network that capture interrelations, structure, and label information between and within graphs. Experimental results show that NSS is up to $10^6$ times faster than state-of-the-art exact methods for query processing, while also providing higher accuracy compared to existing learning-based solutions."
37,"[D. Zhang, Y. Huang, H. Wang, D. Yang, Z. He and J. Xu, ""Continuous Trajectory Similarity Search for Online Outlier Detection,"" IEEE Transactions on Knowledge and Data Engineering, vol. 33, no. 10, pp. 3405-3419, Oct. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9306918/]","In this paper, we study a new variant of trajectory similarity search from the context of continuous query processing. Given a moving object from s to d, ..."
38,"[J. Zhang, J. Tang, C. Ma, X. Chen, Y. Liu, and J. Li, ""Fast and Flexible Top-k Similarity Search on Large Networks,"" ACM Transactions on Information Systems, vol. 36, no. 2, Article 14, pp. 1-34, May 2018. [Online]. Available: https://dl.acm.org/doi/10.1145/3086695]","The paper introduces Panther, a novel framework for fast and flexible similarity search in large-scale networks, targeting applications like collaborator recommendation and community detection. Existing methods such as SimRank and Personalized PageRank suffer from cubic time complexity, limiting scalability. Panther overcomes this by using a random path sampling strategy: it samples random paths from the network and estimates node similarity based on their co-occurrence in these paths, providing unbiased estimation of arbitrary network features. This approach allows Panther to flexibly incorporate diverse structural features into the similarity measurement and maintain high accuracy while achieving dramatic speedupsover 300 times faster than existing algorithmsscaling linearly with the number of edges. Experiments on networks with up to 1.9 billion edges demonstrate that Panther achieves comparable or superior accuracy to traditional methods, with query times reduced from days to seconds. The framework offers a quantifiable trade-off between speed and accuracy, supports building a compact index for efficient querying, and is extensible to various user-defined features. Major challenges addressed include escaping cubic complexity and generalizing similarity measurements, while future work is directed toward handling dynamic networks and automating feature selection. In conclusion, Panther enables practical, accurate, and scalable similarity search for big data network analysis."
39,"[S. Wang, L. Qin, J. X. Yu, R. Jin, and L. Chang, ""Continuously Adaptive Similarity Search,"" ACM Transactions on Information Systems, vol. 38, no. 3, Article 28, pp. 1–28, June 2020. [Online]. Available: https://dl.acm.org/doi/10.1145/3318464.3380601]","The paper presents OASIS, a framework designed for online and adaptive similarity search in high-dimensional data analytics where the similarity function may evolve dynamically due to user feedback or changing data distributions. Unlike prior methods that assume a fixed similarity function, OASIS incrementally updates a Mahalanobis distance metric $M$ in response to streaming feedback by solving an online convex optimization problem, typically via online gradient descent or Passive-Aggressive algorithms. It maintains a family of Locality Sensitive Hashing (LSH) indices for a covering set of similarity functions, dynamically selecting or creating the most relevant index as $M$ evolves, and employs model reuse to reduce storage by sharing LSH structures between similar metrics. Experimental results on datasets like MNIST and CIFAR demonstrate that OASIS achieves higher recall and precision, 10100$\times$ faster query times compared to retraining, and uses less memory via efficient index reuse. The framework is robust to concept drift and adversarial feedback, improving the performance of k-NN classification, outlier detection, and active learning tasks. While limitations include reliance on linear Mahalanobis metrics and sensitivity to feedback patterns, OASIS marks a significant advance for real-time interactive analytics where adaptive similarity search is required, and future work may extend it to nonlinear similarity models and deeper index-sharing strategies."
40,"[J. Li, B. He, and D. Wang, ""A Scalable Random-Walk-Based Network Embedding Algorithm with Local Structural Information,"" Journal of Artificial Intelligence Research, vol. 71, pp. 651–683, 2021. [Online]. Available: https://jair.org/index.php/jair/article/view/12567/26689]","Higher-order proximity preserved network embedding methods aim to capture not only direct but also more complex, multi-hop relationships in networks. While random-walk-based approaches offer scalability and the capability to explore higher-order neighborhoods, existing methods often lack expressiveness for personalized proximity and clear theoretical objectives. This paper proposes a general scalable random-walk-based network embedding framework that explicitly integrates random walks into a theoretical objective, allowing for arbitrary higher-order proximity preservation. By introducing a random walk with restart mechanism, the method achieves personalized and weighted proximity preservation across different orders. Experimental results on real-world networks show that the proposed method substantially and consistently outperforms current state-of-the-art network embedding approaches."
41,"[A. Koudounas, C. Papagiannopoulou, L. Rokach, and S. Papadopoulos, ""Gradient-based Learning Methods Extended to Similarity-Based Models for Large-Scale Data,"" Journal of Artificial Intelligence Research, vol. 69, pp. 1209–1247, 2020. [Online]. Available: https://jair.org/index.php/jair/article/view/12192/26600]","Grassmann manifold based sparse spectral clustering is a classification technique focused on learning a latent data representation as a sparse subspace basis. Spectral clustering is reformulated as a loss minimization problem over the Grassmannian manifold, necessitating specialized optimization algorithms since traditional gradient-based techniques are unsuitable for constrained parameter spaces. This work extends classical learning algorithmsfrom standard gradient descent to adaptive momentum methodsonto curved manifold spaces using manifold calculus, evaluates their clustering performance against established methods, and demonstrates that the proposed algorithms achieve comparable clustering efficacy with reduced computational complexity."
42,"[Jorge Martinez-Gil, ""Evaluation of Code Similarity Search Strategies in Large-Scale Codebases,"" Machine Learning with Applications, vol. 10, Art. no. 100423, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666827022000868]","The paper addresses the pressing need for efficient automatic identification of similar code fragments within vast code repositories, which is essential for tasks such as code reuse and debugging. It presents a comparative analysis of current and emerging techniques for code similarity search, evaluating them across a variety of codebases using metrics like indexing time, search speed, and the semantic relevance of retrieved results. The study's goal is to provide practical guidance for software developers facing the challenges posed by ever-growing codebases and to aid them in selecting effective strategies for performing code similarity searches."
43,"[Muhammad Umair Hassan, Xiuyang Zhao, Raheem Sarwar, Naif R. Aljohani, S. M. M. Rahman, K. Muhammad, and M. A. Raza, ""SODRet: Instance retrieval using salient object detection for self-service shopping,"" Machine Learning with Applications, vol. 15, Art. no. 100523, March 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666827023000762]","Self-service shopping technologies are pervasive, but there remains a need for more efficient consumer services. Advancements in mobile and IoT technologies enable new information dissemination approaches, and object retrieval applications are identified as beneficial for online or self-service shopping environments. To address this gap, the study introduces an object retrieval system based on a fusion-driven salient object detection (SOD) method. With significant recent progress in SOD models for object detection and retrieval, the proposed systemSODRetleverages saliency maps to predict the position and facilitate retrieval of objects from commodity datasets. Evaluation on the INSTRE and Flickr32 datasets demonstrates that SODRet surpasses current state-of-the-art object retrieval methods, suggesting its potential for large-scale deployment in self-service shopping points of sale."
44,"[Jean Bertin, ""Advancing Similarity Search with GenAI: A Retrieval Augmented Generation Approach,"" arXiv preprint arXiv:2501.04006 [cs.IR], Dec. 2024. [Online]. Available: https://arxiv.org/abs/2501.04006]","This article presents a novel Retrieval Augmented Generation (RAG) approach to similarity search, deploying a generative model to more effectively capture nuanced semantic relationships between sentence pairs. Focusing on the BIOSSES dataset, which contains 100 biomedical sentence pairs rated for semantic similarity, the method departs from traditional embedding and vector-based similarity computations by establishing a conversational chain using carefully engineered prompts for a large language model. Performance is measured via the Pearson correlation coefficient between model-generated similarity scores and expert annotations, where results peak at a Pearson correlation of $0.905$ with a temperature parameter of $0.5$ and 20 in-context examples in the system promptsurpassing previous bests, such as $0.871$ from a supervised method combining several techniques. Key findings include the positive influence of prompt engineering and moderate sampling temperature on accuracy, though challenges in computational resources, output robustness, and test reproducibility remain due to model constraints and the iterative architecture requirement. The authors note that further gains may be possible through advanced prompt strategies and alternative generative models, positioning this methodology as an important advance in semantic similarity search across domains with complex language understanding requirements."
45,"[Simeon Emanuilov and Aleksandar Dimov, ""Billion-scale Similarity Search Using a Hybrid Indexing Approach with Advanced Filtering,"" Cybernetics and Information Technologies, vol. 24, no. 4, pp. 45-58, 2024. [Online]. Available: https://arxiv.org/abs/2501.13442 ; https://doi.org/10.2478/cait-2024-0035]","This paper presents a novel approach for similarity search with complex filtering capabilities on billion-scale datasets, optimized for CPU inference. Our method extends the classical IVF-Flat index structure to integrate multi-dimensional filters. The proposed algorithm combines dense embeddings with discrete filtering attributes, enabling fast retrieval in high-dimensional spaces. Designed specifically for CPU-based systems, our disk-based approach offers a cost-effective solution for large-scale similarity search. We demonstrate the effectiveness of our method through a case study, showcasing its potential for various practical uses."
46,"[Patrick Schäfer, Jakob Brand, Ulf Leser, Botao Peng, and Themis Palpanas, ""Fast and Exact Similarity Search in less than a Blink of an Eye,"" arXiv preprint arXiv:2411.17483 [cs.DB], revised Dec. 2024. [Online]. Available: https://arxiv.org/abs/2411.17483]","Similarity search for data series (DS)sequences of real values like time seriesis essential in scientific fields, but existing symbolic summarization approaches such as SAX struggle with high-frequency, noisy, or non-normally distributed data due to their rigid quantization assumptions. The proposed SymbOlic Fourier Approximation index (SOFA) leverages Symbolic Fourier Approximation (SFA), which combines the Discrete Fourier Transform, variance-based feature selection, and learned, data-adaptive equi-width binning (handled via Multiple Coefficient Binning, MCB) to produce compact, representative DS embeddings that tightly bound the Euclidean distance (ED) between series. SOFA organizes these representations in a parallel hierarchical tree index (inspired by MESSI), supporting fast exact queries via the GEMINI approach, using lower-bounding calculations accelerated by SIMD. Experiments across 17 large, diverse datasets (spanning seismology, astronomy, neuroscience, totaling 1TB and 1 billion DS) show SOFA outperforms MESSI, FAISS, and parallel sequential scan by 238x, particularly on high-frequency or non-normal datafor example, yielding query times as low as 58 ms for 1-NN searches. Ablation studies confirm variance-based coefficient selection and equi-width binning achieve the tightest lower bounds, and SOFA maintains efficiency as data and core counts scale. The method is especially robust and translates advanced pruning and representation quality into dramatic improvements in query time, enabling sub-second exact similarity search at billion-scale. Future directions include adapting SOFA for approximate search and GPU acceleration, and exploring further applications of its symbolic embedding approach for large datasets."
47,"[S. Ray and B. Nickerson, ""Temporally relevant parallel top-k spatial keyword search,"" Journal of Spatial Information Science, no. 24, pp. 1–36, 2022. [Online]. Available: https://josis.org/index.php/josis/article/view/199]","New spatio-textual indexing methods are required to efficiently search and update the increasing volume of spatially referenced text from location-based services, such as those providing geo-tagged documents for recommendations and events. While existing spatio-textual indexes can support top-k spatial keyword queries (TkSKQ) and some offer live updates, they often neglect to combine temporal relevance, textual similarity, and spatial proximity in their ranking schemes, and lack efficient parallelism during updates and queries. The proposed parallel spatio-textual index, Pastri, addresses these issues by allowing incremental updates over real-time streams and introducing a dynamic ranking scheme that prioritizes temporal relevance at query time. Integrated with a persistent document store and multithreaded execution to exploit parallelism, Pastri is empirically shown, using both real and synthetic datasets, to sustain high update throughput and deliver TkSKQ performance that is one to two orders of magnitude faster than competing spatio-textual indexes."
48,"[M. F. Mokbel, ""Thinking spatial,"" Journal of Spatial Information Science, no. 21, pp. 3–11, 2020. [Online]. Available: https://josis.org/index.php/josis/article/view/128]","The paper argues that although general-purpose systems such as database systems, big data platforms, data streaming architectures, and machine learning systems have achieved widespread success, they often treat spatial data as an afterthought by supporting it with add-on modules or functions rather than integrating it at the core engine level. The author contends that spatial data warrants native support, being treated as a first-class citizen in system design to achieve better accuracy and performance for spatial applications. The article identifies that database systems, big data frameworks, machine learning systems, recommender systems, and social network platforms would all significantly benefit from embedding spatial data and operations directly into the underlying system architecture rather than relying on external spatial add-ons."
49,"[M. Hojati, R. Feick, S. Roberts, C. Farmer, and C. Robertson, ""Distributed spatial data sharing: a new model for data ownership and access control,"" Journal of Spatial Information Science, no. 27, pp. 1–26, 2023. [Online]. Available: https://josis.org/index.php/josis/article/view/220]","With the rise of new technologies and increased participation in geospatial data production, spatial data sharing is now often transacted and controlled by a few private corporate services rather than centralized experts. Data creation has evolved to hybrid models where diverse individuals co-produce and share spatial data but remain subject to centralized architectures. Driven by Web 2.0 advancements and accessible Internet tools, this shift raises concerns over data access, especially regarding sensitive or personally identifiable information, which intensifies interest in distributed file technologies as alternatives to centralized platforms. The paper explores a potential evolution toward decentralized spatial data sharing as distributed web technologies, such as Web 3.0, mature, leading to hybridized ownership and access control systems. Opportunities and barriers for distributed spatial data sharing are discussed, including implications for managing large geographic datasets and the necessity for protocols that enable sharing, integration, and processing of spatial data over distributed networks."
50,"[S. Ladra, M. Rodríguez Luaces, J. R. Parama, and F. Silva-Coira, ""Compact and indexed representation for LiDAR point clouds,"" International Journal of Geographical Information Science, vol. 27, no. 4, pp. 1035–1070, 2024. [Online]. Available: https://www.tandfonline.com/doi/full/10.1080/10095020.2022.2121664]","LiDAR devices generate large datasets of 3D point clouds with attributes like color, position, and time, typically stored in compressed formats such as LAZ that allow partial decompression but are not optimized for flexible querying. This paper introduces the first data structure that losslessly compresses point clouds with attributes while simultaneously indexing all spatial dimensions and attribute values, enabling direct querying of the compressed data. By keeping the compressed index structure in main memory, disk access is avoided, and query speeds are increased; the proposed method is reported to run range and attribute queries up to 100 times faster than previous approaches."
51,"[A. Chaves Carniel, ""Defining and designing spatial queries: the role of spatial relationships,"" Geo-spatial Information Science, vol. 27, no. 6, pp. 1868–1892, 2024. [Online]. Available: https://www.tandfonline.com/doi/full/10.1080/10095020.2022.2163924]","Spatial relationships are fundamental to the construction and interpretation of spatial queries, as they characterize how spatial objects relate in space (e.g., topologically, metrically, or by direction). This paper introduces a taxonomy for naming, describing, and classifying spatial query types based on these relationship categories: (i) topological (such as overlap, meet, or inside), (ii) metric (like nearest neighbors), and (iii) direction (e.g., cardinal directions). The taxonomy facilitates discussion of intuitive descriptions, formal definitions, and implementation techniques for various spatial queries, illuminating correspondences between different query types and highlighting ongoing challenges and open research questions in spatial information retrieval."
52,"[Y. Li, R. Zhang, Q. Ma, J. Song, B. Zhang, M. Bai, W. Wang, and Y. Li, ""CSD-RkNN: reverse k nearest neighbors queries with category-sensitive distance,"" International Journal of Geographical Information Science, vol. 37, no. 8, pp. 1709–1730, 2023. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/13658816.2023.2249521]","The reverse $k$ nearest neighbor (R$k$NN) query identifies all points for which the query point falls among their $k$ nearest neighbors, inversing the traditional $k$NN that locates $k$ closest points to a query. This paper introduces a conics-based, efficient verification method for R$k$NN and implements an algorithm on VoR-tree, achieving computational complexity $O(k^{1.5} \cdot \log k)$. Comparative experiments demonstrate that this proposed algorithm outperforms two leading R$k$NN algorithms in efficiency."
53,"[K. Vrotsou, J. Johansson, J. Jern, A. Backman, and R. Strömberg, ""An Interactive Approach for Exploration of Flows Through a Matrix-based Visual Representation,"" Journal of Geovisualization and Spatial Analysis, vol. 1, no. 1–2, pp. 21–36, Dec. 2017. [Online]. Available: https://link.springer.com/article/10.1007/s41651-017-0001-7]","This paper addresses the challenge of visualizing geographic flowsorigin-destination data representing movementswhose complexity often leads to cluttered, hard-to-interpret displays due to extensive link crossings and overlaps. The authors propose Flowcube, an interactive 3D visualization tool that represents flows as arcs over a 2D base map and introduces a key innovation: the Direction-Based Filtering (DBF) lens. This lens, implemented as a movable rectangular band in screen space, filters the display to show only those flows whose endpoints fall within the lens, thus systematically revealing arrangement patterns (concentration, trend, alignment) along specific spatial directions without distorting geographic context or losing details. Applied to a dataset of over 1.3 million geolocated Flickr photos in Greater London, the system enables users to discover movement patterns of tourists (e.g., city center concentration, flows towards Heathrow and Wembley) and locals (e.g., residential dispersal, flows to shopping and recreation areas), including subtle trends in weaker flows missed by other techniques. User studies highlight that the DBF lens allows detection of these patterns without excessive filtering or loss of information, though the exploration remains user-driven and dependent on lens configuration. The Flowcube approach outperforms traditional methods by exposing nuanced flow arrangements, and future work aims to augment this with automated pattern suggestions and temporal analysis. The mathematical model formalizes flows as $L \subseteq P \times P$, with a magnitude mapping $M: L \to \mathbb{R}^+$, representing each flow as a triple $(p_i, p_j, m_{ij})$ quantifying directed spatial interactions."
54,"[V. Pandey, A. van Renen, E. T. Zacharatou, A. Kipf, I. Sabek, J. Ding, V. Markl, and A. Kemper, ""Enhancing In-Memory Spatial Indexing with Learned Search,"" arXiv preprint arXiv:2309.06354, 2023. [Online]. Available: https://arxiv.org/abs/2309.06354]","Spatial data is increasingly generated from numerous sources such as GPS devices, applications, and social media, creating challenges in efficient spatial data processing. This study adapts a state-of-the-art machine-learned search technique for single-dimensional data to spatial indexing, employing six spatial partitioning strategies and integrating machine-learned search within each partition to handle point, range, distance, and spatial join queries. By optimizing each partitioning method for its respective instance, the study finds that (i) grid-based index structures outperform tree-based ones by $1.23\times$ to $2.47\times$, (ii) learning-enhanced spatial indexes outperform traditional versions by $1.44\times$ to $53.34\times$, (iii) machine-learned search surpasses binary search within a partition by $11.79\%$$39.51\%$ for single-dimension filtering, (iv) the benefits decrease with more complex operations like high-selectivity scans, Haversine distance, or point-in-polygon tests, and (v) index lookup is the main bottleneck for tree-based structures, suggesting that linearizing partitions may alleviate this."
55,"[S. Shaham, G. Ghinita, and C. Shahabi, ""Fair Spatial Indexing: A paradigm for Group Spatial Fairness,"" arXiv preprint arXiv:2302.02306, 2023. [Online]. Available: https://arxiv.org/abs/2302.02306]","Machine learning (ML) is increasingly used in decision-making contexts such as loan approvals and job applicant screenings, where concerns about equitable access for under-privileged backgrounds arise. While fairness with respect to protected attributes like gender, race, or income has been widely studied, the impact of location data on fairness has often been overlooked, despite the growing use of geospatial attributes in MLattributes that correlate strongly with protected classes and can introduce unfair biases. This work addresses the underexplored issue of location bias in ML by proposing mitigation techniques, particularly focusing on miscalibration in geospatial data. Emphasizing spatial group fairness, the authors introduce a fairness-aware spatial indexing algorithm inspired by KD-trees. Experimental results on real datasets show that this approach significantly enhances fairness in ML outcomes without compromising learning accuracy."
56,"[M. Lawson, W. Gropp, and J. Lofstead, ""Exploring Spatial Indexing for Accelerated Feature Retrieval in HPC,"" arXiv preprint arXiv:2106.13972, 2021. [Online]. Available: https://arxiv.org/abs/2106.13972]","Despite the critical importance of range queries in high-performance computing (HPC) applications for analysis and visualization, there has not been a comprehensive evaluation of the indices designed to accelerate these queries within HPC contexts. This paper addresses this gap by analyzing 20 open-source C and C++ libraries that support range queries, providing insights into their viability for HPC, comparing them on metrics such as build time, query time, memory usage, and scalability, discussing various trade-offs, determining whether a universal best solution exists, and identifying scenarios where a brute-force approach might be optimal. The findings and lessons presented aim to assist both HPC application scientists and developers of spatial indexing libraries."
57,"[Abhinav Natarajan, Maria De Iorio, Andreas Heinecke, Emanuel Mayer, and Simon Glenn, ""Cohesion and Repulsion in Bayesian Distance Clustering,"" Journal of the American Statistical Association, vol. 119, no. 546, pp. 1374–1384, Apr. 2024. [Online]. Available: https://doi.org/10.1080/01621459.2023.2191821]","Clustering in high-dimensions poses many statistical challenges. While traditional distance-based clustering methods are computationally feasible, they lack probabilistic interpretation and rely on heuristics for estimation of the number of clusters. On the other hand, probabilistic model-based clustering techniques often fail to scale and devising algorithms that are able to effectively explore the posterior space is an open problem. Based on recent developments in Bayesian distance-based clustering, we propose a hybrid solution that entails defining a likelihood on pairwise distances between observations. The novelty of the approach consists in including both cohesion and repulsion terms in the likelihood, which allows for cluster identifiability. This implies that clusters are composed of objects which have small 'dissimilarities' among themselves (cohesion) and similar dissimilarities to observations in other clusters (repulsion). We show how this modelling strategy has interesting connection with existing proposals in the literature as well as a decision-theoretic interpretation. The proposed method is computationally efficient and applicable to a wide variety of scenarios. We demonstrate the approach in a simulation study and an application in digital numismatics."
58,"[Qing Mai, Xin Zhang, Yuqing Pan, and Kai Deng, ""A Doubly Enhanced EM Algorithm for Model-Based Tensor Clustering,"" Journal of the American Statistical Association, vol. 117, no. 540, pp. 2120–2134, Oct. 2022. [Online]. Available: https://doi.org/10.1080/01621459.2021.1904959]","This paper introduces a model-based clustering framework for tensor-valued data via the tensor normal mixture model (TNMM), addressing the unique challenges of high-dimensional, multiway scientific data such as those found in neuroimaging and chemometrics. The TNMM models tensor observations $X \in \mathbb{R}^{p_1 \times \cdots \times p_d}$ as drawn independently from a mixture of $K$ tensor normal distributions, each characterized by a mean tensor $\mu_k$ and a separable covariance structure $(\Sigma_{k,1}, \ldots, \Sigma_{k,d})$, i.e., $X \sim TN(\mu_k, \Sigma_{k,1}, \ldots, \Sigma_{k,d})$ for cluster $k$. To enhance parsimony and interpretability, the authors incorporate penalization (such as L1 or sparsity-inducing penalties) on the mean and covariance factors. For estimation and clustering, they propose a doubly-enhanced expectation-maximization (DEEM) algorithm, which features carefully tailored E-steps and M-steps that leverage the tensor structure with adaptive penalties and separability constraints. Theoretical results demonstrate that DEEM achieves consistent clustering even as tensor dimensions grow exponentially with sample size, with explicit error bounds dependent on mode dimensions and cluster separability. Extensive simulations and real data applications show that DEEM outperforms existing methods (e.g., vectorized EM, tensor $k$-means), with lower misclassification rates and scientifically meaningful cluster discovery. The paper discusses challenges such as high-dimensional estimation, initialization sensitivity, and assumptions on covariance separability, and suggests future work in extending the model's flexibility, scalability, and applicability. Overall, the TNMM with DEEM provides an interpretable, theoretically justified, and computationally efficient solution for clustering complex tensor data."
59,"[A. Adolfsson, M. Ackerman, and N. C. Brownstein, ""To cluster, or not to cluster: An analysis of clusterability methods,"" Pattern Recognition, vol. 88, pp. 13–26, Apr. 2019. [Online]. Available: https://doi.org/10.1016/j.patcog.2018.10.026]","Clustering is a central unsupervised learning technique used across diverse fields, but its effectiveness relies on the presence of true cluster structure in data. This paper extensively compares 12 clusterability measuresincluding data reduction approaches, multimodality tests (such as the Dip and Silverman's tests on pairwise distances or principal component projections), spatial randomness methods (like Hopkins and VAT), and classical indices (Dunn, silhouette)using detailed simulations across 11 synthetic data types and analyses of 17 real-world datasets. The study finds that many classical indices are sensitive to noise and do not reliably distinguish clusterable from unclusterable data, while multimodality-based methods (notably Dip-dist, PCA Dip, Silv-dist) provide robust performance across most situations, except certain structures (outliers, chaining, small clusters) where specific challenges persist. Computational efficiency varies, with methods involving principal curve fits being slower. The paper recommends multimodality tests on pairwise distances or principal components as general-purpose clusterability measures, cautions against using classic clustering scores for this purpose, and provides practical software and decision guidelines for users. It highlights ongoing challenges in high-dimensional, noisy, or structured data, and proposes avenues for new methods better tailored to such cases."
60,"[N. Wiroonsri, ""Clustering performance analysis using a new correlation-based cluster validity index,"" Pattern Recognition, vol. 145, p. 109910, Jan. 2024. [Online]. Available: https://doi.org/10.1016/j.patcog.2023.109910]","There are various cluster validity indices used for evaluating clustering results, primarily to determine the optimalyet typically unknownnumber of clusters in data. While certain indices perform well with clusters of differing densities, sizes, and shapes, they often have the limitation of suggesting only a single optimal number, which might not reflect the true structure in complex real-world scenarios. This work proposes a novel cluster validity index derived from the correlation between the actual distance of data point pairs and the centroid distance of their respective clusters. Unlike previous indices, the proposed method consistently produces several local peaks, thus addressing the limitation of singular cluster count recommendations. Comparative experiments using a variety of scenarios, including real-world datasets from UCI, demonstrate the strengths of this new validity index over several established alternatives."
61,"[Y. Pang, X. Zhou, J. Zhang, Q. Sun, and J. Zheng, ""Hierarchical electricity time series prediction with cluster analysis and sparse penalty,"" Pattern Recognition, vol. 126, p. 108599, Jun. 2022. [Online]. Available: https://doi.org/10.1016/j.patcog.2022.108599]","In big data applications, hierarchical time series prediction is crucial for decision-making, with aggregation consistency typically maintained via reconciliation methods. This paper introduces a novel framework for hierarchical electricity time series prediction that leverages multiple alternative clusterings. Rather than passively adopting aggregation consistency, the method uses time series mining to construct hierarchies before applying an optimal reconciliation process to boost prediction accuracy. Specifically, the approach repeatedly employs the $k$-means clustering algorithm with varying $k$ values to generate numerous time series clusters (patterns), and constructs hierarchical structures from these clusters. These cluster-based hierarchies, alongside the original geographical hierarchy, feed into an optimal aggregation consistency reconciliation method for forecasting. Additionally, a sparse penalty mechanism is introduced for ""ideal"" cluster selection, bolstering prediction performance. On real-world electricity load and solar power datasets, the proposed approach achieves one-step ahead forecast improvements of $11.13\%$ and $24.07\%$ over state-of-the-art methods, respectively."
62,"[J. Yang and C.-T. Lin, ""Autonomous clustering by fast find of mass and distance peaks,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 1, pp. 1-14, 2025. [Online]. Available: https://ieeexplore.ieee.org/document/10856563]","This paper presents the torque clustering algorithm, a novel and fully parameter-free approach that autonomously recognizes various cluster types, determines the optimal number of clusters, and identifies noise without requiring user input. The algorithm is based on a natural principle: a cluster and its nearest neighbor with higher 'mass' are merged, allowing for robust and adaptable clustering outcomes. Tested on 1,000 diverse datasets, the method achieved an average adjusted mutual information score of 97.7%, substantially outperforming other state-of-the-art clustering techniques that average around 80%. These results indicate its effectiveness and promise for enhancing autonomous artificial intelligence systems, facilitating the development of adaptable AI with minimal human intervention. The torque clustering algorithm is anticipated to have broad impact across scientific and technical fields by offering a breakthrough in unsupervised learning."
63,"[N. Phelps and A. Metzler, ""An exploratory clustering analysis of the 2016 National Financial Well-Being Survey,"" PLoS ONE, vol. 19, no. 9, e0309260, 2024. Available: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309260]","This study applies cluster analysis to data from the 2016 National Financial Well-Being Survey in the U.S., aiming to reveal nuanced financial subpopulations beyond typical demographic splits. Using K-means clustering on scores for financial well-being (FWB), financial knowledge (FK), and financial skill (FS), the authors identify four distinct groups: (1) financially comfortable, skilled, and knowledgeable; (2) financially coping, somewhat skilled, and knowledgeable; (3) financially coping, fairly skilled, and unknowledgeable; and (4) financially stressed, unskilled, and unknowledgeable. Notably, Groups 2 and 3 exhibit nearly identical FWB but sharply differing objective financial situations (OFS)for example, Group 3 has three times the rate of financial fragility as Group 2demonstrating a systematic bias between subjective and objective financial constructs. This finding challenges the notion that FWB can substitute for OFS, emphasizing the need to consider both to accurately assess financial resilience or subjective well-being. The analysis further uncovers a meaningful disconnect between self-assessed knowledge (SAK) and actual FK, particularly for Group 3, where overconfidence is pronounced. The results suggest existing conceptual models may underestimate the importance of FK in determining objective financial outcomes. The lack of clear cluster boundaries presents challenges for targeted interventions, and future research using alternative clustering methods or regression analyses is recommended to further explore relationships among the financial constructs. The study underscores the risk of policy missteps when failing to distinguish between objective and subjective financial measures and demonstrates the value of cluster analysis for generating hypothesis and insights from large-scale survey data."
64,"[M. Z. Rodriguez, C. H. Comin, D. Casanova, O. M. Bruno, D. R. Amancio, L. da F. Costa, and F. A. Rodrigues, ""Clustering algorithms: A comparative approach,"" PLoS ONE, vol. 14, no. 1, e0210236, 2019. Available: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210236]","This paper provides a systematic comparison of nine prominent clustering algorithms implemented in R, using 400 artificially generated, normally distributed datasets varying in number of classes ($C$), features ($F$), objects per class ($N_e$), and inter-class separation. Performance was evaluated using external indicesJaccard, Adjusted Rand Index (ARI), Fowlkes Mallows, Normalized Mutual Information (NMI)and internal indices such as Silhouette and Dunn. The findings show that spectral clustering consistently excels in high-dimensional scenarios (e.g., for $F=200$, ARI up to $68.16\%$), especially with default parameters, while hierarchical methods often struggle unless finely tuned. Adjusting key parameters (even a single one, such as EM's `modelName` or k-means number of starts) can yield substantial gains (up to $39.1\%$ improvement for some algorithms). Randomized parameter sampling outperforms default settings for many methods but introduces computational cost considerations. The study concludes that both algorithm choice and careful parameter selection critically impact clustering outcomes, providing practical guidance for R users, though cautions about generalizability beyond high-dimensional, normally distributed, synthetic data. Extensions could include exploring other clustering algorithms, distributions, and semi-supervised approaches."
65,"[E. J. Aguilar and V. C. Barbosa, ""Shape complexity in cluster analysis,"" PLoS ONE, vol. 18, no. 5, e0286312, 2023. Available: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0286312]","This paper introduces a novel approach to determining data scaling factors for cluster analysis by leveraging the concept of multidimensional shape complexity, originally introduced in cosmology, to guide preprocessing before employing clustering algorithms such as k-means. Traditionally, data are scaled using statistical measures like standard deviation ($1/\sigma_k$) or pooled standard deviation along each dimension; however, the authors formulate a constrained nonlinear programming problem (Problem P) that seeks scaling factors $\alpha_k$ such that scaling each feature by $\alpha_k/\sigma_k$ optimally balances intra- and inter-cluster distances, focusing particularly on midrange distances that may confuse clustering. Multiple candidate scaling sets are generated through random initializations and local minimizations of the shape complexity (SC) function, and their impact is evaluated using benchmarks like the Iris and BCW-Diag-10 datasets via measures such as Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI). Results demonstrate that the best set of $\alpha_k$ values frequently yields better clustering performance than traditional scaling methods, although both expert visual inspection and domain knowledge may be necessary for ambiguous cases. The approach enables more nuanced cluster discovery but may be computationally intensive for large or high-dimensional datasets; future work will focus on scalable extensions, alternate optimization formulations, and more automated guidance for selecting the optimal scaling."
66,"[Z. Gniazdowski, ""New Approach to Clustering Random Attributes,"" Zeszyty Naukowe WWSI, no. 31, vol. 19, pp. 41-90, 2024. [Online]. Available: https://arxiv.org/abs/2412.09748. doi:10.48550/arXiv.2412.09748]","This paper introduces a novel method for similarity analysis and clustering of both numerical and nominal random attributes, hinging on the crucial step of encoding nominal values into a numerical format to enable their inclusion in quantitative analyses. The process involves representing data as an $m \times n$ table, encoding nominal attributes, calculating correlations, performing factor analysis, rotating factors, and finally clustering attributes based on their similarity to the extracted factorsthus allowing clusters to comprise both original numerical and encoded nominal attributes. The method was rigorously tested on several benchmark datasets (Simple Weather Forecast, Mushroom, Automobile, Breast Cancer), with detailed procedures for preprocessing, factor selection, application of the clustering algorithm, and analysis of results provided, using tables and statistical summaries to underline the methods universality. Notably, the algorithm accommodates situations where traditional statistical concepts (e.g., standardization, correlation) may be ill-defined for non-normally distributed or highly categorical data, yet still yields interpretable and actionable clusters. Challenges discussed include optimal encoding of nominal data, handling of attributes with equicardinal classes, and interpretation limitations outside of normal data settings. The authors propose further exploration of graph-based relationships and real-world mixed-type datasets, especially in surveys and social research. The work underscores that while classical interpretations may not always be appropriate, the presented approach consistently produces clusters meaningful across diverse data types, extending the toolkit for attribute clustering in complex, mixed-type tables."
67,"[J. Paparrizos, F. Yang, and H. Li, ""Bridging the Gap: A Decade Review of Time-Series Clustering Methods,"" arXiv preprint arXiv:2412.20582, 2024. [Online]. Available: https://arxiv.org/abs/2412.20582]","Time series are foundational sequential data representations with applications spanning computer science, biology, geology, astronomy, and environmental sciences, where advanced sensing and storage have produced high-dimensional datasets requiring sophisticated analysis for latent structure discovery. This comprehensive survey unifies the taxonomy of time-series clustering, bridging classic methods with newer deep learning-based algorithms. Four primary methodological categories are detailed: distance-based, distribution-based, subsequence-based, and representation-learning-based approaches. Each method is discussed in terms of technical background, representative algorithms, mathematical formulations, and summarized in concise tables. Evaluation methodologies leverage both external and internal indices such as purity, Rand index, adjusted Rand index, NMI, silhouette coefficient, Davies-Bouldin index, Dunn index, and WCSS, with practical comparative examples. Key challenges include the curse of dimensionality, handling noisy/missing data, determining optimal similarity measures, computational complexity, and the inadequacy of universal evaluation metrics. Over 100 clustering algorithms are organized into a two-level taxonomy (4 main and 10 secondary categories), with analysis emphasizing the importance of the chosen dissimilarity measures and representations for clustering accuracy and efficiency. No method universally outperforms others across datasets, as shown in recent benchmarks and the Odyssey evaluation engine. Partitional methods, such as k-Means, optimize for the balance between accuracy and runtime, while hierarchical approaches allow flexible resolution; distribution-based (e.g., Hidden Markov Models) and subsequence-focused methods offer robustness to noise and applicability to varied data modalities. Deep learning and representation learning have introduced powerful, unsupervised dimensionality reduction and feature extraction, demonstrating growing efficacy. Nonetheless, unresolved issues remain for scalable, interpretable, and adaptive time-series clustering under real-world constraints. The survey thus establishes a foundational, comparative perspective to guide ongoing research and refinement in this domain, underlining the necessity to balance accuracy, computational cost, and domain applicability in future algorithm design."
68,"[A. J. Gallego, J. R. Rico-Juan, and J. J. Valero-Mas, ""Efficient k-nearest neighbor search based on clustering and adaptive k values,"" Pattern Recognition, vol. 122, pp. 108356, Feb. 2022. [Online]. Available: https://doi.org/10.1016/j.patcog.2021.108356]","This paper introduces caKD+, a novel algorithm for efficient and accurate k-nearest neighbor (kNN) search and classification in large-scale and high-dimensional datasets. The approach integrates three main innovations: (1) feature learning (e.g., PCA, LDA) to obtain more compact representations, (2) fast clustering and hierarchical indexing of the database using techniques such as k-means, and (3) adaptive, query-dependent selection of k based on local data density and intra-cluster properties, sometimes using cross-validation for optimality. Extensive experiments on ten varied datasets (like MNIST, CIFAR-10, ImageNet subsets) demonstrate that caKD+ achieves 210x speedups over 16 state-of-the-art baseline methods with equal or better classification accuracy, often accessing only 520% of the database per query. The methods combination of automatic parameter selection, cluster-based narrowing, and parallelizable implementation enables robust sublinear query times and moderate memory use, with ablation tests confirming the value of all components. Identified challenges include maintaining clustering quality in complex data and choosing cluster numbers and adaptive k optimally; future work aims to improve deep feature learning and distributed scalability. Overall, caKD+ sets a new standard for scalable and accurate kNN search and classification, as evidenced by rigorous empirical evaluation and public release of code."
69,"[B. Tang, H. He, and S. Zhang, ""MCENN: A variant of extended nearest neighbor method for pattern recognition,"" Pattern Recognition Letters, vol. 133, pp. 116–122, May 2020. [Online]. Available: https://doi.org/10.1016/j.patrec.2020.01.015]","Recent studies have shown that the extended nearest neighbor (ENN) method improves classification performance over traditional k-nearest neighbor (KNN) methods by employing a two-way communication decision process, maximizing intra-class coherence gain across the data set. However, ENN remains an instance-based algorithm without a training stage. This paper introduces Maximum intra-class Coherence Extended Nearest Neighbor (MCENN), a variant that incorporates distances to neighbors in the decision process and utilizes a novel distance metric learning algorithm to learn an optimal linear transformation in a dedicated training stage, thereby maximizing the intra-class coherence of training data. The proposed MCENN approach demonstrates improved discriminative performance for pattern recognition, as validated by experiments on real-world data sets."
70,"[Y. Zhang, M. Xiang, and B. Yang, ""Graph regularized nonnegative sparse coding using incoherent dictionary for approximate nearest neighbor search,"" Pattern Recognition, vol. 70, pp. 75–88, Oct. 2017. [Online]. Available: https://doi.org/10.1016/j.patcog.2017.05.004]","This paper introduces a novel method for approximate nearest neighbor (ANN) retrieval leveraging graph regularized nonnegative sparse coding with an incoherence constraint on the dictionary. The method formulates the sparse coding problem as $\min_{D \ge 0,\, S \ge 0} \|X-DS\|_F^2 + \lambda_1 \|S\|_1 + \lambda_2 \operatorname{Tr}(SLS^T) + \lambda_3 \operatorname{Tr}(D^TLD) + \lambda_4 \|D^T D - I\|_F^2$, where $L$ is a graph Laplacian preserving local geometry and the incoherence penalty $\|D^T D - I\|_F^2$ encourages diversity among dictionary atoms. Both the dictionary $D$ and sparse codes $S$ are constrained to be nonnegative, enhancing interpretability and sparsity. The authors derive an efficient iterative algorithm alternating projected gradient updates for $D$ and $S$. Experiments on SIFT1M, MNIST, and CIFAR-10 demonstrate that their approach (GNNSC-ID) outperforms k-means, product quantization, and other sparse coding baselines in recall@R, e.g., achieving Recall@1 of 0.35 versus 0.29 (PQ) and 0.31 (GSC) on SIFT1M at $k$=256. Analysis highlights the benefits of manifold-preserving regularization and incoherent codebooks, with robustness to regularization hyperparameters but computational challenges for very large datasets due to graph construction. Future work is aimed at improving scalability and optimization efficiency."
71,"[S. Uddin, I. Haque, H. Lu, M. A. Moni, and E. Gide, “Comparative performance analysis of K-nearest neighbour (KNN) algorithm and its different variants for disease prediction,” Scientific Reports, vol. 12, Art. no. 6256, 2022. [Online]. Available: https://www.nature.com/articles/s41598-022-10358-x]","This paper comprehensively evaluates various K-nearest neighbour (KNN) algorithm variantsincluding classic KNN, adaptive, locally adaptive, k-means clustering, fuzzy, mutual, ensemble, Hassanat, and generalised mean distance KNNfor disease risk prediction using eight benchmark datasets from Kaggle, UCI, and OpenML. The study uses accuracy, precision, and recall as its primary performance metrics, calculated respectively as $ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $, $ \text{Precision} = \frac{TP}{TP + FP} $, and $ \text{Recall} = \frac{TP}{TP + FN} $. A Relative Performance Index (RPI) is additionally proposed for nuanced comparison: $ \text{RPI} = \frac{\sum (a_i - a_i^*)}{d} $, where $a_i$ is the performance of a variant on dataset $i$, $a_i^*$ is the minimum across variants for dataset $i$, and $d$ is the number of datasets. Results show that Hassanat KNN achieved the highest average accuracy (83.62%), the ensemble approach KNN excelled in precision (82.88%) and performed well overall, and generalised mean distance KNN led in recall (76.84%). Statistical analysis found no significant group-wise differences (ANOVA, $p > 0.05$). The discussion highlights that most KNN variants outperform the classic approach, though each has distinct limitations, such as sensitivity to parameter $k$ and data noise. The conclusion recommends the ensemble approach KNN for its consistently high performance across measures, while also recognizing the adaptability and domain relevance of other top variants. The study suggests further exploration of hybrid and advanced KNN designs, emphasizing their potential in robust disease risk analytics for healthcare."
72,"[A. Ali, Z. Khan, H. Du, and S. Aldahmani, “Double weighted k nearest neighbours for binary classification of high dimensional genomic data,” Scientific Reports, vol. 15, Art. no. 12681, 2025. [Online]. Available: https://www.nature.com/articles/s41598-025-97505-2]","This paper introduces the Double Weighted k Nearest Neighbors (DWkNN) method for improving binary classification of high-dimensional gene expression data, where the number of genes (features) greatly exceeds the sample count. Unlike standard kNN, DWkNN assigns feature weights based on each genes ability to differentiate between classes, robustly defining gene weights $\rho_i$ using correctly classified sample proportions via interquartile analysis, and computes a weighted Euclidean distance: $d_w(X^j, X') = \sqrt{\sum w_i(x_{ij} - x'_i)^2}$. For a test point, DWkNN locates the $k$ nearest neighbors using this metric, calculates an exponential neighbor weight $\exp[-d_w(X^j, X')]$ for each, and sums these for each class, assigning the class with the larger sum. Evaluations on seven benchmark datasets, across various train/test splits and compared to standard kNN, weighted kNN (WkNN), RkNN, ExNRule, kCNN, EkCNN, and SVM, demonstrate that DWkNN consistently delivers superior accuracy (e.g., 0.991 on D2, 0.951 on D3 for 70/30 split), Cohens kappa, sensitivity, and F1-score, while maintaining high computational efficiency (average runtime 0.357 seconds versus WkNNs 12.258 seconds). DWkNN achieves stable, robust performance across splits and varying $k$, outperforming competing methods in both predictive power and speed. The methods effectiveness is particularly notable in scenarios dominated by high-dimensional, small-sample data, though it may face challenges where class separation is extremely subtle or features are categorical. Overall, DWkNN offers a robust, efficient, and generalizable classification approach for gene expression and other high-dimensional datasets, with future work to broaden applicability beyond current binary and continuous feature restrictions."
73,"[R. K. Halder, M. N. Uddin, M. A. Uddin, S. Aryal, and A. Khraisat, ""Enhancing K-nearest neighbor algorithm: a comprehensive review and performance analysis of modifications,"" Journal of Big Data, vol. 11, no. 1, Article no. 113, 2024. [Online]. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00973-y]","The paper provides a comprehensive survey and performance analysis of exact k-Nearest Neighbors (kNN) techniques, focusing on both kNN search and kNN join methods, especially for high-dimensional data scenarios. By reviewing 31 kNN search and 12 kNN join algorithms, the authors offer methodological overviews, analytical insights, and side-by-side comparative tables (provided inline via LaTeX tabular), highlighting each method's strengths, limitations, computational complexity, empirical performance, and tested datasets. The selection process followed a systematic literature review protocol (PRISMA), ultimately synthesizing works from 1967 to 2022 across multiple paradigms: I/O-based, main-memory-based, and parallel/distributed settings. Key challenges identified include optimal $k$ selection, the curse of dimensionality, handling noise/outliers, scalability, and efficiency for big data. The discussion navigates such issues with strategies like feature selection, adaptive algorithms, parallelization, and ensemble techniques. Practical applications span data mining, anomaly detection, recommendation, IoT, robotics, and industrial analytics. To address research gaps, the authors propose a novel Region-Based Neighbors Searching Classification Algorithm, aiming for robust, efficient classification in large-scale, high-dimensional or streaming environments. The study concludes that modern exact kNN methods are progressing rapidly in tackling dimensionality, computation, and accuracy, signifying a transformative era for scalable, real-time kNN-based data processing."
74,"[A. A. Amer, S. D. Ravana, and R. A. A. Habeeb, ""Effective k-nearest neighbor models for data classification enhancement,"" Journal of Big Data, vol. 12, Article no. 86, 2025. [Online]. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-025-01137-2]","This paper addresses the challenge of imbalanced class distributions and class overlaps in classification tasks by introducing three enhanced k-nearest neighbor (kNN) models: PRkNN, EPRkNN, and WPRkNN. Central to these models is the novel proximal ratio (PR), which quantifies local class consistency by computing, for each data point, the ratio of its same-class neighbors within a class-dependent radius to the total neighbors in that radiusoutliers and noisy points thus receive reduced influence in classification decisions. These PR-based weights are further combined with new schemes to address class imbalance, resulting in models that directly and simultaneously mitigate both overlap and imbalance, such as WPRkNN, which employs a composite weighting of PR scores, inverse distances, and neighbor class proportions. Extensive experimentation across 52 diverse datasetsevaluated via accuracy, F1, AUROC, sensitivity, specificity, and geometric meandemonstrates these models, particularly WPRkNN, consistently outperform or rival state-of-the-art kNN and machine learning classifiers, especially on imbalanced, noisy, and time-series data, with stable performance across k values and robust handling of outliers. While WPRkNN is somewhat less efficient computationally than basic kNNs, its reliability and lack of parameter tuning make it suitable for practical applications in domains like fraud and intrusion detection. The authors note future work will target developing even more efficient variants for large or noisier datasets, aiming to optimize the trade-off between runtime and classification accuracy."
75,"[G. Chatzigeorgakidis, S. Karagiorgou, S. Athanasiou, and S. Skiadopoulos, ""FML-kNN: scalable machine learning on Big Data using k-nearest neighbor joins,"" Journal of Big Data, vol. 5, Article no. 4, 2018. [Online]. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0115-x]","Efficient management and analysis of large data volumes is increasingly important, prompting the development of distributed computing frameworks such as MapReduce, Apache Hadoop, Spark, and Flink. This article introduces FML-kNN, a novel distributed framework leveraging Apache Flink to perform probabilistic classification and regression via a k-nearest neighbor (kNN) joins algorithm, capable of operating on massive datasets of varying granularity and dimensionality in a single distributed session. Unlike traditional approaches requiring multiple sessions, FML-kNN unifies all processing stages, thus reducing initialization overheads and I/O costs. The key methodological challenge is efficiently computing kNN joins at scale, contending with the curse of dimensionality and space filling curve (SFC) approximation errors, which the framework mitigates through dataset shifting and optimized data partitioning. Experimental evaluations demonstrate FML-kNNs superior wall-clock completion time and scalability compared to Spark and Hadoop implementations, with quantitative benchmarks validating its performance. Two real-world use casesshort-term water consumption forecasting from smart meter data and consumer characteristic extraction from shower usageunderscore its practical value in knowledge extraction from large-scale data. FML-kNN's strengths in single-session execution, scalability, and practical insight generation are discussed, alongside future plans to support streaming data and extend distributed machine learning tasks. All code and data are publicly available, fostering reproducibility and further research."
76,"[Sarita de Berg and Frank Staals, “Nearest Neighbor Searching in a Dynamic Simple Polygon,” presented at the Symposium on Computational Geometry (SoCG) 2025, arXiv preprint arXiv:2503.03435, 22 pages, 2025. [Online]. Available: https://arxiv.org/abs/2503.03435]","This paper introduces the first dynamic data structure enabling efficient nearest neighbor queries in a dynamic simple polygon domain, where both the set of point sites $S$ and the polygon $P$ itself are subject to insertions. The authors address the challenge that modifying the polygon can globally alter shortest path metrics, complicating traditional data structures. They present a solution using balanced geodesic triangulations, incorporating two main components: a Dynamic Shortest Path data structure (building on Goodrich and Tamassia) for accommodating new polygon vertices, and a Cone Query structure for rapid nearest neighbor search within subpolygons. The data structure supports updates such as inserting barrier segments into $P$ and adding new sites, with space complexity $O(n(\log\log n + \log m) + m)$, amortized update time $\tilde{O}(n^{3/4} + m^{3/4})$, and expected query time $\tilde{O}(n^{3/4} + n^{1/4}m^{1/2})$, where $n$ is the number of sites and $m$ the complexity of $P$. While the times are not yet polylogarithmic, they match or improve upon bounds for related dynamic polygon problems. The work opens avenues for supporting site deletions and further generalizing to more dynamic domain changes."
77,"[Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, and Peng Cheng, “VSAG: An Optimized Search Framework for Graph-based Approximate Nearest Neighbor Search,” arXiv preprint arXiv:2503.17911, 16 pages, 2025. [Online]. Available: https://arxiv.org/abs/2503.17911]","Approximate nearest neighbor search (ANNS) in vector databases is hindered by random memory access, computational overhead, and costly parameter tuning in graph-based approaches. The authors present VSAG, an open-source framework optimized for in-production graph-based ANNS, deployed at Ant Group. VSAG integrates three core enhancements: (i) memory access optimization using software prefetching, deterministic access greedy search, and cache-aware vector layouts to greatly reduce L3 cache misses; (ii) automated parameter tuning through a three-level mechanism that selects parameters efficiently without index re-building, as detailed in their proof-of-concept (see Appendix D), reducing tuning costs dramatically; and (iii) efficient distance computation leveraging integer and low-precision representations, scalar quantization, and selective re-ranking for performance improvement. Experimental evaluation on real-world datasets demonstrates up to 4x speedup over the industry-standard HNSWlib with equivalent accuracy. The results are substantiated by ablation studies, cache miss analyses, and real-world case deployments. The authors conclude that VSAG significantly enhances graph-based ANNS on both efficiency and tuning cost, and future work may extend its principles to wider frameworks and evolving hardware."
78,"[Mingyu Yang, Wentao Li, and Wei Wang, “Fast High-dimensional Approximate Nearest Neighbor Search with Efficient Index Time and Space,” arXiv preprint arXiv:2411.06158, 8 pages, version 3, 2025. [Online]. Available: https://arxiv.org/abs/2411.06158]","This paper presents Minimization Residual Quantization (MRQ), an innovative method for Approximate K Nearest Neighbor (AKNN) search in high-dimensional Euclidean spaces that addresses the limitations of prior quantization techniques such as RabitQspecifically, the inflexible requirement for quantization bits to match vector dimensions, resulting in a fixed compression ratio. MRQ exploits the long-tailed variance distribution observed in datasets after PCA by prioritizing quantization along high-variance dimensions and handling remaining low-variance dimensions via residual correction. The method utilizes a multi-stage distance correction framework: (1) initial vector quantization to compact codes, (2) projection to a lower-dimensional PCA space for capturing major variance, and (3) precise computation using residual components only as needed, with error bounds managed through statistical inequalities (e.g., Chebyshev's). Experimental results show that MRQ significantly outperforms state-of-the-art graph- and quantization-based AKNN approaches, delivering up to 3x faster search with only 1/3 the quantized code length while retaining accuracy. This adaptive and efficient design makes MRQ especially well-suited to resource-constrained and high-dimensional scenarios, advancing both the speed and precision of large-scale AKNN search."
79,"[Jean-Daniel Boissonnat, Karthik C. S., and Sébastien Tavenas, ""Building Efficient and Compact Data Structures for Simplicial Complexes,"" Algorithmica, vol. 79, no. 2, pp. 530-567, Nov. 2017. [Online]. Available: https://doi.org/10.1007/s00453-017-0373-8]","The paper addresses efficient representation and compression of simplicial complexes, focusing on the Simplex Tree (ST) data structure and its optimally compressed variant, achieved via DFA minimizationtermed the Minimal Simplex Automaton (MSA)which shares identical subtrees to minimize storage while preserving functionality. Additionally, two new structures are proposed: the Maximal Simplex Tree (MxST), which stores only prefixes of maximal simplices for greater compactness, and the Simplex Array List (SAL), leveraging array intersections on edges for efficient set-based queries. Theoretical analysis shows that ST requires $\Theta(m \log n)$ bits, while C(ST) and MSA achieve up to order-of-magnitude compression depending on redundancy and vertex labeling, the latter influencing compression significantly but optimally labeling being NP-hard. MxST uses $O(kd \log n)$ space by focusing on maximal simplices, and SAL, with $O(kd^2(d + \log n + \log k))$ space, offers efficient and relabeling-invariant operations, excelling with few maximal simplices or low dimension. Experimental validation with Rips complexes and random graphs confirms substantial real-world compression. The discussion highlights space-query tradeoffs: C(ST) excels in static, highly compressible settings; MxST is ideal for compactness in sparse complexes; SAL offers efficient dynamic operations. Challenges include high-dimensional combinatorics, storage redundancy, and the intrinsic complexity of optimal labeling and minimization. Future work seeks approximation algorithms, improved dynamic structures, tighter compression bounds, and approximate (error-tolerant) formats. The results underscore the effectiveness of automaton-based compression and introduce foundations for practical and theoretical advances in topological data analysis."
80,"[Alberto Policriti and Nicola Prezza, ""LZ77 Computation Based on the Run-Length Encoded BWT,"" Algorithmica, vol. 80, no. 7, pp. 1986-2011, July 2018. [Online]. Available: https://doi.org/10.1007/s00453-017-0379-2]","This paper addresses the challenge of computing the LZ77 factorization for text compression using working space significantly smaller than existing solutions by leveraging the number \( r \) of equal-letter runs in the BurrowsWheeler Transform (BWT) of the reversed text, which, like the LZ77 factor count \( z \), can be exponentially smaller than text length \( n \). The authors introduce two algorithms that compute the LZ77 factorization in \(\mathcal{O}(r\log n)\) bits of space and \(\mathcal{O}(n\log r)\) time, mainly by sampling BWT runs and maintaining a constant number of memory words per run. Their methods enable direct conversion from run-length BWT (RLBWT) to LZ77 without full decompression, and yield asymptotically optimal algorithms for constructing self-indexes on repetitive inputs. Experiments using Wikipedia, software history, and DNA datasets show these algorithms use as little as 1% of dataset size in working space, achieving space efficiency two to three orders of magnitude better than entropy-compressed or suffix-array based alternatives (see inline Table 2 and Figure 4 in the paper). However, dynamic run-length encoded structures, though theoretically optimal, limit practical speed. The paper leaves open several avenues for future work, including constructing full-text compressed indexes in \(\mathcal{O}(r)\) space with locate support, and closing the performance gap of dynamic data structures. These contributions advance both the theory and practical engineering of highly repetitive text compression and indexing."
81,"[Johannes Fischer, Tomohiro I, Dominik Köppl, and Kunihiko Sadakane, ""Lempel-Ziv Factorization Powered by Space Efficient Suffix Trees,"" Algorithmica, vol. 80, no. 7, pp. 2048-2081, July 2018. [Online]. Available: https://doi.org/10.1007/s00453-017-0354-y]","This paper presents unified algorithms for the online computation of LempelZiv-77 (LZ77) and LempelZiv-78 (LZ78) factorizations of a text of length $n$ over an integer alphabet of size $\sigma$, achieving linear time and using $O(n \log \sigma)$ bits of working space. The methods leverage compressed suffix trees (CSTs) and introduce a novel witness structure to efficiently navigate ancestry and phrase boundaries during factorization. For each input position, the witness structure records the deepest ancestor relevant for determining factor matches (LZ77) or phrase creation (LZ78), combining bit vector marking and CST traversal. Empirical results indicate that these algorithms improve practical space usage compared to classical pointer-based approaches, particularly yielding the first linear-time, online LZ78 solution in sublinear space. The approach unifies and simplifies prior CST-based and suffix tree-based strategies, and extensions to overlapping LZSS factorizations and other text analytics are discussed. Theoretical analysis confirms tight bounds, while practical variants accommodate both succinct and pointer-based representations, making the solutions suitable for large-scale, repetitive datasets such as genomic sequences."
82,"[Huacheng Yu, ""Nearly Optimal Static Las Vegas Succinct Dictionary,"" SIAM Journal on Computing, vol. 51, no. 3, pp. 20-174, 2022. [Online]. Available: https://dblp.org/rec/journals/siamcomp/Yu22]","This paper introduces a static data structure for storing an $n$-element subset $S$ of a universe $U = [u] = \{0, 1, ..., u-1\}$ that supports always-correct (Las Vegas) membership queries. The structure achieves, for any $n \leq \beta u$ (with constant $\beta < 1$), $O(1)$ expected query time using only $O(n \log \log(u/n)) + O(n)$ bits, nearly matching the information-theoretic lower bound $B(n, u) = \log_2 \binom{u}{n} \approx n \log_2(u/n) + O(n)$. By employing a two-level hashing scheme and succinct representations, carefully selecting bucket parameters, and using new parameter-counting probabilistic arguments, the structure guarantees injective hashing and compact encoding for all bucket sizes. For general $n$ and $u \geq n$, query times are $O(\log^* n)$ in exchange for retaining the same succinct space guarantee. This Las Vegas construction addresses technical hurdles unresolved by prior work, ensuring always-correct queries without sacrificing space or relying on non-standard assumptions. Future directions include eliminating the $O(n)$ additive term, derandomizing the scheme, and expanding support to dynamic or more complex query types."
83,"[Kasper Green Larsen, Omri Weinstein, and Huacheng Yu, ""Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure Lower Bounds,"" SIAM Journal on Computing, vol. 49, no. 5, 2020. [Online]. Available: https://dblp.org/rec/journals/siamcomp/LarsenWY20]","This paper establishes the first super-logarithmic lower bounds on the cell probe complexity for dynamic boolean (decision) data structure problems, achieving a milestone in this field. The authors introduce a novel technique for proving such lower bounds, resulting in a $\tilde{\Omega}(\log^{1.5} n)$ bound on the operation time for various boolean data structure problemsmost notably, dynamic range counting over $\mathbb{F}_2$. This resolves a key open problem by showing an $\omega(\lg n)$ lower bound for dynamic range counting, as referenced in the obituary of Mihai Patrascu, and also extends to the classical 2D range counting problem, fundamental to computational geometry and spatial databases. Similar bounds are derived for boolean versions of dynamic polynomial evaluation, 2D rectangle stabbing, and for non-boolean problems such as range selection and range median. The technical innovation lies in ""weakly"" simulating dynamic data structures using efficient one-way communication protocols that have a slight advantage over random guessing, involving techniques from low-degree (Chebyshev) polynomials and presenting a new perspective on the ""cell sampling"" method of Panigrahy et al."
84,"[Sayan Bhattacharya, Monika Henzinger, and Giuseppe F. Italiano, ""Deterministic Fully Dynamic Data Structures for Vertex Cover and Matching,"" SIAM Journal on Computing, vol. 47, no. 3, pp. 859-887, 2018. [Online]. Available: https://dblp.org/rec/journals/siamcomp/BhattacharyaHI18]","We present the first deterministic data structures for maintaining approximate minimum vertex cover and maximum matching in a fully dynamic graph $G = (V,E)$, with $|V| = n$ and $|E| =m$, in $o(\sqrt{m}\,)$ time per update. In particular, for minimum vertex cover we provide deterministic data structures for maintaining a $(2+\eps)$ approximation in $O(\log n/\eps^2)$ amortized time per update. For maximum matching, we show how to maintain a $(3+\eps)$ approximation in $O(\min(\sqrt{n}/\epsilon, m^{1/3}/\eps^2))$ \emph{amortized} time per update, and a $(4+\eps)$ approximation in $O(m^{1/3}/\eps^2)$ \emph{worst-case} time per update. Our data structure for fully dynamic minimum vertex cover is essentially near-optimal and settles an open problem by Onak and Rubinfeld from STOC' 2010."
85,"[Z. Yuan, C. L. Philip Chen, ""Forgetful Forests: Data Structures for Machine Learning on Data Streams with Incremental Computation and Filtering,"" Algorithms, vol. 16, no. 6, Art. no. 278, 2023. [Online]. Available: https://www.mdpi.com/1999-4893/16/6/278]","Database research offers contributions to machine learning, particularly through improved data structures. This paper introduces ""forgetful"" tree-based learning algorithms that combine incremental computation with sequential and probabilistic filtering, enabling effective adaptation to concept driftwhere the mapping from input to classification evolves over time. The proposed algorithms deliver high time efficiency: up to 24 times faster than leading incremental methods with, in the worst case, only a 2% decrease in accuracy, or achieving at least double the speed without sacrificing accuracy. These properties make the algorithms especially suitable for high-volume streaming data applications."
86,"[M. Aleksandrov, P. J. Prentice, F. Wereszczuk, ""Voxelisation Algorithms and Data Structures: A Review,"" Sensors, vol. 21, no. 24, Art. no. 8241, 2021. [Online]. Available: https://www.mdpi.com/1424-8220/21/24/8241]","This paper reviews voxel-based data structures, algorithms, and frameworks leveraged across domains such as computer graphics, medicine, geosciences, and simulation, focusing on effective and unified digital representations for complex 3D tasks. It systematically compares voxelisation algorithms targeting various geometric primitivespoints, lines, triangles, surfaces, and solidsincluding scanline, ray casting, and rasterisation techniques, evaluating them using properties like connectivity, computational efficiency, and memory requirements. Notably, the research distinguishes between static data structures (e.g., regular grids, Sparse Voxel OctreesSVO, SVDAG, SSVDAG) and state-of-the-art dynamic structures (OpenVDB, NanoVDB, SPGrid, DT-Grid), summarizing their features in comparative tables and highlighting NanoVDB for superior speed and out-of-core capability. Surface voxelisation is emphasized as preferable to solid voxelisation for its speed and memory efficiency in sparse representations. The discussion stresses advances in GPU-accelerated methods and memory-efficient representations, the trade-offs between voxel connectivity, and the limitations posed by non-watertight models and lack of robust standard implementations. The major challenges identified include the scarcity of tested algorithm libraries, difficulties handling non-watertight and semantically rich models, and efficiency at extreme resolutions. Future work calls for the creation of open-source libraries, standard benchmarks, and advanced GPU-ready data structures, as well as targeted efforts to improve support for semantics and dynamic large-scale scenes. The study concludes that continued tool development and integration with modern databases and APIs are vital for enabling high-performance, robust voxel-based applications across disciplines."
87,"[J. E. Schmitz, J. Zentgraf, and S. Rahmann, ""Smaller and More Flexible Cuckoo Filters,"" arXiv preprint arXiv:2505.05847 [cs.DS], 2025. [Online]. Available: https://arxiv.org/abs/2505.05847]","Cuckoo filters are space-efficient approximate set membership data structures that, unlike Bloom filters, store multi-bit fingerprints of keys in a hash table via variants of Cuckoo hashing, allowing controllable false positive rates (FPR) and no false negatives. Classic designs require fingerprint sizes of $(k+3)$ bits per key and incur extra overhead, reaching up to $2.1(1+3/k)kn$ bits due to the restriction that the number of hash buckets be a power of 2. This paper introduces two major improvements: it removes the power-of-2 bucket constraint with a signed-offset addressing scheme, and it switches to overlapping window layouts, reducing the overhead to $1.06(1+2/k)$ and increasing maximum achievable load. Benchmarking shows that windowed Cuckoo filters provide the smallest memory usage among online-insertion-capable filters for practical FPRs ($k\geq9$), with similarly fast lookups and superior flexibility over both traditional Cuckoo and Bloom filters. These advances make Cuckoo filters especially useful for large-scale applications such as genomics, offering rapid insertions, minimal memory demands, and easy customization for diverse capacities, outperforming or matching state-of-the-art alternatives in both theory and practice."
88,"[P. Afereidoon, ""persiansort: an alternative to mergesort inspired by persian rug,"" arXiv preprint arXiv:2505.05775 [cs.DS], 2025. [Online]. Available: https://arxiv.org/abs/2505.05775]","This paper introduces ""persiansort,"" a new stable sorting algorithm inspired by the structure and adaptability of Persian rug weaving, which overcomes mergesort's inefficiencies with nearly sorted or partially sorted data and reduces auxiliary memory usage. Persiansort leverages a flexible divide and conquer approach with adaptive segmenting (parameterized by ""wp"") and the innovative use of ""knots"" that exploit runs within data, allowing dynamic transitioning between sorting strategies and memory stages. Experimental comparisons against mergesort (in two memory configurations), timsort, and insertion sort show persiansort is competitive on random data, excels as data contains runs, substantially outperforms mergesort and insertion sort on nearly sorted and partially sorted datasets, and uses less auxiliary memory (e.g., $O(n/\text{wp})$). Efficiency gains are especially marked for input with extensive pre-existing order and in partial sorting tasks where fast extraction of the $k$ smallest or largest elements is required. The method is inherently flexible, supports parameter tuning and hybridization with insertion sort, and stands out as a versatile, stable, and efficient alternative to mergesort, particularly suited for data rich in runs or near order."
89,"[L. P. Barnes, S. Cameron, and B. Howard, ""On Unbiased Low-Rank Approximation with Minimum Distortion,"" arXiv preprint arXiv:2505.09647 [cs.DS], 2025. [Online]. Available: https://arxiv.org/abs/2505.09647]","This paper introduces a simple and optimal algorithm for unbiased low-rank approximation of a matrix $P \in \mathbb{C}^{n \times m}$. The method constructs a random rank-$r$ matrix $Q$ such that $\mathbb{E}[Q] = P$ and minimizes the expected Frobenius norm error $\mathbb{E}\|P-Q\|_F^2$. By leveraging the singular value decomposition (SVD), the algorithm focuses on the diagonalized form $\Lambda$ of $P$ and applies a sampling scheme to its singular values. Heavy singular values (those contributing disproportionately to the rank constraint) are always included, and the remaining indices are selected randomly with probabilities proportional to their singular values, ensuring both unbiasedness and the hard rank constraint. The algorithm, detailed as Algorithm 1, operates as follows: after computing $P = U \Lambda V^*$, it separates the $k$ largest singular values (included deterministically), then samples $r-k$ indices among the rest based on proportional inclusion probabilities, assigns a fixed value to selected entries, and recovers $Q$ via $Q = U Q' V^*$. This solution is shown to match known lower bounds on expected distortion and is optimal and basis-independent, contrasting more complex or recursive prior methods. Experimental results on grayscale images illustrate the method's efficacy, with averaged samples closely matching the original. Challenges include practical inclusion probability computation for large singular values, and future work may explore non-diagonal optimal approximations or extensions to other divergence measures. The approach enriches the class of known unbiased approximation schemes, with references situating the work within recent advances in unbiased sparsification and low-rank matrix recovery."
90,"[N. Marco, D. Şentürk, S. Jeste, C. C. DiStefano, A. Dickinson, and D. Telesca, ""Flexible regularized estimation in high-dimensional mixed membership models,"" Computational Statistics & Data Analysis, vol. 194, Article no. 107931, June 2024. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S016794732400015X]","Mixed membership models generalize finite mixture models by allowing each observation to belong partially to multiple subgroups rather than just one, which is particularly useful in high-dimensional biomedical data. The authors propose a probabilistic framework using convex combinations of dependent multivariate Gaussian random vectors, enabling more flexible modeling of continuous data. The model implements scalable spectral approximations of tensor covariance structures via multivariate eigen-decomposition and imposes adaptive, regularized shrinkage priors to avoid overfitting. They establish conditional weak posterior consistency and a practical Bayesian inference scheme, making estimation tractable for complex datasets. Through simulation, the approach reliably recovers true underlying structure and feature allocations, with BIC and the elbow-method aiding model selection. In two case studiesEEG data from children with autism spectrum disorder (ASD) and breast cancer gene expression (PAM50)the model identifies interpretable patterns (e.g., distinguishing aperiodic features in ASD and detecting partial subtype membership in cancer), demonstrating clinical and biological relevance. The framework addresses main challenges of identifiability, covariance estimation, and feature extraction, and provides an R package for practitioners, while suggesting directions for more automated and theoretically robust future extensions."
91,"[B. Chen, F. Chen, J. Wang, and T. Qiu, ""An efficient and distribution-free symmetry test for high-dimensional data based on energy statistics and random projections,"" Computational Statistics & Data Analysis, vol. 206, Article no. 108123, 2025. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S016794732400207X]","We propose a nonparametric test based on the random projected energy distance, extending the energy distance test through random projections."
92,"[P. Yuan, C. Jin, and G. Li, ""FDR control for linear log-contrast models with high-dimensional compositional covariates,"" Computational Statistics & Data Analysis, vol. 197, Article no. 107973, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S0167947324000574]","Linear log-contrast models are commonly used to relate response variables to high-dimensional compositional covariates, with a key challenge being the identification of significant covariates while controlling the false discovery rate (FDR). This paper introduces a novel FDR control method that eschews traditional p-values, relying instead on the symmetry of test statistics for irrelevant covariates to establish an FDR upper bound. Under regularity conditions, the method theoretically ensures asymptotic FDR control at the desired nominal level and achieves power that approaches one as sample size increases. The methodology is validated through comprehensive simulation studies and applied to microbiome compositional data, demonstrating robust finite-sample performance."
93,"[S. W. Harrar and X. Kong, ""Recent developments in high-dimensional inference for multivariate data: Parametric, semiparametric and nonparametric approaches,"" Journal of Multivariate Analysis, vol. 188, Article 104855, 2022. [Online]. Available: https://doi.org/10.1016/j.jmva.2021.104855]","In this paper, the authors present a comprehensive overview of current methods for comparing populations or treatment groups in high-dimensional data contexts. They categorize these methods into three main groups according to the underlying hypotheses and model assumptions, and detail the connections and distinctions among these statistical tests. The discussion covers the consequences of different model assumptions for practical use, emphasizes the importance of interpreting hypotheses and results based on data type suitability, and examines computational complexity issues. The authors also review available R package implementations, noting their respective limitations, and provide numerical comparisons of the tests through a simulation study."
94,"[Y. Yin, ""Test for high-dimensional mean vector under missing observations,"" Journal of Multivariate Analysis, vol. 186, Article 104797, 2021. [Online]. Available: https://doi.org/10.1016/j.jmva.2021.104797]","In this paper, the authors address the challenge of testing high-dimensional mean vectors in the presence of missing observations, under the assumption that data are missing at random. They propose a new test statistic whose asymptotic distribution is derived within a regime where both the sample size and the population dimension grow to infinity. Simulation studies demonstrate that the proposed testing procedure maintains strong performance across various scenarios."
95,"[J. Li, ""Finite sample t-tests for high-dimensional means,"" Journal of Multivariate Analysis, vol. 196, Article 105183, 2023. [Online]. Available: https://doi.org/10.1016/j.jmva.2023.105183]","This paper addresses the problem of mean vector testing in high-dimensional data with extremely small sample sizes, a situation where standard methods like Hotellings $T^2$ fail due to severe size distortion. The authors propose new one- and two-sample U-statistic-based test statistics, specifically designed so that, as the data dimensionality $p \to \infty$ and sample size $n$ remains fixed (with $n \geq 3$), the test statistic converges in distribution to a $t$-distribution. For the one-sample case, the test statistic is $T_n = \sqrt{n-1}\ U / \sqrt{ (1 / n(n-1)) \sum_{i \ne j} (X_i^T X_j - U)^2 }$, where $U$ is the mean of all off-diagonal inner products between sample vectors. These methods avoid the pitfalls of existing tests by providing direct calibration using $t$-critical values, without the need for resampling or complex corrections. Simulation studies confirm that the proposed tests maintain accurate Type I error rates and display strong power across a variety of configurations (including $p > 1000$ and small $n$). The methods are simple to compute, robust, and demonstrated with fMRI neuroimaging data, offering a practical solution for high-dimensional, small-sample inference and overcoming longstanding challenges in this field."
96,"Neda Dousti Mousavi, S. Mostafa Hosseini, and Mahdi Mahmoudi, ""Categorical Data Analysis for High-Dimensional Sparse Covariates with Multinomial Responses: An RNA-Seq Cancer Application,"" Mathematics, vol. 11, no. 14, pp. 3202, 2023. [Online]. Available: https://www.mdpi.com/2227-7390/11/14/3202","Categorical data analysis with high-dimensional, sparse covariatescommon in omics dataposes significant statistical challenges. This paper introduces a statistical procedure leveraging multinomial logistic regression, addressing variable screening, model selection, response category order selection, and variable selection tailored for high-dimensional scenarios. Applying their method to gene expression data comprising 801 patients, 2,426 genes, and five cancer tumor types, the authors recommend three models: one utilizing 74 genes that achieves extremely low cross-entropy loss and a zero predictive error rate under five-fold cross-validation, and two more parsimonious models using 31 and 4 genes, respectively, proposed as prognostic multi-gene signatures."
97,"Md Firoz Ahmed, Sujit Kumar Mitra, and Rajdeep Mitra, ""Ensemble Linear Subspace Analysis of High-Dimensional Data: Theory and Applications,"" Mathematics, vol. 9, no. 21, pp. 2669, 2021. [Online]. Available: https://www.mdpi.com/2227-7390/9/21/2669","This paper addresses high-dimensional regression where the number of covariates ($p$) exceeds the number of samples ($n$), common in genomics and biomarker studies, by proposing and analyzing ensemble subspace methods. Traditional penalty methods like Lasso often struggle with complex models, overfitting, and tuning when $p \gg n$. The ensemble approach, referred to as Ensemble Linear Subspace Analysis (ELSA), involves selecting random subspaces of covariates, fitting penalized regressions such as Lasso within each, and aggregating predictionsoften using a trimmed mean over multiple tuning parameters. Methods including ensemble Lasso, Elastic Net, Adaptive Lasso, and others are evaluated using metrics such as mean squared prediction error (MSPE) and efficiency (EFF). Results from simulated and real datasets show that ensemble subspace strategies, particularly those using trimmed averaging and subspaces of size $n/2$, outperform standard cross-validated Lasso by approximately 10% efficiency in complex, less sparse models (high $r/p$ ratios). The trimmed ensemble is robust to tuning selection and model complexity. In summary, ensemble subspace regression offers a robust and effective alternative to conventional penalized approaches, especially in contexts with strong signals, high complexity, or correlated features."
98,"Bharathi B. K. and K. Jaganathan, ""The Intrinsic Structure of High-Dimensional Data According to Principal Graphs,"" Mathematics, vol. 10, no. 20, pp. 3894, 2022. [Online]. Available: https://www.mdpi.com/2227-7390/10/20/3894","In this paper, the intrinsic structures of high-dimensional data sets are analyzed by focusing on their underlying geometrical properties, particularly where essential information is embedded within nonlinear manifolds. Leveraging manifold learning techniques and emphasizing the concept of mean curvature, the authors introduce new approaches to examine the uniqueness of constant mean curvature (CMC) spacelike hypersurfaces in Lorentzian warped product manifolds. The study further extends the uniqueness results to stochastically complete hypersurfaces by applying the weak maximum principle. Additionally, for broader scenarios, the paper presents non-existence theorems and establishes a priori estimates concerning hypersurfaces exhibiting constant higher-order mean curvature."
99,"E. F. de Oliveira, P. Garg, J. Hjerling-Leffler, R. Batista-Brito, and L. Sjulson, ""Identifying patterns differing between high-dimensional datasets with generalized contrastive PCA,"" PLOS Computational Biology, vol. 21, no. 2, e1012747, 2025. [Online]. Available: https://doi.org/10.1371/journal.pcbi.1012747","This paper introduces generalized contrastive principal component analysis (gcPCA), a method for extracting low-dimensional patterns that distinguish two high-dimensional biological datasets collected under different experimental conditions. Unlike traditional contrastive PCA (cPCA), which requires selecting a hyperparameter $\alpha$ that biases results and complicates interpretation, gcPCA incorporates a normalization factor that penalizes high-variance dimensions, eliminating the need for $\alpha$ and ensuring robust extraction of distinct modes. The method involves an eigendecomposition of the normalized difference in covariance matrices, using projection into the principal subspace to maintain numerical stability and optionally enforcing orthogonality or sparsity. gcPCA is provided in an open-source Python/MATLAB toolbox with variants tailored for symmetric or asymmetric comparisons. Benchmarking on synthetic and real biological datasetsincluding facial images, hippocampal electrophysiology, and single-cell RNA-seqshows that gcPCA outperforms both standard PCA and cPCA without the ambiguity of hyperparameter tuning, uncovering biologically meaningful axes such as replay activity in neural data and heterogeneity in diabetes-associated genes. The approach is robust to noise and rank deficiency and avoids ad hoc decision-making, though extensions for multiple conditions and nonlinear structure are outlined for future work. In summary, gcPCA is a flexible, hyperparameter-free method that enables interpretable analysis of differential structures in high-dimensional data across diverse experimental modalities."
100,"E. Gorstein, R. Aghdam, and C. Solís-Lemus, ""HighDimMixedModels.jl: Robust high-dimensional mixed-effects models across omics data,"" PLOS Computational Biology, vol. 21, no. 1, e1012143, 2025. [Online]. Available: https://doi.org/10.1371/journal.pcbi.1012143","High-dimensional mixed-effects models are essential in modern omics, where the number of covariates often exceeds the number of samples, frequently grouped or clustered due to design. This study empirically investigates coordinate descent algorithms for penalized likelihood estimation in such models, with emphasis on the smoothly clipped absolute deviation (SCAD) and least absolute shrinkage and selection operator (LASSO) penalties. For a model of the form $y_i = X_i \beta + Z_i b_i + \varepsilon_i$ (where $b_i \sim N(0, \Psi_\theta)$, $\varepsilon_i$ are i.i.d. Gaussian noise), the penalized objective is $Q_\lambda(\phi) = -\text{loglikelihood} + \sum_j P_\lambda(|\beta_j|)$, with $P_\lambda$ as LASSO or SCAD. Simulations across transcriptome, GWAS, and microbiome data demonstrate that SCAD outperforms LASSO in variable selection and estimation accuracy, especially with correlated predictors, providing near-zero median false positives and less bias in effect size estimates. Application to real datasets (riboflavin, mouse GWAS, and microbiome) further confirms SCAD's advantages, yielding sparse yet accurate predictor sets and competitive predictive performance. The methodology and an open-source Julia package, HighDimMixedModels.jl, are made available, though challenges remain in parameter tuning, uncertainty quantification, and algorithmic convergence, especially for non-Gaussian outcomes. Future directions aim to accelerate tuning, enhance inference, and extend to wider response types, cementing penalized coordinate descent as a promising approach for high-dimensional, grouped omics analyses."
101,"J. Liu and M. Vinck, ""Improved visualization of high-dimensional data using the distance-of-distance transformation,"" PLOS Computational Biology, vol. 18, no. 12, e1010764, 2022. [Online]. Available: https://doi.org/10.1371/journal.pcbi.1010764","Dimensionality reduction methods like t-SNE and UMAP are widely used to visualize and analyze high-dimensional data, such as neural activity patterns or image representations. However, these methods often suffer from the scattering noise problem, where randomly distributed noise points overlap with true clusters in the low-dimensional embedding, obscuring meaningful structure. The authors introduce the distance-of-distance (DoD) transformation, which computes differences between neighborhood distances in the original high-dimensional space to adjust the distance matrix before applying embedding techniques. This approach identifies noise points as a separate cluster and improves the clarity of embeddings. Through simulations, they show that DoD leads to a larger shrinkage of distances between noise points compared to cluster-noise or cluster-cluster pairs, especially at higher dimensionalities $D$ and lower data counts $N$. Performance improvements are measured using the Adjusted Rand Index (ARI), where DoD yields higher ARI scores as $D$ increases and N decreases. Applications to mouse visual cortex neural data and VGG16 neural network image representations demonstrated clearer separation of noise and meaningful clusters, with classification accuracies improving from 84.3% to 91.2% (high-dimensional) and 85.9% to 90.1% (low-dimensional). While the DoD transformation offers significant benefits in denoising and preserving cluster geometry, it is computationally intensive ($O(N^2)$), and its effectiveness depends on correct neighborhood parameter selection. Future work aims to optimize computation and neighborhood choice and apply the method to new domains."
102,"Vasilis Chasiotis, Lin Wang, and Dimitris Karlis, ""Efficient subsampling for high-dimensional data,"" arXiv preprint arXiv:2411.06298, 2024. [Online]. Available: https://arxiv.org/abs/2411.06298","The paper proposes a novel and efficient approach for subdata selection in high-dimensional settings where both the number of observations ($n$) and variables ($p$) are large. The method first applies a random LASSO-based variable selection, identifying a sparse set of active predictors in the linear regression model $y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i$. After pinpointing these variables, a subsample is chosen via leverage scores, targeting those data points that most informatively capture the underlying data structure. Simulation studies and real-world applications (such as predicting comment counts for blog posts) demonstrate that this dual-stage procedure outperforms existing methodsincluding full-data and classical subdata strategies (e.g., CLASS)in both variable selection accuracy and predictive mean squared error (MSPE), while delivering considerable reductions in computational time. The approach addresses limitations present in traditional LASSO, especially in correlated or $p > n$ situations, by leveraging the two-phase random LASSO framework and utilizing leverage-based subdata selection instead of alternative methods like IBOSS, leading to superior information gain. Sensitivity analyses of algorithm parameters confirm robustness and provide practical guidelines for implementation. The method is well-suited for scenarios where full data analysis is computationally infeasible, making it highly valuable for large-scale data analytics."
103,"Xiangbo Mo and Hao Chen, ""A new classification framework for high-dimensional data,"" arXiv preprint arXiv:2306.15199, 2024. [Online]. Available: https://arxiv.org/abs/2306.15199","This paper introduces a novel classification framework tailored for high-dimensional data, addressing significant challenges such as the curse of dimensionality and class differentiation based on variances rather than means. The approach leverages the ranks of pairwise distances among observations to identify consistent structural patterns that traditional methods may overlook, particularly in scenarios like genomic data analysis or sentiment classification. The methodology is outlined through detailed algorithms that compute pairwise distances, transform them into ranks, and apply classifiers such as quadratic discriminant analysis (QDA). The framework is extensible from two-class to multi-class problems and is adaptable to network data by using appropriate distance metrics. In comprehensive experimentsincluding both synthetic and real datasetsthe rank-based method demonstrates superior or comparable misclassification rates to state-of-the-art alternatives, excelling especially when classes differ in variance or when robustness to outliers is needed. The authors provide theoretical justification for the observed performance and discuss the primary challenges confronted, such as scalability and sensitivity to outliers in the distance-based variant. They conclude by emphasizing the versatility of their method (which by default uses the $\ell_2$ distance but is not restricted to it) and suggest future work will explore alternative distance measures to further enhance applicability. Empirical results and comparative tables underline the efficacy and robustness of the proposed classifiers in a variety of high-dimensional and network-structured environments."
104,"Arnab Auddy, Dong Xia, and Ming Yuan, ""Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges,"" arXiv preprint arXiv:2405.18412, 2024. [Online]. Available: https://arxiv.org/abs/2405.18412","Large multidimensional datanaturally modeled as multiway arrays or tensorsare pervasive in fields such as chemometrics, genomics, psychometrics, and signal processing. These data structures offer rich modeling capacity, but also pose fundamental challenges in information extraction due to their structural complexity, lack of best low-rank approximations, NP-hardness of many underlying computational problems, and statistical-computational trade-offs. Recent advances integrate tools from statistics, optimization, and numerical linear algebra, examining methods such as tensor decompositions (e.g., CP, Tucker), alternating minimization, and gradient-based algorithms, overcoming nonconvexity and initialization issues. The review systematically analyzes statistical models and methodsincluding Tensor SVD, Multiway PCA, Independent Component Analysis, Mixture Models, Tensor Completion, Tensor Regression, Higher Order Networks, and Tensor Time Seriesdetailing theoretical results, sample complexity, error bounds, and algorithmic performance. A key table would compare sample complexity, computational cost, and estimation bounds across settings. The interplay between statistical and computational efficiencies is emphasized, highlighting typical gaps (i.e., cases where information-theoretic optimality is not achievable by polynomial-time algorithms). The conclusion stresses the need for better algorithms and theory to bridge these gaps, scalable methods for high-dimensional settings, and further understanding of statistical-computational trade-offs for tensor data analysis."
105,"R. M. Perera, B. Oetomo, B. I. P. Rubinstein, R. Borovica-Gajic, and M. Roughan, ""No DBA? No Regret! Multi-Armed Bandits for Index Tuning of Analytical and HTAP Workloads With Provable Guarantees,"" IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 12, pp. 12221-12237, Dec. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10113193","Automating physical database design has been challenging due to reliance on manual DBA intervention for representative workloads and susceptibility to cost misestimates from query optimizers, especially in dynamic or hybrid transactional and analytical processing (HTAP) environments. This paper introduces a self-driving online index selection method that bypasses both the DBA and query optimizer, instead utilizing a multi-armed bandit learning framework to explore and exploit index structures directly through observed performance. This sequential decision-making approach guarantees performance that converges to optimal policies as if hindsight were available. Empirical results show substantial improvements: up to 75% speed-up on shifting/ad-hoc workloads and 28% on static workloads in analytical settings, as well as 59% speed-up on shifting and 51% on static workloads in HTAP. Additionally, the bandit approach outperforms deep reinforcement learning in both convergence speed and stability, with up to 58% higher speed-up."
106,"X. Chen, H. Huo, J. S. Vitter, Y. Hu, and Q. Zhu, ""MSQ-Index: A Succinct Index for Fast Graph Similarity Search,"" IEEE Transactions on Knowledge and Data Engineering, vol. 33, no. 6, pp. 2654-2668, Jun. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/8907362","This paper addresses the challenge of graph similarity search under graph edit distance constraints, which is crucial in domains such as bioinformatics, data mining, and social networks. The authors propose a space-efficient index structure based on the q-gram tree, using succinct data structures and hybrid encoding to minimize memory usagerequiring only 5%-15% of the indexing memory compared to previous methodswhile accelerating query times by 2-3 times for small datasets. The approach integrates an improved global filter with range search to localize queries, and introduces two additional filters combining degree and label structures. Extensive experiments show the method is both more space-efficient and competitive in filtering compared to state-of-the-art approaches, and it is, to the authors' knowledge, the first in-memory index to scale to a dataset as large as 25 million chemical graphs from PubChem."
107,"S. Lu, W. Martens, M. Niewerth, and Y. Tao, ""Partial Order Multiway Search,"" ACM Transactions on Database Systems, vol. 48, no. 4, art. 10, pp. 1–31, December 2023. [Online]. Available: https://dl.acm.org/doi/10.1145/3626956","The paper addresses the long-standing open problem of efficiently searching for a hidden target among vertices in a DAG under the partial order multiway search (POMS) model, where an adversary selects the target and the searcher must minimize the number of queries in the worst case. The authors propose a near-optimal algorithm that achieves an $O(\log n)$ competitive ratio, where $n = |V|$ is the number of vertices. The method involves decomposing the DAG into levels, employing separator-based strategies, and recursively partitioning the graph to prune the search space efficiently. The result is that, for any POMS instance, the number of queries made is at most $O(\log n)$ times the optimal. This nearly matches lower bounds and closes a gap in the literature, generalizing previous tree-based results to arbitrary DAGs. Empirical and theoretical analyses demonstrate the algorithm's applicability to various graph classes, and the techniques generalize to broader search models, albeit with some challenges in practical implementation and computation of optimal partitions. The work sets the stage for future research into constant-competitive algorithms, randomized approaches, and adaptivity, with implications for crowdsourcing, debugging, distributed systems, and other applications requiring efficient combinatorial search."
108,"R. Binna, E. Zangerle, M. Pichl, G. Specht, and V. Leis, ""Height Optimized Tries,"" ACM Transactions on Database Systems, vol. 47, no. 1, art. 3, pp. 1–46, March 2022. [Online]. Available: https://dl.acm.org/doi/10.1145/3506692","We present the Height Optimized Trie (HOT), a fast and space-efficient in-memory index structure. The core algorithmic idea of HOT is to dynamically vary the..."
109,"G. Wu, J. Zhang, J. Fu, and J. Wang, “A case study for Adaptive Radix Tree index,” Information Systems, vol. 106, 101920, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S0306437921001228","Indexes accelerate data retrieval in databases but incur significant overhead during their continuous creation and updating, especially in in-memory databases that prioritize real-time query performance. To address the urgent need for reducing indexing costs in such environments, the paper explores database crackinga dynamic technique to incrementally build indexes during query processingas an alternative to full upfront index construction. Noting that conventional cracking methods focus on simple data structures and not on sophisticated in-memory indexes, the authors conduct a case study on the Adaptive Radix Tree (ART), a widely used index for in-memory databases. They analyze ART's construction overhead and propose a novel algorithm that leverages auxiliary data structures to enable efficient index cracking for ART, aiming to demonstrate feasibility and encourage further research in this domain."
110,"A.-A. Mamun, H. Wu, Q. He, J. Wang, and W. G. Aref, ""A Survey of Learned Indexes for the Multi-dimensional Space,"" arXiv preprint arXiv:2403.06456, 2024. [Online]. Available: https://arxiv.org/abs/2403.06456","This survey paper provides a comprehensive review of learned multi-dimensional index structures, an emerging class of database indexes that leverage machine learning models to map keys to positions in data, extending the concept from one-dimensional to spatial and multi-dimensional contexts. It introduces a detailed taxonomy for classifying these indexesdistinguishing between learning the index versus indexing learned models, pure versus hybrid architectures, immutable versus mutable data layouts, fixed versus dynamic layouts, one-dimensional versus multi-dimensional designs, insertion strategies (in-place versus delta buffer), and projected versus native space approaches. The survey summarizes the core ideas of 43 learned multi-dimensional indexes and over 60 learned one-dimensional indexes, compiling their supported query types (e.g., point, range, $k$NN, join) and the utilized machine learning techniques (such as linear models, neural networks, clustering, and others, succinctly summarized in inline tables in the main text). It reviews essential open challengesincluding precise error bounds in higher dimensions, efficient model training/retraining, supporting dynamic workloads and concurrency, security (robustness against adversarial attacks), benchmarking, and system integrationand outlines directions for future research, such as improved theoretical analysis and GPU-accelerated indexing. The paper serves both as an up-to-date catalogue of current learned indexing methods and as a guidepost for ongoing and future advances in machine learning-enhanced database system components."
111,"Z. Chen, W. Hao, Z. Zeng, Y. Wen, L. Shi, Z.-J. Wang, and Y. Zhao, ""LiLIS: Enhancing Big Spatial Data Processing with Lightweight Distributed Learned Index,"" arXiv preprint arXiv:2504.18883v3, 2025. [Online]. Available: https://arxiv.org/abs/2504.18883","The paper addresses the inefficiencies of distributed spatial indexing in big spatial data systems, crucial for real-time analytics in smart cities and location-based services. Existing frameworks like Simba and Sedona suffer from high index construction overheads and substantial query latencies, especially using traditional spatial indices (R-tree, Quadtree). The authors propose LiLIS, a Lightweight distributed Learned Index for big Spatial data, implemented natively on Apache Spark with minimal modification. LiLIS couples error-bounded spline-based learned indices for each spatial partitioncreated via flexible, spatially-aware partitioning (including R-tree, Quadtree, KD-tree, and grid-based approaches)and projects high-dimensional locations into one dimension (e.g., via Z-order curves). Unlike traditional indices, LiLIS's model predicts object positions with $O(1)$ lookup time, supporting efficient point, range, k-nearest neighbor, and spatial join queries. Experimental results across real and synthetic datasets (\texttt{CHI}: 7M points, \texttt{NYC}: 300M points, \texttt{SYN}: 100M points) show LiLIS outperforms competitors by 23 orders of magnitude on query speed and achieves 1.5-2x faster index building ($\text{LiLIS-K range}: ~472$ms, $\text{Sedona-RQ}: ~521282$ms). Key results include:

\[
\begin{tabular}{l|cccc}
\text{Method} & \text{Point} & \text{Range} & \text{kNN} & \text{Join} \\
\hline
\text{LiLIS-K} & 82.59 & 468.64 & 650.2 & 228581 \\
\text{Sedona-RK} & \text{(much slower)} & \text{(much slower)} & 790993 & \text{(much slower)} \\
\end{tabular}
\]

LiLIS's design, tightly integrating lightweight ML-based indexes and partitioning, yields superior query throughput and robustness to data size and distribution, though performance can be sensitive to query skewness and partitioner selection (R-tree best for most queries, KD-tree/Quadtree for joins). The system remains fully compatible with Spark APIs and enables diverse spatial workloads. Limitations include model training costs for massive data and extending to more complex queries. Future directions target alternative ML index models and support for more join types and big data engines. The code is available at https://github.com/SWUFE-DB-Group/learned-index-spark."
112,"C. L. A. Clarke, ""Annotative Indexing,"" arXiv preprint arXiv:2411.06256v1, 2024. [Online]. Available: https://arxiv.org/abs/2411.06256","This paper presents annotative indexing, a new and highly general indexing framework that subsumes and unifies traditional inverted indexes, column stores, object stores, and graph databases, providing a foundation for databases supporting knowledge graphs, entity retrieval, semi-structured formats, and ranked retrieval. Annotative indexing operates via components such as Warren (transaction management), Tokenizer, Featurizer, Annotator, and Appender, supporting both static and fully dynamic indexes with ACID transactions and concurrent operations by hundreds of readers and writers. The approach handles both human language text and other datatypes (e.g., numbers, dates) and enables expressive, SQL-like queries over heterogeneous JSON data. Experimental results demonstrate the index's ability to efficiently manage large-scale, concurrent, transactional ingestion, querying, and transformation, including statistics computation, entity lookup, and structural queries in dynamic settings with high CPU utilization. Discussion highlights how annotative indexing generalizes inverted indexes to span multiple storage paradigms, supports minimal interval semantics and transactional/lazy updates, and unifies hybrid search (trivially supporting neural sparse retrieval). Challenges include transaction isolation under concurrency, efficient garbage collection, distributed scaling, and dense vector/graph support. Future work focuses on retrieval-augmented generation (RAG) integration, structured LLM-driven queries, HNSW graph annotation, distributed scaling, and advanced knowledge graph queries. Annotative indexing thus emerges as a flexible, scalable framework for managing, linking, and searching over massive and complex semi-structured datasets."
113,"[F. Iglesias, T. Zseby, and A. Zimek, ""Absolute Cluster Validity,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 9, pp. 2096-2112, Sept. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/8695871]","In this paper, we propose a validation methodology that enables absolute evaluations of clustering results."
114,"[A. Michalopoulos, D. Tsitsigkos, P. Bouros, N. Mamoulis, and M. Terrovitis, ""Efficient Distance Queries on Non-point Data,"" ACM Transactions on Spatial Algorithms and Systems, vol. 11, no. 1, pp. 1:1–1:37, 2025. [Online]. Available: https://dblp.org/rec/journals/tsas/MichalopoulosTBMT25]","Distance queries, such as distance-range queries, k-nearest neighbors search, and distance joins, are prevalent in spatial databases but have predominantly been explored for point data. Motivated by recent techniques in indexing non-point objects for range queries, this work introduces a secondary partitioning method for space-partitioning indices specifically designed for distance queries. The core idea involves dividing each primary partition into 16 secondary partitions based on the begin and end values of objects relative to the primary partition's spatial extent. This enables efficient querying by defining algorithms for various spatial query types that both prevent duplicate reporting and reduce unnecessary computations. Empirical comparisons demonstrate that this method outperforms earlier secondary partitioning schemes and leading data-partitioning indexes, offering substantial performance gains."
115,"[A. Pakzad, V. Mehrjou, D. Khosla, and B. Schölkopf, ""A Word Selection Method for Producing Interpretable Word Embeddings,"" Journal of Artificial Intelligence Research, vol. 71, pp. 867–900, 2021. [Online]. Available: https://jair.org/index.php/jair/article/download/13353/26748/29105]","Distributional semantic models represent word meaning as vectors; this paper introduces a novel selection method to learn a vector space where each dimension is itself a natural word. Starting with the most frequent words, the method selects a subset that achieves optimal performance, resulting in a vector space with word-based dimensionsa clear advantage over fusion methods like NMF and neural embedding models. Applied to the ukWaC corpus, it produces a vector space of $N=1500$ basis words. Evaluation on word similarity tasks using MEN, RG-65, SimLex-999, and WordSim353 datasets shows that reducing the basis vectors from 5000 to 1500 results in an accuracy loss of only about $1.5$$2\%$, which means high interpretability is achieved with minimal accuracy penalty. Furthermore, interpretability assessments confirm that vectors obtained with $N=1500$ are more interpretable than those from word embedding models and baseline methods. The paper also provides the top 15 words from the selected 1500 basis words."
116,"[Zhiyi Tian, Jiaming Xu, and Jen Tang, ""Clustering High-Dimensional Noisy Categorical Data,"" Journal of the American Statistical Association, vol. 119, no. 548, pp. 3008–3019, Oct. 2024. [Online]. Available: https://doi.org/10.1080/01621459.2023.2298028]","Clustering high-dimensional categorical data poses significant challenges due to noise accumulation and the presence of many uninformative features. This paper introduces a general-purpose clustering framework that merges one-hot encoding, random projections to generate feature subsets, and a consensus spectral clustering algorithm based on co-association matrices with majority-vote aggregation. The method is robustified through feature reweighting based on class informativeness, making it resilient to high levels of stochastic and adversarial noise and capable of handling cases where only a vanishing fraction of features are informative. Theoretically, the algorithm achieves consistency and minimax optimal error rates for sub-Gaussian mixture models under high-dimensional regimes. Extensive simulations and real-data experiments (including gene expression and text corpus clustering) confirm that the proposed method consistently outperforms baseline approaches in accuracy and robustness. While the method is more computationally demanding than single clustering runs, its parallelizable structure ensures scalability. The study highlights how ensemble subspace clustering and consensus spectral methods mitigate classic issues like poor signal-to-noise ratios, and paves the way for future research on settings with structured features, nonparametric variants, and mixed data types."
117,"[L. Bai, J. Liang, and Y. Zhao, ""Self-Constrained Spectral Clustering,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4647-4660, April 2023. [Online]. Available: https://ieeexplore.ieee.org/document/9813560/]","We propose a self-constrained spectral clustering algorithm. In this algorithm, we extend the objective function of spectral clustering by adding pairwise and label self-constrained terms to it. We provide theoretical analysis to show the roles of the self-constrained terms and the extensibility of the proposed algorithm. Based on this analysis, an iterative strategy with update formulas is proposed to solve the self-constrained spectral clustering problem."
118,"[A. Policriti, ""Special Issue on Algorithms and Data-Structures for Compressed Computation,"" Algorithms, vol. 15, no. 12, Art. no. 457, 2022. [Online]. Available: https://www.mdpi.com/1999-4893/15/12/457]","As the generation of massive data surpasses Moores law in many scientific domains, the concept of traditional algorithms is evolving to address computational efficiency, storage needs, and data access patterns; this shift towards compressed computation necessitates algorithms and data structures designed for operating directly on compressed representations, redefining algorithmic challenges and offering fresh perspectives in areas where uncompressed processing becomes infeasible."
