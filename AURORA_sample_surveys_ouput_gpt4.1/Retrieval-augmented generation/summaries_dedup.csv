Index,Citation,Summary
1,"C. S. Ong, N. T. Obey, Y. Zheng, A. Cohan, and E. B. Schneider, “SurgeryLLM: a retrieval-augmented generation large language model framework for surgical decision support and workflow enhancement,” npj Digital Medicine, vol. 7, Article no. 364, 2024. Published: 18 December 2024. Available: https://www.nature.com/articles/s41746-024-01391-3","SurgeryLLM is a Retrieval-Augmented Generation (RAG)-enhanced large language model framework designed to incorporate up-to-date, domain-specific knowledge from evidence-based surgical guidelines into clinical decision-making. The study highlights the urgent need for such innovations due to projected shortages and high workloads among surgeons, especially in cardiac specialties. SurgeryLLM utilized LangChain for document loading, GPT4All for embedding generation, and a local Llama-based LLM framework for processing queries. In tests with simulated cardiovascular patient cases, SurgeryLLM outperformed a conventional, non-augmented LLM (VanillaLLM) across four tasks: identifying abnormal lab values, detecting missing investigation data, formulating management recommendations per national surgical guidelines, and drafting structured operative notes. SurgeryLLMs responses aligned with current clinical guidelines (such as recommending CABG for coronary disease and TAVI for valve conditions), while VanillaLLM either declined, provided vague responses, or showed uncertainty. Challenges remain around patient data quality and completeness, and ongoing development aims to further refine context-aware retrieval, reporting precision, and bias mitigation. The study concludes that SurgeryLLM demonstrates the feasibility and promise of RAG-augmented LLMs in enhancing surgeon efficiency, patient safety, and optimizing outcomes, particularly as healthcare systems face increasing demand and specialization."
2,"Y. H. Ke, L. Jin, K. Elangovan, H. R. Abdullah, N. Liu, A. T. H. Sia, C. R. Soh, J. Y. M. Tung, J. C. L. Ong, C.-F. Kuo, S.-C. Wu, V. P. Kovacheva, D. S. W. Ting, “Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness,” npj Digital Medicine, vol. 8, Article no. 187, 2025. Published: 05 April 2025. Available: https://www.nature.com/articles/s41746-025-01519-z","Large Language Models (LLMs), when enhanced by Retrieval Augmented Generation (RAG), can effectively and safely deliver preoperative medical guidance by incorporating specialized guidelines. This study evaluated ten LLMs, including GPT-3.5, GPT-4, Gemini, Llama2/3, and Claude, comparing their performancevia a novel S.C.O.R.E. frameworkagainst human clinicians over 14 clinical scenarios using 35 local and 23 international perioperative guidelines. The GPT-4 RAG model paired with international guidelines achieved the highest accuracy (96.4%), significantly outperforming both human assessments (86.6%, $p=0.016$) and other LLM variants, and demonstrated high reproducibility (4.86/5) and safety (4.93/5). The LLM-RAG delivered faster and more consistent outputs with reduced hallucinations compared to both humans and non-RAG LLMs, especially in triaging surgical fitness and issuing guideline-based instructions. While accuracy for medication guidance was slightly better among human evaluators, LLM-RAG excelled in other domains, evidencing the power of international, text-based guideline integration in minimizing workflow inconsistencies and potential surgery cancellations. Despite challenges such as computational overhead, varying local guideline quality, and occasional hallucinations in some LLMs (notably Llama2-local), the adaptable and scalable design of LLM-RAG frameworks like Llamaindex portends broader clinical utility. Future advancements may focus on dynamic retrieval, context-aware augmentation, and fine-tuning to further enhance real-world deployment and standardization in healthcare."
3,"I. Lopez, A. Swaminathan, K. Vedula, S. Narayanan, F. Nateghi Haredasht, S. P. Ma, A. S. Liang, S. Tate, M. Maddali, R. J. Gallo, N. H. Shah, J. H. Chen, “Clinical entity augmented retrieval for clinical information extraction,” npj Digital Medicine, vol. 8, Article no. 45, 2025. Published: 19 January 2025. Available: https://www.nature.com/articles/s41746-024-01377-1","Large language models (LLMs) using retrieval-augmented generation (RAG) have enhanced clinical information extraction, but often struggle with inefficient retrieval due to reliance on embeddings. This paper introduces CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that leverages clinical named entity recognition (NER) to identify and retrieve relevant note chunks before variable extraction, optimizing both performance and efficiency. CLEAR involves zero-shot NER (using Flan-T5-XXL), entity selection (Bio+Clinical BERT and GPT-4), entity augmentation (UMLS, GPT-4), and retrieval of note chunks with MedSpaCy, followed by extraction using multiple LLMs. Evaluated on two real-world EHR datasets (encompassing 18 variables such as substance use, mental health, social determinants, and chest radiograph findings) and compared to embedding-based RAG and full-note approaches, CLEAR achieved superior results: average F1 scores of 0.90 to 0.96, inference times as low as 1.04s per note, >70% reductions in token usage and processing time, and fewer model queries per note (see simplified results table):

\[
\begin{tabular}{l|c|c|c}
 & \text{CLEAR} & \text{Chunk Embedding} & \text{Full Note} \\
\hline
\text{Avg F1} & 0.90-0.97 & 0.86-0.88 & 0.79-0.90 \\
\text{Time (s/note)} & 1.04-4.95 & 4.92-17.41 & 7.2-20.08 \\
\text{Tokens/note (k)} & 1.1 & 3.8 & 6.1 \\
\end{tabular}
\]

CLEARs context prioritization avoids degradation common in long-context LLMs and proves more effective than embedding methods. Beyond improving accuracy and efficiency for variable extraction (up to 97% time savings compared to manual annotation), this approach is extensible to automated knowledge graph generation and has potential for other clinical NLP applications. Code is available at https://github.com/iv-lop/clear, though data sharing is restricted due to privacy requirements. Limitations include focus on variable extraction and the need for further study on other tasks and deployment efficiencies. Overall, CLEAR offers a validated, practical advance for scalable, accurate clinical information extraction."
4,"L. Huang, J. Yang, and Z. H. Zhang, “A Comprehensive Review on Retrieval-Augmented Language Models,” IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 5, pp. 2348–2361, 2022. URL: [link unavailable, see Google Scholar or IEEE Xplore for direct access]","The advent of Retrieval-Augmented Generation (RAG) models marks a significant step forward in artificial intelligence, especially for natural language processing and generation. RAG models combine neural networksleveraging architectures such as transformers and attention mechanismswith external databases, producing a dual framework that boosts AI performance by enabling contextually relevant, accurate responses. This synergy helps overcome the typical limitations of classical generative models like hallucination and factual errors, as external data repositories are integrated into the generation pipeline. The paper explores RAGs architectural details and operational methodologies, emphasizing the interplay between retrieval and generation, and demonstrates the models superiority in applications including content creation, personalized AI assistants (with adapting responses based on user preferences and history), and customer service. Through dynamic retrieval of real-time information, RAG models achieve higher precision and contextuality in their outputs, improving both dialogue and factual accuracy. The research also discusses challenges facing RAG, such as computational overhead, handling heterogeneous data sources, and ethical concerns tied to accessing external databases. Finally, it highlights future research directions, suggesting that interdisciplinary work spanning computer science, linguistics, and cognitive psychology will be key to realizing the full potential of RAG models in evolving AI applications."
5,"S. Liu, A. B. McCoy, and A. Wright, “Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines,” Journal of the American Medical Informatics Association, vol. 32, no. 4, pp. 605–615, Apr. 2025. doi: 10.1093/jamia/ocaf008. URL: https://pubmed.ncbi.nlm.nih.gov/39812777/","This systematic review and meta-analysis evaluated the effectiveness of retrieval-augmented generation (RAG) in enhancing large language models (LLMs) for biomedical and clinical tasks, synthesizing findings from 20 eligible studies published in 2023 and 2024 that compared baseline LLMs with RAG-enhanced systems. The pooled effect size indicated a statistically significant improvement, with RAG achieving an odds ratio of 1.35 ($95\%$ CI: $1.19$$1.53$, $P=.001$) over baseline LLMs. Clinical tasks varied, but improvements were consistent across different models and retrieval strategies. The study presented a new framework, GUIDE-RAG, which organizes clinical RAG implementation into iterative stages: pre-retrieval (task definition, resource identification, content preprocessing), retrieval (chunking, indexing, advanced strategies), and post-retrieval (evaluation, knowledge updates, few-shot learning). The authors recommend future research to target (1) system-level enhancements by combining RAG with autonomous agents, (2) knowledge-level improvements through deeper knowledge integration into LLMs, and (3) integration-level innovations by embedding RAG into electronic health records, concluding that RAG materially advances clinical LLM application performance."
6,"Y. Li, J. Zhao, M. Li, Y. Dang, E. Yu, J. Li, Z. Sun, U. Hussein, J. Wen, A. M. Abdelhameed, J. Mai, S. Li, Y. Yu, X. Hu, D. Yang, J. Feng, Z. Li, J. He, W. Tao, T. Duan, Y. Lou, F. Li, and C. Tao, “RefAI: a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation and summarization,” Journal of the American Medical Informatics Association, vol. 31, no. 9, pp. 2030–2039, Sep. 2024. doi: 10.1093/jamia/ocae129. URL: https://pubmed.ncbi.nlm.nih.gov/38857454/","This study introduces RefAI, a retrieval-augmented generation tool that leverages the strengths of large language models (LLMs) while addressing their limitations in biomedical literature recommendation and summarization. Unlike typical GPT-based solutionswhich either generate fabricated articles or fail to cite sourcesRefAI integrates systematic retrieval from PubMed, a novel multivariable article recommendation algorithm, and GPT-4 turbo for summarization. Evaluated against ChatGPT-4, ScholarAI, and Gemini on queries about ""cancer immunotherapy and target therapy"" and ""LLMs in medicine,"" RefAI demonstrated statistically significant improvements ($P < .05$) across five key criteria: literature relevance, recommendation quality, summary accuracy, comprehensiveness, and citation integration. Expert assessments validate that RefAI substantially mitigates common issues like hallucinated articles, metadata errors, limited recommendations, and poor reference inclusion, establishing it as a potent solution for biomedical professionals needing reliable literature navigation and synthesis."
7,"Z. Zhan, S. Zhou, M. Li, and R. Zhang, “RAMIE: retrieval-augmented multi-task information extraction with large language models on dietary supplements,” Journal of the American Medical Informatics Association, vol. 32, no. 3, pp. 545–554, Mar. 2025. doi: 10.1093/jamia/ocaf002. URL: https://pubmed.ncbi.nlm.nih.gov/39798153/","The paper introduces the RAMIE (retrieval-augmented multi-task information extraction) framework, a novel large language model (LLM)-based approach for extracting diverse dietary supplement (DS) information from clinical records across four core tasks: named entity recognition (NER), relation extraction (RE), triple extraction (TE), and usage classification (UC). Recognizing the critical public health importance and challenging clinical text variability involved in accurately extracting DS-related information, RAMIE incorporates instruction fine-tuning with task-specific prompts, multi-task learning (MTL) for simultaneous training and efficiency, and retrieval-augmented generation (RAG) to introduce relevant example retrieval during generation. On annotated datasets, LLMs using RAMIE (e.g., Llama2-13B and MedAlpaca-7B) significantly outperformed BERT-based models, with F1 improvements notable in NER (up to 87.39, a 3.51% gain), RE (up to 93.74, +1.15%), and especially TE (79.45, +14.26%). Ablation studies showed RAGs vital role in boosting accuracy while MTL greatly reduces resource demand; for instance, average F1 dropped 2.72% without RAG. Despite challenges such as clinical language complexity and dataset limitations, RAMIE provides a scalable, storage-efficient, and high-performing solution for automated multi-task DS data extraction; the framework, code, and resources (except for data) are available at https://github.com/Learner4everrr/RAMIE."
8,"Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, and H. Wang, “Retrieval-Augmented Generation for Large Language Models: A Survey,” arXiv preprint arXiv:2312.10997, 2024. [Online]. Available: https://arxiv.org/abs/2312.10997","Large Language Models (LLMs) demonstrate strong abilities but struggle with issues like hallucinations, unreliable reasoning, and obsolete or untraceable information. Retrieval-Augmented Generation (RAG) addresses these by integrating external database knowledge, thereby improving accuracy, credibility, and allowing ongoing updates and specialization for knowledge-intensive applications. This review paper thoroughly traces the evolution of RAG, analyzing approaches such as Naive RAG, Advanced RAG, and Modular RAG, while systematically assessing three core elements: retrieval, generation, and augmentation techniques. State-of-the-art methods for each component are discussed, alongside the latest evaluation frameworks and benchmarks. The review concludes by outlining key challenges and directions for future research in RAG-based systems."
9,"H. Wang, A. Prasad, E. Stengel-Eskin, and M. Bansal, “Retrieval-Augmented Generation with Conflicting Evidence,” arXiv preprint arXiv:2504.13079, 2025. [Online]. Available: https://arxiv.org/abs/2504.13079","Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation."
10,"P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 33, 2020. arXiv:2005.11401. [Online]. Available: https://arxiv.org/abs/2005.11401","Large pre-trained language models effectively store factual knowledge and excel in downstream NLP tasks after fine-tuning, but face challenges in precise knowledge manipulation, updating, and providing decision provenance. Retrieval-augmented generation (RAG) models, which integrate pre-trained seq2seq parametric models with non-parametric memory (a dense vector index of Wikipedia accessed via a neural retriever), address these limitations by offering differentiable access to external knowledge. The paper proposes two RAG variants: one that conditions on static retrieved passages for the entire generation sequence, and another that allows different passages per token. Evaluated across multiple knowledge-intensive NLP tasks, these RAG models achieve state-of-the-art results on three open-domain QA benchmarks, outperforming both parametric-only seq2seq models and architecture-specific retrieve-and-extract systems, while also generating more specific, diverse, and factual outputs."
11,"Chuyuan Wei, Ke Duan, Shengda Zhuo, Hongchun Wang, Shuqiang Huang, Jie Liu, “Enhanced Recommendation Systems with Retrieval-Augmented Large Language Model,” Journal of Artificial Intelligence Research, vol. 82, 2025, pp. 1–27. [Online]. Available: https://jair.org/index.php/jair/article/view/17809","Recommender systems often suffer from cold start and data sparsity, resulting in suboptimal recommendation quality. Traditional efforts to resolve these issues via side information frequently introduce noise, lack adaptability for data expansion, and are vulnerable to data inconsistency, impeding precise user preference modeling. This work proposes ER2ALM, a novel framework that leverages large language models (LLMs) augmented with Retrieval-Augmented Generation (RAG) to flexibly enhance auxiliary information and effectively capture users' implicit interests while employing a noise reduction strategy to maintain information reliability. Experiments on two real-world datasets show that ER2ALM significantly outperforms state-of-the-art baselines in both recommendation accuracy and robustness, positioning the framework as a promising new paradigm in preference mining for recommender systems."
12,"Maurice Abaho, Jialiang Guo, Sebastien Harpe, “Enhanced Dense Retrieval Knowledge Graph Augmentation,” Journal of Artificial Intelligence Research, vol. 80, 2024, pp. 1139–1178. [Online]. Available: https://jair.org/index.php/jair/article/view/14365","Injecting textual information into knowledge graph (KG) entity representations has been shown to enhance performance in KG-oriented NLP tasks. Traditional approaches leverage external knowledge sources such as dependency-parsed features, relevant keywords, or full text descriptions (e.g., from Wikipedia) to enrich KG embeddings. However, this work proposes that further gains can be achieved by moving beyond a single text description per entitysince a single description may not sufficiently capture the entity due to lexical ambiguity. The authors introduce a multi-task framework that jointly selects a set of relevant text descriptions for each KG entity and aligns or augments KG embeddings with these texts. Unlike earlier approaches that rely on formal entity descriptions from knowledge bases, this framework utilizes a retriever model to selectively identify richer, highly relevant descriptions. Moreover, the framework treats the number of text descriptions used as a tunable parameter, enabling an empirical search for the optimal count. Experimental results for the Link Prediction task show a 5.5% increase in Mean Reciprocal Rank (MRR) and a 3.5% increase in Hits@10 compared to standard text-enhanced knowledge graph augmentation methods using traditional CNNs."
13,"Federico Castagna, Sara Tonelli, Serena Villata, “Computational Argumentation-based Chatbots: a Survey,” Journal of Artificial Intelligence Research, vol. 80, 2024, pp. 1269–1330. [Online]. Available: https://jair.org/index.php/jair/article/view/15407","This paper presents a comprehensive survey of computational argumentation-based chatbots, which are conversational agents enhanced by formal models of argument to enable more natural, explainable, and persuasive dialogue compared to standard bots. The review considered systems employing argument mining, argumentative templates, and reasoning enginesprimarily using retrieval-based architectures and deployed in closed domains like health and educationwith the Bipolar Argumentation Framework (BAF) commonly used to model both supporting and attacking relations among arguments. Results show that roughly 70% of surveyed bots deliver replies using computational argument models, with user studies indicating such systems are more persuasive, informative, and trustworthy. The primary challenges identified include labor-intensive knowledge base construction, the integration of argumentation models with generative LLMs, scalability to open domains, and the need for more rigorous evaluation. Although current large language model (LLM) systems like ChatGPT outperform these bots in fluency, they lack transparency and robust argumentationa gap that could be addressed by hybridizing LLMs with computational argumentation for future open-domain, explainable conversational agents. This approach holds promise for improving chatbot transparency, reasoning, and robustness, with suggested future work focusing on integrating argumentation engines into LLM-based bots, automating argument acquisition, and standardizing argumentative dialogue protocols."
14,"O. Rubin and J. Berant, “Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval,” Transactions of the Association for Computational Linguistics, vol. 12, 2024. [Online]. Available: https://transacl.org/index.php/tacl/article/view/6313","The paper introduces the Retrieval-Pretrained Transformer (RPT), a language model architecture and training procedure that jointly learns both the language modeling and retrieval components from scratch, with the aim of modeling long texts more effectively. Unlike prior retrieval-augmented language models (RALMs), where the retriever is typically trained separately or added post-hoc, RPT integrates the retriever as a first-class component, leveraging the model's internal representations for self-retrieval and fusing the retrieved information into the decoder via chunked cross-attention. Supervision is provided by a semantic objective: the retriever is trained to identify chunks that increase the probability of correctly generating the next chunk, as scored by a reference language model; the training loss consists of standard language modeling loss and a LambdaRank retrieval loss. RPT is evaluated on four tasksbooks (PG19, Books3), code (CodeParrot), and mathematical writing (ArXiv)and consistently improves perplexity and retrieval quality (measured by precision, recall, NDCG) over strong baselines such as Transformer-XL, RETRO, and others. Ablations confirm the importance of semantic retrieval, joint training, and the proposed neighbor gating mechanism. The main challenges include scaling supervision beyond top-20 BM25 candidates and handling very large document indices, with future work targeting open-domain retrieval and integration with efficient or hybrid model architectures. The results indicate that deeply integrating retrieval at both the architectural and training levels enables RPT to access and use long-range, semantically relevant context, laying groundwork for next-generation pretrained language models with robust retrieval capabilities."
15,"O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham, “In-Context Retrieval-Augmented Language Models,” Transactions of the Association for Computational Linguistics, vol. 11, 2023. [Online]. Available: https://transacl.org/index.php/tacl/article/view/5039","This paper introduces In-Context Retrieval-Augmented Language Modeling (In-Context RALM), a method that improves language model (LM) grounding and factual accuracy by prepending externally retrieved documents to the LMs input without modifying the model architecture or parameters. Using off-the-shelf retrievers such as BM25 and dense dual encoders, the approach invokes retrieval at fixed intervals (every $k$ tokens), inserts the top $m$ retrieved documents before each block, and optionally applies re-ranking (which can use the LM itself). Experiments across datasets (Wikitext-103, RealNews, Pile, several QA benchmarks) and models (GPT-2, GPT-J, GPT-Neo, LLaMA variants) show that In-Context RALM yields large improvements in perplexity and downstream generation metrics; for example, GPT-2s perplexity on Wikitext-103 improves from 24.1 (base) to 16.5 with BM25-augmented context. Performance is affected by the retrieval frequency $k$ and query window length, with intermediate query lengths balancing recall and relevance. The method is practical for deployment even with closed-source or API-based LMs. While challenges remain (such as optimal retrieval granularity, handling very large corpora, and integration of advanced rerankers), In-Context RALM demonstrates substantial potential for factual grounding and source attribution in language generation tasks without retraining or architectural changes."
16,"H. Chen, Z. Chen, Y. Zhao, M. Wang, L. Li, M. Zhang, and M. Zhang, “Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification,” Transactions of the Association for Computational Linguistics, vol. 12, 2024. [Online]. Available: https://transacl.org/index.php/tacl/article/view/6137","This paper introduces the first in-context learning (ICL) framework for few-shot hierarchical text classification (HTC) using large language models (LLMs). HTC involves classifying texts under a multi-level, tree-structured label space, presenting significant challenges for few-shot learning due to expansive and semantically ambiguous label sets. The proposed method utilizes a retrieval database to select relevant demonstrations for each test instance and iteratively narrows down label candidates level-by-level within the hierarchy. Key innovations include constructing HTC label-aware text representations by continually training a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS), and a novel divergent contrastive learning (DCL) objective specifically tuned to differentiate semantically similar, adjacent labels. This retrieval-style ICL framework leverages these representations to select top-K relevant, label-distinct demonstrations at each level for prompt construction. Experiments on three benchmark datasets (WOS, DBPedia, Chinese Patent) demonstrate that the method outperforms baselinesincluding standard BERT fine-tuning and other hierarchical/classification modelsespecially in extremely low-resource few-shot scenarios, achieving state-of-the-art Micro-F1 and Macro-F1 scores. Ablation studies confirm the contributions of each component (e.g., structured label descriptions, DCL loss, iterative prompting). The approach is robust across languages and settings, and analysis shows improvements in annotation efficiency, but challenges remainincluding scaling to larger training sets and generative label expansion. The framework, with code provided for reproducibility, pushes the boundaries of few-shot HTC and suggests future work in dataset expansion and further leveraging LLM generation capabilities."
17,"Y. Wu, W. Wu, C. Xing, C. Xu, Z. Li, and M. Zhou, “A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots,” Computational Linguistics, vol. 45, no. 1, pp. 163–197, 2019. [Online]. Available: https://aclanthology.org/J19-1005/","We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task involves matching a response candidate with a conversation context, the challenges for which include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching. This motivates us to propose a new matching framework that can sufficiently carry important information in contexts to matching and model relationships among utterances at the same time. The new framework, which we call a sequential matching framework (SMF), lets each utterance in a context interact with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network (RNN) that models relationships among utterances. Context-response matching is then calculated with the hidden states of the RNN. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experiment results show that both models can significantly outperform state-of-the-art matching methods. We also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage important information in contexts for matching."
18,"Xiyan Fu and Anette Frank, “Exploring Continual Learning of Compositional Generalization in NLI,” Transactions of the Association for Computational Linguistics, vol. 12, pp. 912–932, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.51/","Compositional Natural Language Inference (NLI) has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order primitives and compositional inference types, and examining correlations between subtasks. Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability."
19,"L. Zhen, P. Hu, X. Peng, R. S. M. Goh, and J. T. Zhou, “Deep Multimodal Transfer Learning for Cross-Modal Retrieval,” IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 2, pp. 798–810, Feb. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9236655","Cross-modal retrieval (CMR) enables flexible retrieval experience across different modalities (e.g., texts versus images), which maximally benefits us from the abundance of multimedia data. Existing deep CMR approaches commonly require a large amount of labeled data for training to achieve high performance. However, it is time-consuming and expensive to annotate the multimedia data manually. Thus, how to transfer valuable knowledge from existing annotated data to new data, especially from the known categories to new categories, becomes attractive for real-world applications. To achieve this end, we propose a deep multimodal transfer learning (DMTL) approach to transfer the knowledge from the previously labeled categories (source domain) to improve the retrieval performance on the unlabeled new categories (target domain). Specifically, we employ a joint learning paradigm to transfer knowledge by assigning a pseudolabel to each target sample. During training, the pseudolabel is iteratively updated and passed through our model in a self-supervised manner. At the same time, to reduce the domain discrepancy of different modalities, we construct multiple modality-specific neural networks to learn a shared semantic space for different modalities by enforcing the compactness of homoinstance samples and the scatters of heteroinstance samples. Our method is remarkably different from most of the existing transfer learning approaches. To be specific, previous works usually assume that the source domain and the target domain have the same label set. In contrast, our method considers a more challenging multimodal learning situation where the label sets of the two domains are different or even disjoint. Experimental studies on four widely used benchmarks validate the effectiveness of the proposed method in multimodal transfer learning and demonstrate its superior performance in CMR compared with 11 state-of-the-art methods."
20,"C. Bai, X. Fan, J. Liu, W. Tang, H. Huang, and J. Yin, “Graph Convolutional Network Discrete Hashing for Cross-Modal Retrieval,” IEEE Transactions on Neural Networks and Learning Systems, vol. 35, no. 4, pp. 1714–1727, Apr. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/9779852","With the rapid development of deep neural networks, cross-modal hashing has advanced significantly; however, an asymmetry exists between modalities: images, with high enough resolution, can nearly fully reproduce real-world scenes, while text often reflects subjective emotions and contains less objective information. Existing end-to-end cross-modal hashing methods typically unify semantic feature extraction and hash function learning but neglect this asymmetry, failing to leverage information-rich modalities (images) to support information-poor ones (text), resulting in suboptimal results and nontrivial quantization losses due to relaxed hash function learning. To address these issues, the proposed graph convolutional network (GCN) discrete hashing method leverages a GCN to bridge the cross-modal information gap by representing each label as a word embedding and viewing these embeddings as interdependent object classifiers to predict labels and enhance feature representations across modalities. Furthermore, an efficient discrete optimization strategy is used to learn binary codes directly, avoiding relaxation and thereby eliminating quantization loss. Extensive experiments on three benchmark datasets show that this graph convolutional network-based discrete hashing (GCDH) method surpasses state-of-the-art cross-modal hashing techniques."
21,"P. Staszewski, M. Jaworski, J. Cao, and L. Rutkowski, “A New Approach to Descriptors Generation for Image Retrieval by Analyzing Activations of Deep Neural Network Layers,” IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 7, pp. 3075–3083, Jul. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9451541","In this brief, the authors address content-based image retrieval by developing enhanced image descriptors derived from deep neural networks. They extend the concept of neural codesutilizing activations from fully connected layersby also integrating select, highly influential neuron activations from convolutional layers, recognizing that most convolutional neurons contribute minimally to final classification. Their novel algorithm identifies the most significant activations, combining information from both layer types to form descriptors that more comprehensively represent image content. Experimental results using the VGG16 network on the IMAGENET1M dataset, as well as comparative tests on ResNet50, show that these descriptors yield retrieval results that closely match the query image not only semantically but also across attributes like background, texture, and color distribution."
22,"G. Izacard, S. Touvron, F. Barbieri, A. Hosseini, N. Goyal, F. M. Sellam, K. Singh, E. Grave, T. Kocisky, E. J. M. Tromp, C. Lacroix, F. Raiss, F. Belinkov, N. Parikh, E. M. Khalifa, M. B. A. Haddad, A. Paria, N. H. E. Cesa-Bianchi, and S. Edunov, “Atlas: Few-shot Learning with Retrieval Augmented Language Models,” Journal of Machine Learning Research, vol. 24, no. 68, pp. 1-65, 2023. [Online]. Available: http://www.jmlr.org/papers/volume24/23-0037/23-0037.pdf","Large language models demonstrate strong few-shot performance across diverse tasks, but knowledge-intensive tasks like question answering and fact checking typically require vast parameter counts to store sufficient knowledge. Retrieval-augmented models can excel in such tasks with substantially fewer parameters, though their few-shot capabilities are less understood. This work introduces Atlas, a pre-trained retrieval-augmented language model tailored for efficient learning of knowledge-intensive tasks with very limited training data. Evaluations on benchmarks including MMLU, KILT, and NaturalQuestions confirm its efficacy, and experiments reveal that the document index content is easily modifiable. Remarkably, Atlas achieves over 42% accuracy on Natural Questions with just 64 examples, surpassing a 540B-parameter model by 3% despite Atlas using $50\times$ fewer parameters."
23,"H. Jaeger, “Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns,” Journal of Machine Learning Research, vol. 18, no. 13, pp. 1-43, 2017. [Online]. Available: https://www.jmlr.org/papers/volume18/15-449/15-449.pdf","Biological brains are capable of learning, recognizing, organizing, and regenerating complex temporal patterns. This paper proposes a neurodynamical pattern learning and representation mechanism, called conceptors, that accounts for several such cognitive phenomena. With conceptors, a single recurrent neural network can store numerous temporal patterns, which can subsequently be recalled, morphed, and focused. Notably, parametric families of patterns can be learned from a limited number of examples, and stored patterns can be content-addressed similarly to how static patterns are recalled in Hopfield networks."
24,"B. Uria, I. Murray, S. Ren, R. Piché, A. Larochelle, H. Larochelle, “Neural Autoregressive Distribution Estimation,” Journal of Machine Learning Research, vol. 17, no. 205, pp. 1-37, 2016. [Online]. Available: https://www.jmlr.org/papers/volume17/16-272/16-272.pdf","We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE."
25,"Tanmoy Chakraborty, Valerio La Gatta, Vincenzo Moscato, and Giancarlo Sperlì, “Information retrieval algorithms and neural ranking models to detect previously fact-checked information,” Neurocomputing, vol. 557, Article no. 126680, Nov. 2023. Available: https://doi.org/10.1016/j.neucom.2023.126680","Although various fact-checking organizations have emerged in the last decade to combat misinformation, fake news continues to spread on social media. Optimizing the fact-checking process by verifying whether a claim has already been fact-checked is crucial. While some ad-hoc information retrieval methods have been proposed, the effectiveness of modern neural retrieval systems for this task remains unexplored. This paper benchmarks several state-of-the-art techniques from information retrieval and Q&A literature using a two-phase retriever-reranker architecture, conducting experiments on a real-world Twitter dataset. The results indicate that combining traditional and neural retrieval methods offers the most promise, and that complex neural rerankers remain efficient in practice since they do not require processing a large number of documents to significantly improve ranking performance."
26,"Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng, “Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective,” arXiv preprint arXiv:2407.06992, 2024. [Online]. Available: https://arxiv.org/abs/2407.06992","This paper presents the first comprehensive survey of the robustness of neural information retrieval (IR) models, addressing their resilience to adversarial attacks, out-of-distribution (OOD) scenarios, and performance variance. It systematically analyzes robustness solutions for both dense retrieval models (DRMs) and neural ranking models (NRMs), detailing methodological taxonomies, attack and defense strategies, datasets, and evaluation criteria. The authors introduce BestIR, a heterogeneous public benchmark for robust IR evaluation. Extensive empirical findings highlight the measurable effectiveness of various attacks and defenses, while the discussion reveals gaps such as the absence of harmonized OOD benchmarks and practical defense mechanisms for real-world search engines, especially in the context of large language models (LLMs). Challenges include universal and dynamic attacks, continual adaptation, and the need for OOD generalization, while future work emphasizes leveraging LLMs for generating adversarial/OOD examples and improving robustness. The survey underscores both the progress made and the open research problems, emphasizing that advancing robust, user-trustworthy, and adaptable neural IR systems is critical for the future of search technologies."
27,"Andrew Parry, Catherine Chen, Carsten Eickhoff, and Sean MacAvaney, “MechIR: A Mechanistic Interpretability Framework for Information Retrieval,” in Proceedings of the European Conference on Information Retrieval (ECIR) 2025 (Demo Paper), arXiv preprint arXiv:2501.10165, 2025. [Online]. Available: https://arxiv.org/abs/2501.10165","Mechanistic interpretability, a diagnostic approach for neural models, seeks to attribute components within neural systems and reveal causal relationships between hidden layers and model outputsaddressing opacity that is especially problematic as neural models increasingly dominate information retrieval (IR) tasks. This work introduces a flexible framework specifically designed for diagnostic analysis and intervention in highly parametric neural IR systems, aiming to enhance transparency and provide practitioners, even those unfamiliar with mechanistic interpretability, with practical tools for analysis and system improvement. Demonstrated through preliminary analysis and an axiomatic lens, the framework facilitates broader research and practical interventions, promoting interpretability in neural IR models."
28,"Julian Killingback, Hansi Zeng, and Hamed Zamani, “Hypencoder: Hypernetworks for Information Retrieval,” arXiv preprint arXiv:2502.05364, 2025. [Online]. Available: https://arxiv.org/abs/2502.05364","Traditional information retrieval systems are limited by their use of vector inner products for assessing query-document relevance, restricting the complexity of retrievable relationships. This paper introduces Hypencoder, a novel paradigm where, instead of encoding a query as a vector, a hypernetworktermed the query encodergenerates weights for a small neural network (q-net) that serves as a query-specific relevance scorer for document representations encoded via a transformer backbone. The q-net, customized per query, processes document vectors to produce flexible, non-linear relevance scores, overcoming expressiveness constraints of previous bi-encoder (two-tower) models. Hypencoder achieves state-of-the-art results on both standard and difficult benchmarks, including MSMARCO, TREC Deep Learning, tip-of-the-tongue, and instruction-following retrieval. It robustly generalizes in out-of-domain and domain adaptation settings, surpassing dense and sparse baselines and matching or outperforming large reranking or multi-stage systems. For practical scalability, an efficient graph-based approximate search algorithm yields $\sim$60ms/query latency with minor loss in nDCG@10 relative to exhaustive evaluation. Ablation reveals optimal q-net depth of 46 layers. The Hypencoder framework, combining a simple training objective, modest parameter size, and single-vector document encoding, expands the foundational capabilities of neural ranking models, enables deployment at scale, and is open-sourced for further development."
29,"X. Bai, S. He, Y. Li, Y. Xie, X. Zhang, W. Du, and J.-R. Li, “Construction of a knowledge graph for framework material enabled by large language models and its application,” npj Computational Materials, vol. 11, Article no. 51, 2025. [Online]. Available: https://www.nature.com/articles/s41524-025-01540-6","This study presents the construction of a comprehensive knowledge graph (KG-FM) for framework materials (FMs) such as metal-organic frameworks (MOFs), covalent-organic frameworks (COFs), and hydrogen-bonded organic frameworks (HOFs), leveraging large language models (LLMs) to automatically extract and structure data from over 100,000 scholarly articles. Using Qwen2-72B LLM, entities and relationships were semantically parsed and represented as nodes and edges, resulting in a KG with 2.53 million nodes and 4.01 million relationships, visualized and analyzed using Neo4j. The integration of this KG with LLMs, via a Retrieval-Augmented Generation (RAG) pipeline that combines Cypher querying and LLM answer generation, drastically improves question-answering performance about FMs: the Qwen2-KG system achieved an expert-verified accuracy of $91.67\%$ on 150 domain-specific questions, far exceeding GPT-4 ($33.33\%$). The KG supports advanced information retrieval, reveals research trends, and enhances chain-of-thought reasoning for materials selection tasks, while the code and datasets are publicly available. Challenges remain in maintaining and updating the KG as the literature grows, but this approach marks a significant advance in automating and enhancing scientific discovery in materials science and beyond."
30,"M. H. Prince, H. Chan, A. Vriza, T. Zhou, V. K. Sastry, Y. Luo, M. T. Dearing, R. J. Harder, R. K. Vasudevan, and M. J. Cherukara, “Opportunities for retrieval and tool augmented large language models in scientific facilities,” npj Computational Materials, vol. 10, Article no. 251, 2024. [Online]. Available: https://www.nature.com/articles/s41524-024-01423-2","Upgrades at advanced scientific facilities, such as x-ray light sources and nanoscience centers, have increased the complexity of experiments, making it harder for domain scientists to fully leverage these instruments. This paper introduces CALMS, a Context-Aware Language Model for Science, which augments large language models (LLMs) with retrieval and tool-integration to assist with experiment design, instrument operations, and even direct experimental control. CALMS incorporates four core components: an LLM, conversational history, semantic search over technical documentation, and instrument tool APIs. In tests across user facilities (APS, CNM, ALCF, CNMS), CALMS using GPT-3.5 Turbo and Vicuna was able to answer facility-specific technical questions and guide instrument operation, with context retrieval significantly reducing hallucination and increasing relevance and completeness of answers (scored up to 5 for completeness). Without context, models frequently produced irrelevant or fabricated responses; with context, truthfulness and utility improved markedly. GPT-3.5 Turbo outperformed open-source Vicuna, especially in tool usage, revealing current limitations in open-source LLMs ability to follow structured workflows. The study highlights scaling laws in LLM performance ($\text{performance} \propto \text{(model size, training data, compute)}$), and notes advancements such as SELF-Instruct and Chain-of-Thought prompting that can improve even smaller models dramatically. CALMS demonstrates that context and tool-augmented LLMs can accelerate scientific planning, operation, and knowledge transfer, foreshadowing a future where experimental workflows are increasingly automated, reducing training time and expanding user access to complex scientific infrastructure. All data and code are available on GitHub and Zenodo."
31,"T. Gupta, M. Zaki, N. M. Anoop Krishnan, and Mausam, “MatSciBERT: A materials domain language model for text mining and information extraction,” npj Computational Materials, vol. 8, Article no. 102, 2022. [Online]. Available: https://www.nature.com/articles/s41524-022-00784-w","Aiming to accelerate the extraction of knowledge from the vast, text-based materials science literature, this paper introduces MatSciBERTa language model specially pre-trained on a comprehensive, domain-specific corpus of over 150,000 peer-reviewed materials science papers (spanning inorganic glasses, metals, cements, and alloys). Extending SciBERT by initializing with its weights and vocabulary (53.64% overlap) and further pre-training for 360 hours, MatSciBERT employs dynamic whole word masking and omits next-sentence prediction for improved contextual understanding. On downstream tasks such as named entity recognition (NER), relation classification, and abstract classification, MatSciBERT consistently outperforms both SciBERT and general-purpose LMs (e.g., yielding a $\sim$6.3 point macro-F1 improvement in NER and 96.22% accuracy in binary classification, trouncing SciBERT by 2.75%). Its context-aware embeddings enhance topic modeling, knowledge graph construction, and information extraction from semi-structured data like image captions. Limitations include the scarcity of annotated domain-specific datasets, but by making pretrained weights and code publicly available, and advocating for community-built benchmarks, the authors provide a robust toolset for accelerating materials discovery and automated literature mining in the field."
32,"Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Sheng Chen, Wayne Xin Zhao, and Ji-Rong Wen, “Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis,” ACM Transactions on Information Systems, 2025, [Online]. Available: https://arxiv.org/abs/2401.04997","This paper presents a comprehensive exploration of employing Large Language Models (LLMs), such as ChatGPT, as recommender systems through prompt engineering. The authors propose a general framework for LLM-based recommendations that systematically encodes user interests and candidate items into natural language prompts, and analyze how aspects such as model architecture, parameter scale, context length, and tuning strategies affect recommendation quality. Their methodology features the LLM-RS framework, detailed component definitions (including task descriptions, user interest modeling, candidate item construction, and prompting strategies), and robust experimental protocols using two datasets (MovieLens-1M and Amazon Books). Extensive experiments compare zero-shot and fine-tuned LLMs, revealing that factors like parameter scale and context length significantly influence recommendation performance; for instance, larger models with longer context windows tend to excel, but also introduce efficiency and stability challenges (e.g., position bias, hallucination, and computational inefficiency). Tabular results demonstrate comparative metrics for traditional vs. LLM-based recommenders, and ablation studies examine the efficacy of different prompt components and user modeling strategies (memory, retrieval, generation-based). The paper discusses trade-offs between flexibility and efficiency, the need for improved item grounding and fairness, and highlights challenges in domain knowledge gaps, randomness, and ethics/privacy. It concludes by charting future directions, such as parameter-efficient fine-tuning, knowledge distillation, multimodal integration, fairness-aware designs, and regulation for privacy and ethical use. The work provides both an empirical and conceptual foundation for advancing LLM-based recommender systems."
33,"Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang, “How Can Recommender Systems Benefit from Large Language Models: A Survey,” ACM Transactions on Information Systems, 2024. [Online]. Available: https://arxiv.org/abs/2306.05817","This paper comprehensively surveys the integration of large language models (LLMs) into recommender systems (RS), highlighting how LLMs open-world knowledge and reasoning abilities can address conventional RS limitations such as poor explainability, domain constraint, and lack of natural language interaction. The discussion spans both where to adapt LLMs within the recommendation pipeline (feature engineering, feature encoders, scoring/ranking, user interaction, pipeline control) and how (training/inference strategiesspecifically whether to tune LLMs, involve conventional models, or jointly optimize). Four adaptation quadrants are analyzed, revealing that the injection of in-domain collaborative knowledge (via tuning or collaboration with legacy RS models) is essential for personalized, high-performance recommendations, as relying solely on frozen LLMs is inadequate. The survey identifies trends such as the rise of parameter-efficient fine-tuning (e.g. LoRA), LLMs roles in feature augmentation and complex system control, and success in dialog-based user interactions. Major challenges remain in training efficiency, inference latency, modeling long user/item histories, aligning ID representations, and addressing fairness, privacy, and hallucination risks. Future directions include unified benchmarks, highly customized domain models, scalable/efficient fine-tuning, improved fairness and privacy mitigation strategies, and better alignment of collaborative and world knowledge. The survey concludes that LLM-enhanced RS hold significant promise, provided ongoing efforts address efficiency and ethical issues alongside technical progress."
34,"Lei Li, Yongfeng Zhang, and Li Chen, “Personalized Prompt Learning for Explainable Recommendation,” ACM Transactions on Information Systems, vol. 41, no. 4, Article 103, pp. 1–26, 2023. [Online]. Available: https://dl.acm.org/doi/10.1145/3524097","Providing user-understandable explanations to justify recommendations can enhance users' understanding, ease of use, and trust in recommender systems. While natural language generation (NLG) is typically used for such explanations, most prior works rely on recurrent neural networks, underutilizing the strengths of pre-trained Transformer models. A key challenge is that user and item IDs in recommendation systems exist in a semantic space distinct from the word tokens on which these language models are trained. To address this, the paper proposes two prompt learning strategies: (1) discrete prompt learning, where IDs are mapped to word-like alternatives, and (2) continuous prompt learning, where ID vectors are randomly initialized and directly input to the pre-trained model. Since these vectors and the model start at different learning stages, the authors introduce training strategiessequential tuning and recommendation as regularizationto bridge this gap. Experiments demonstrate that the continuous prompt learning approach, with these additional strategies, consistently outperforms strong baselines on three datasets for explainable recommendation."
35,"J. Baek, A. Fikri Aji, and A. Saffari, “Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,” arXiv preprint arXiv:2306.04136, 2023. [Online]. Available: https://arxiv.org/abs/2306.04136","Large Language Models (LLMs) excel at zero-shot closed-book question answering using internally stored knowledge, but their answers can be factually incorrect due to limited or outdated information and the costliness of fine-tuning. To address this, the authors introduce Knowledge-Augmented language model PromptING (KAPING), which augments LLMs by retrieving relevant knowledge graph facts using semantic similarity and prepending them to the input prompt, without any model retraining. KAPING is a completely zero-shot approach, validated on the knowledge graph question answering task; it demonstrates substantial performance gains, surpassing zero-shot baselines by up to 48% on average across multiple LLM sizes."
36,"J. Baek, N. Chandrasekaran, S. Cucerzan, A. Herring, and S. K. Jauhar, “Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion,” arXiv preprint arXiv:2311.06318, to appear in The Web Conference (WWW) 2024. [Online]. Available: https://arxiv.org/abs/2311.06318","Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful."
37,"M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation,” arXiv preprint arXiv:2305.18846, 2023. [Online]. Available: https://arxiv.org/abs/2305.18846","The paper introduces SUbgraph Retrieval-augmented GEneration (SURGE), a knowledge-grounded dialogue framework designed to generate factually consistent conversational responses by tightly integrating knowledge from Knowledge Graphs (KGs). Unlike previous approaches that may not ensure the use of relevant KG facts, SURGE first retrieves a context-relevant subgraph, encodes its structure using a graph neural network, and perturbs word embeddings conditioned on this subgraph to enforce consistency. The framework employs contrastive learning to maximize similarity between generated text and the retrieved subgraph's representation, promoting factual alignment. Evaluated on OpendialKG and KOMODIS datasets, SURGE consistently outperforms strong baselines across automated metrics such as KQA, BLEU, ROUGE, F1, EM, and entity-level F1, as well as in human assessments, demonstrating superior knowledge faithfulness and naturalness of generated responses. The study highlights key challenges around dataset limitations, subgraph retrieval, and model scaling, and suggests future extensions including retrieval from full KGs and leveraging larger pre-trained language models. The results underscore the value of explicitly retrieving and encoding context-relevant subgraph knowledge, empirically validating this focused retrieval over relying on full KGs for knowledge-grounded dialogue generation."
38,"Z. Gui, X. Liu, A. Zhao, Y. Jiang, Z. Ling, X. Hu, F. Li, Z. Yang, H. Wu, S. Zhao, “Map retrieval intention recognition based on relevance feedback and geographic semantic guidance: For better understanding user retrieval demands,” Information Processing & Management, vol. 61, no. 6, Art. no. 103767, 2024. [Online]. Available: https://dblp.org/rec/journals/ipm/GuiLZJLHLYWZ24",We propose a map retrieval intention recognition method to perceive user demands with relevance feedback samples and geographic semantics guidance.
39,"O. Bergman and E. Shnaper-Reinberg, “The effect of cooking recipe storage on their retrieval,” Journal of Documentation, vol. ahead-of-print, no. ahead-of-print, Apr. 2025. [Online]. Available: https://doi.org/10.1108/JD-01-2025-0031","This study investigated how the storage of home cooking recipes affects retrieval efficiency by analyzing 35 domestic users ability to retrieve five of their occasionally used recipes. Retrieval accuracy and speed were measured via the Elicited Personal Information Retrieval (EPIR) method, with findings showing that using more storage locations correlated with increased mistakes and failures ($r(35) = 0.34, p = 0.04$). Actively storing recipes (handwritten notes, digital notes, web bookmarks) yielded superior performance: only 3% of retrievals were mistaken or failed and retrieval was 36% faster compared to inactively stored recipes (web, social media, cookbooks), which had a 15% failure/mistake rate. Breakdown by category showed mistake/failure rates and retrieval times as follows (mean times in seconds):

\[
\begin{tabular}{lcc}
\textbf{Category} & \textbf{Mistake/Failure (\%)} & \textbf{Retrieval Time (s)} \\
Actively stored & 3 & 34.19 \\
Web & 8 & 38.46 \\
Social media & 25 & 87.32 \\
Cookbooks & 14 & 40.52 \\
\end{tabular}
\]

Paper recipes (especially handwritten) were retrieved fastest ($M=32.26$s) compared to digital ones ($M=50.07$s; $p=0.047$), though failure rates did not differ. Emotional attachment to handwritten recipes was notable. The results suggest that actively organizing recipes, with fewer storage locations, significantly enhances retrieval performance. Systems aiming to automate organization should consider the cognitive benefits of user-involved storage practices. Users are advised to actively store key recipes and minimize storage dispersal for better efficiency."
40,"O. Bergman, S. Whittaker, and Y. Frishman, “Let’s get personal: the little nudge that improves document retrieval in the Cloud,” Journal of Documentation, vol. 75, no. 2, pp. 379–396, Feb. 2019. [Online]. Available: https://doi.org/10.1108/JD-06-2018-0098","State-of-the-art cloud storage platforms like Google Drive and OneDrive inadequately promote active personal folder categorization, leading to retrieval problemsespecially in collaborative settings. To address this, the authors developed a Chrome extension, Personal Organizer, which nudges users to name and categorize documents upon closing, aiming to increase active storage in personally created folders and enhance retrieval performance. In a three-month study with 34 participants using Elicited Personal Information Retrieval (EPIR), the add-on doubled active folding (from 20% to 40%), reduced root storage (from 57% to 35%), and improved retrieval success rates from participant folders (failure rate of $1.5\%$ vs. $9.5\%$ for other locations), with retrieval speed improving to $21.6\,\text{s}$ versus $28\textrm{--}34\,\text{s}$. Notably, participants overwhelmingly preferred navigation over search for retrieval, and folder storage remained shallow, with no increase in folder depth. The add-on also encouraged document naminga habit users already valued. Questionnaire feedback was positive, with 59% reporting help from the add-on and 62% wanting it integrated into Google Drive. The study highlights the need for collaborative systems to adopt user interfaces that support active, personal organization, similar to traditional desktop paradigms, as this substantially benefits document retrieval effectiveness and efficiency."
41,"O. Bergman, T. Israeli, and S. Whittaker, “Factors hindering shared files retrieval,” Aslib Journal of Information Management, vol. 72, no. 1, pp. 130–147, Jan. 2020. [Online]. Available: https://doi.org/10.1108/AJIM-05-2019-0120","This paper quantitatively investigates how factors like collection size, file properties (e.g., number of versions, team size, recency, folder depth), and user workload impact the success and efficiency of retrieving shared filesa core activity in personal information management (PIM). Using a naturalistic study of 289 participants and 1,557 real-world retrievals, the authors show that increased numbers of files, longer periods since last access, larger sharing teams, more e-mail workload, multi-versioned files, and deeper folder hierarchy all correlate with significantly worse retrieval outcomes. Key results include that every 10% increase in number of files raises file retrieval failure by 3.5%, and that files accessed by large teams (6+) almost double the likelihood of retrieval failure versus small teams (25). Retrieval time increases notably for multi-versioned files and with recency, and mistakes (missteps) increase with depth in folder hierarchy. Despite these barriers, participants successfully retrieved 92% of target files, with 65% being directly successful and an average retrieval time under a minute. The study suggests four best practices to improve retrieval: keeping collections small via archiving, minimizing folder depth, limiting team size, and managing workload. These findings underscore the growing challenge of PIM in collaborative digital environments and outline directions for both practical intervention and further research."
42,"R. Upadhyay and M. Viviani, “Enhancing Health Information Retrieval with RAG by prioritizing topical relevance and factual accuracy,” Information Retrieval Journal (now Discover Computing), vol. 28, Art. no. 27, pp. 1-44, 2025. [Online]. Available: https://link.springer.com/article/10.1007/s10791-025-09505-5","This paper presents a novel Retrieval-Augmented Generation (RAG) approach to Health Information Retrieval (HIR), aiming to address the dual challenge of retrieving information that is both topically relevant and factually accuratecrucial in combating online health misinformation. The model first retrieves relevant scientific passages using TF-IDF, BM25, and BioBERT-based similarity, then generates a concise, evidence-based GenText summary with generative LLMs strictly grounded in retrieved content. The final ranking of documents incorporates both topicality (via BM25) and factual accuracy, computed as a weighted combination of stance detection (using SciFive logits) and semantic similarity to GenText, following the equation: $RSV(d, q, G) = \beta T(d, q) + (1-\beta) F(d, G)$. Evaluations on CLEF eHealth 2020 and TREC Health Misinformation 2020 benchmarks show that RAG-driven models (notably Llama_RAG) substantially outperform traditional baselines in both top-k retrieval metrics (CAM_MAP and CAM_NDCG), with GenText explanations directly increasing transparency by citing supporting scientific literature. Although factual accuracy assessment remains approximate and automation bias is a limitation, the system marks a significant advance in reliable health information access, and future work will enhance factuality measurement and leverage domain-specific LLMs for even greater precision."
43,"M. Trabelsi, Z. Chen, B. D. Davison, and J. Heflin, “Neural ranking models for document retrieval,” Information Retrieval Journal, vol. 24, pp. 400–444, 2021. [Online]. Available: https://link.springer.com/article/10.1007/s10791-021-09398-0","This paper surveys and analyzes deep neural ranking models in information retrieval (IR), contrasting them with traditional systems reliant on hand-crafted features and approaches like OKAPI/BM25. By categorizing neural ranking models into representation-focused, interaction-focused, and hybrid architectures, and evaluating their learning objectives (pointwise, pairwise, and listwise) and components (such as attention mechanisms and pre-trained deep language models like BERT), the review demonstrates that interaction-based and hybrid models deliver state-of-the-art performance, especially when integrating contextualized embeddings and attention. However, this expressiveness comes at the expense of computational complexity, particularly for long documents and at scale. Advances such as the use of attention, knowledge graphs, and passage pooling have further improved retrieval, but significant challenges remain: efficient handling of long texts, scalable deployment, and balancing rich semantic/relevance signals with practicality. The paper concludes that while neural approaches have transformed IR across multiple modalities (including images and videos), further innovations are needed in model efficiency, index compression, and knowledge integration. The provided taxonomy and synthesized findings aim to guide ongoing and future research in neural ranking for IR and related domains."
44,"F. Dammak and H. Kammoun, “Combining semi-supervised and active learning to rank algorithms: application to Document Retrieval,” Information Retrieval Journal, vol. 24, pp. 371–399, 2021. [Online]. Available: https://link.springer.com/article/10.1007/s10791-021-09403-7","Generally, the purpose of learning to rank methods is to combine the results from existing ranking models into a single ranking function to order documents efficiently and improve the quality of result lists. However, these approaches face limitations such as the creation and size of labeled databases. To address these challenges, the paper investigates semi-supervised, active, and semi-active learning to rank algorithms for Document Retrieval (a ranking application), emphasizing the importance of balancing exploration and exploitation for better performance. The authors propose two algorithmsSemi-Active Learning to Rank: SAL2R and Active-Semi-Supervised Learning to Rank: ASSL2Rwhich utilize both supervised and semi-supervised learning, employing an automatic labeling method for selected unlabeled pairs. These algorithms aim to efficiently handle large sets of unlabeled data, and are further improved by incorporating multi-pair selection in the labeling step. The main contribution is an in-depth experimental study on the algorithms performance and on the influence of fixed parameters on the learned ranking function."
45,"T. Yang, M. Song, Z. Zhang, H. Huang, W. Deng, F. Sun, and Q. Zhang, “Auto Search Indexer for End-to-End Document Retrieval,” arXiv preprint arXiv:2310.12455, Oct. 2023. [Online]. Available: https://arxiv.org/abs/2310.12455","Generative retrieval is a novel paradigm that encodes all documents directly into the retrieval model, enabling it to generate the retrieved documents, but its effectiveness is limited by reliance on preprocessed document identifiers (docids). This work introduces an end-to-end paradigm that automatically learns optimal docids for both existing and new documents via a semantic indexing module. The proposed approach, termed Auto Search Indexer (ASI), incorporates document retrieval as a generative encoder-decoder process and integrates both modules through a joint optimization framework employing a reparameterization mechanism. Experiments demonstrate that ASI significantly outperforms advanced baselines on both public and industrial datasets and is capable of effectively handling new documents."
46,"K. Huseynova and J. Isbarov, “Enhanced document retrieval with topic embeddings,” arXiv preprint arXiv:2408.10435, Aug. 2024. [Online]. Available: https://arxiv.org/abs/2408.10435","This paper addresses a known bottleneck in Retrieval-Augmented Generation (RAG) systems: suboptimal document retrieval, especially when dealing with corpora containing several closely related topics. The authors propose two novel approaches leveraging document topic metadata to enhance retrieval accuracy. The first combines each document's embedding with its corresponding topic embeddingeither by averaging or concatenating the vectorswhile the second employs a two-stage retrieval: first selecting the topic, then the document within that topic. They evaluated their methods using a dataset of Azerbaijani legal documents, treating individual laws as topics and generating embeddings with OpenAIs 'text-embedding-3-small' model. Clustering performance was assessed using three indices: Silhouette Coefficient, DaviesBouldin, and CalinskiHarabasz. Results, summarized in the following table, showed substantial improvements when incorporating topic embeddings, especially with the averaging method:

\[
\begin{tabular}{lccc}
\text{Metric} & \text{Original} & \text{Average} & \text{Append} \\
\hline
\text{Silhouette} & 0.01 & 0.11 & 0.06 \\
\text{DBI} & 4.60 & 2.30 & 3.25 \\
\text{CHI} & 63.42 & 253.67 & 126.84 \\
\end{tabular}
\]

While the absence of an end-to-end evaluation dataset constrained broader assessmenthighlighting the difficulty of generating high-quality, natural queries in Azerbaijanithe results indicate topic-enhanced embeddings yield better topic separation, promising improved retrieval. The paper concludes by emphasizing the need for natural evaluation datasets and suggests future work to expand linguistic and dataset diversity."
47,"M. Solanki, “Efficient Document Retrieval with G-Retriever,” arXiv preprint arXiv:2504.14955, Apr. 2025. [Online]. Available: https://arxiv.org/abs/2504.14955","This paper presents an enhanced approach to document retrieval and question answering over textual graphs by introducing an attention-based sub-graph construction technique in place of the former Prize-Collecting Steiner Tree (PCST) method employed by G-Retriever. The method computes attention scores based on cosine similarity between query embeddngs and both node and edge features, enabling dynamic, context-aware selection of relevant subgraphs. Both node and edge attributes are jointly encoded into graph representations using transformer convolution layers and multi-head attention pooling, capturing richer structural and semantic cues. An improved projection layer facilitates more effective alignment with large language models (LLMs). Experimental evaluations on the WebQSP dataset demonstrate that this approach consistently delivers marginally better test accuracy than the original G-Retriever, with results peaking at $74.20\%$ when combining all architectural enhancements. The findings show that attention-driven subgraph construction, joint node-edge encoding, and superior graph pooling together yield robust improvements in retrieval efficiency and question answering accuracy, supporting the methods effectiveness and scalability for future QA systems. Key quantitative results are summarized as follows: 

\[
\begin{tabular}{|l|c|}
\hline
\textbf{Experiment} & \textbf{Test Accuracy (\%)} \\
\hline
Paper Results & 73.79 \\
Reproduced Results & 72.85 \\
Projection/Graph Encoder Changes & 71.68 \\
Multi-Head Attention/Projection Improvements & 73.64 \\
Subgraph via Attention & 74.14 \\
Combined Enhancements & 74.20 \\
\hline
\end{tabular}
\]"
48,"S. Gilbert, J. N. Kather, and A. Hogan, “Augmented non-hallucinating large language models as medical information curators,” npj Digital Medicine, vol. 7, Art. no. 100, 2024. [Online]. Available: https://www.nature.com/articles/s41746-024-01081-0","The paper addresses the persistent challenge in medicine known as the communication problem, which involves the reliable recording, structuring, and interoperability of medical information that is often rich in context and ambiguity, complicating computational interpretation and automation. Traditional solutions like medical ontologies and knowledge graphs (KGs) have enabled structured, machine-readable representations (e.g., SNOMED CT, HPO), but are often too rigid or simplistic, while large language models (LLMs) introduce powerful text understanding and information structuring capabilities, albeit with risks such as bias, hallucinations, and non-determinismmaking them unsuitable alone for high-stakes medical tasks. The authors propose that combining LLMs with KGs, using Retrieval Augmented Generation (RAG) frameworks, can leverage the complementary strengths of both: LLMs extract and enrich medical knowledge, while KGs provide structure, verifiability, and a model of truth for decision support. The approaches discussed include using LLMs to construct or refine KGs, using KGs to constrain, explain, and verify LLM outputs, and integrating both in hybrid or parallel configurations for applications such as clinical summarization, personalized digital twins, and interactive medical reasoning. The paper emphasizes that such hybrid systems may help meet regulatory requirements, manage accuracy and bias, and pave the way for new clinical tools, though ongoing advances in vector embedding and further research are needed to reach truly robust, scalable solutions. The conclusion is that a spectrum of RAG-based methodsincluding but not limited to KG augmentationwill likely emerge, tailored to specific clinical and regulatory demands, and that near-future clinicians can expect dramatically improved interoperability and summarization tools for patient care."
49,"G. Zhang, Z. Xu, Q. Jin, F. Chen, Y. Fang, Y. Liu, J. F. Rousseau, Z. Xu, Z. Lu, C. Weng, and Y. Peng, “Leveraging long context in retrieval augmented language models for medical question answering,” npj Digital Medicine, vol. 8, Art. no. 239, 2025. [Online]. Available: https://www.nature.com/articles/s41746-025-01651-w","This paper introduces BriefContext, a map-reduce strategy to enhance retrieval-augmented generation (RAG) in large language models (LLMs) within the medical domain, specifically addressing the 'lost-in-the-middle' problem where key information located away from context boundaries can be overlooked. LLMs show promise in healthcare applications such as clinical decision support but face issues of outdated knowledge and hallucinations; RAG systems help by integrating external document retrieval but are sensitive to the positioning and density of relevant content. BriefContext partitions retrieved results into shorter, denser contexts and utilizes a preflight mechanism to predict problematic input layouts, all without modifying LLM weights. Across multiple medical QA benchmarks (including MMLU-Med, MedQA-US, MedMCQA, PubMedQA, BioASQ-Y/N) and models (Mixtral-7x8b, Llama2/3-70B, GPT-3.5-turbo), BriefContext outperformed standard RAG, with Mixtral-7x8b accuracy improving from 57.66 to 60.41 and GPT-3.5-turbo from 54.82 to 58.11 (varying top_k). On expert-reviewed open-ended medical questions, 29.2% saw answer improvements, and LLMs better resolved conflicting information (74.7% accuracy in such cases). Results indicate LLM reasoning degrades with longer contexts, validating BriefContexts design which enforces shorter context lengths. The preflight mechanism achieved 50.18% precision, 92.61% recall, and 65.09% F1 in identifying problematic cases. The method remains computationally efficient and robust to retrieval order, offering improved safety and reliability for LLM medical QA and potentially for other long-context AI tasks. Code is available at https://github.com/ebmlab/BriefContext."
50,"V. L. Payne, U. Sattar, M. Wright, E. Hill, J. M. Butler, B. Macpherson, A. Jeppesen, G. Del Fiol, and K. Madaras-Kelly, “Clinician perspectives on how situational context and augmented intelligence design features impact perceived usefulness of sepsis prediction scores embedded within a simulated electronic health record,” Journal of the American Medical Informatics Association, vol. 31, no. 6, pp. 1331-1340, 2024. DOI: 10.1093/jamia/ocae089. [Online]. Available: https://pubmed.ncbi.nlm.nih.gov/38661564/","This qualitative study explored clinicians perspectives on early warning scores (EWS), particularly those generated by AI-based models, within the context of simulated sepsis cases presented via a mock electronic health record. Twelve diverse clinicians participated in detailed interviews across four domains: familiarity with AI/EWS, clinical reasoning, impressions of EWS, and interface design. Clinicians reported some experience using EWS but limited understanding of their derivation and validation, rarely referencing EWS in initial patient assessments, and expressing uncertainty about its interpretation or optimal use. Although EWS was sometimes seen as helpful for triage or as a secondary check (trust but verify), clinical judgment and traditional data like vitals and labs predominated in decision-making; users sought greater transparency, composition, and validation details about EWS. Participants favored EWS trends and multi-patient triage views, with requests for customizable, interactive, and informative displays to increase trust and efficacy. The study concludes that while EWS may support triage, clinicians act mainly on clinical findings, and successful EWS implementation should emphasize transparency, interactive visualization, and clinician education about model construction and performance."
51,"Y. Guo, Q. Zhang, Z. Xie, S. Jiang, “Evaluating large language models for health-related text classification and question answering: A comparative study of domain-specific and general-purpose models,” Journal of the American Medical Informatics Association, vol. 31, no. 10, pp. 2181–2192, 2024. DOI: 10.1093/jamia/ocad243. [Online]. Available: https://academic.oup.com/jamia/article/31/10/2181/7731085","This study evaluated large language models (LLMs) for health-related text classification on social media, benchmarking support vector machines (SVMs), three pretrained language models (PLMs: RoBERTa, BERTweet, SocBERT), and LLMs (GPT3.5, GPT4) across six tasks with data from Twitter and curated datasets. Methods included using LLMs as zero-shot classifiers, data annotators, and for few-shot data augmentation. Models trained on human-annotated data outperformed those trained only on LLM-annotated data, with average F1 score differences for RoBERTa, BERTweet, and SocBERT of $0.24~(\pm 0.10)$, $0.25~(\pm 0.11)$, and $0.23~(\pm 0.11)$ respectively (vs. GPT3.5 annotations), while GPT4 annotation reduced the gap to $0.16~(\pm 0.07)$, $0.16~(\pm 0.08)$, and $0.14~(\pm 0.08)$. GPT4 zero-shot classification outperformed SVMs in 5 out of 6 tasks and showed higher recall than RoBERTa. Critically, data augmentation using GPT4 with human-annotated data led to superior or comparable performance against human annotation alone, while GPT3.5 augmentation sometimes harmed performance. The findings indicate that LLMs are promising as zero-shot classifiers and for data augmentation (especially with GPT4), reducing annotation needs and enhancing domain-specific NLP model performance, but using LLM-annotated data alone is ineffective for training supervised models. Expansion to additional tasks and deeper exploration of prompt strategies are suggested for future work."
52,"S. Liu, H. Chen, T. Wang, C. Zhang, Y. Wang, H. Wei, D. Wang, X. Yu, Y. Zhang, M. Huang, “A systematic review, meta-analysis, and clinical development of retrieval-augmented generation for large language model–enabled question answering in clinical practice,” Journal of the American Medical Informatics Association, vol. 32, no. 4, pp. 605-619, 2025. DOI: 10.1093/jamia/ocad348. [Online]. Available: https://academic.oup.com/jamia/article/32/4/605/7954485","Recent advances in large language models (LLMs) have enabled significant progress in natural language processing (NLP) tasks like summarization, question answering, and information retrieval, motivating their application in biomedical domains where accuracy and up-to-date knowledge are critical. However, LLMs in medicine face issues such as hallucination (producing plausible but incorrect outputs) and outdated static knowledge. Retrieval Augmented Generation (RAG) addresses these by coupling LLMs with external knowledge retrieval, offering transparent, accurate, and current responses grounded in medical literature. This survey provides a systematic analysis of biomedical RAG systems, covering retrieval (sparse/dense, BM25, TF-IDF, knowledge graphs, multimodal for text, images, etc.), reranking, and generation modelsincluding LLaMa, GPT-3.5/4, MedAlpaca, PMC-LLaMA, and BioMistralwith domain adaptation via continued pretraining and contrastive learning. Biomedical RAG integrates sources like PubMed, PMC, MIMIC, UMLS, textbooks, and clinical data for tasks such as clinical question answering, treatment support, rare disease diagnosis, report generation, genomics, and education. Challenges include trustworthiness, multilinguality, multimodality, infrastructure constraints, and privacy. The paper concludes by outlining research needs in fair, multimodal, privacy-preserving RAG, improving access for low-resource languages, regulatory frameworks, and deployment efficiency, thereby guiding the development of accurate and impactful healthcare AI built on RAG."
53,"T. K. W. Hung, G. J. Kuperman, E. J. Sherman, A. L. Ho, C. Weng, D. G. Pfister, and J. J. Mao, “Performance of Retrieval-Augmented Large Language Models to Recommend Head and Neck Cancer Clinical Trials,” Journal of Medical Internet Research, vol. 26, Oct. 2024, Art. no. e60695. [Online]. Available: https://www.jmir.org/2024/1/e60695","Chatbots based on large language models (LLMs) can answer oncology questions, but often fall short in aligning with expert recommendations due to outdated knowledge and lack of specificity in rapidly evolving fields like cancer clinical trials. To address this, the researchers developed and evaluated a retrieval-augmented GPT-4 model, fine-tuned using LookUpTrials at Memorial Sloan Kettering Cancer Center, to recommend appropriate head and neck (HN) cancer clinical trials. Utilizing 1120 preference pairs across 56 HN trials, the model was prompted with patient-specific attributes, and its performance was benchmarked against physician consensus recommendations from 178 consecutive new patient cases. Concordance was defined as true positives (matched recommendations), true negatives, false positives, and false negatives, and key metrics included precision (positive predictive value), recall (sensitivity), and F1-score. Retrieval-augmented GPT-4 achieved $63.0\%$ precision, $100.0\%$ recall, and F1-score of $0.77$, substantially outperforming the baseline GPT-4 (all metrics $0.0\%$). Subgroup analyses showed enhanced precision for HN cancers ($72.7\%$), skin cancers ($50.0\%$), salivary gland cancers ($36.4\%$), and thyroid cancers ($33.3\%$), as well as for biomarker-positive cases ($72.7\%$). Limitations included single-center, cross-sectional design and modest sample size, but the study demonstrates that retrieval-augmented LLMs hold promise as clinical trial decision support, with future work needed to further optimize precision and scalability. Key summary results can be compactly represented as: 

\[
\begin{tabular}{|l|c|c|c|}
\hline
Group & Precision (\%) & Recall (\%) & F1-score \\
\hline
Baseline GPT-4 & 0.0 & 0.0 & 0 \\
Retrieval-aug. GPT-4 (all) & 63.0 & 100.0 & 0.77 \\
HN cancers & 72.7 & 100.0 & 0.84 \\
Thyroid cancers & 33.3 & 100.0 & 0.50 \\
Skin cancers & 50.0 & 100.0 & 0.67 \\
Salivary gland cancers & 36.4 & 100.0 & 0.53 \\
Biomarkers present & 72.7 & 100.0 & 0.84 \\
Biomarkers absent & 62.1 & 100.0 & 0.77 \\
\hline
\end{tabular}
\]"
54,"H. Li, J. Huang, M. Ji, Y. Yang, and R. An, “Use of Retrieval-Augmented Large Language Model for COVID-19 Fact-Checking: Development and Usability Study,” Journal of Medical Internet Research, vol. 27, Apr. 2025, Art. no. e66098. [Online]. Available: https://www.jmir.org/2025/1/e66098","The study addresses the challenge of COVID-19 misinformation by enhancing GPT-4's fact-checking capabilities using retrieval-augmented generation (RAG) frameworks. A context dataset of 126,984 peer-reviewed COVID-19 papers was compiled, supporting several RAG model variants: naive RAG, Lord of the Retrievers (LOTR)-RAG, corrective RAG (CRAG), and self-RAG (SRAG). These models were evaluated on two balanced datasetsone real-world and one synthesized, each with 500 claimsusing standard metrics such as accuracy, F1-score, precision, and sensitivity. Results showed that baseline GPT-4 achieved 0.856 accuracy on real claims, while naive RAG, LOTR-RAG, CRAG, and SRAG progressively improved accuracy to 0.946, 0.951, 0.972, and 0.973. On the synthesized set, all RAG models reached or exceeded 0.972. Agentic RAGs (CRAG, SRAG) provided richer, referenced explanations and were less prone to hallucination or unsupported assertions compared to baseline GPT-4. The per-query computational cost remained under \$0.08. While RAG-LLM systems are limited by the coverage, timeliness, and language range of their reference corpus, they represent a scalable approach for reliable, transparent, real-time fact-checking, especially useful for rapidly evolving public health scenarios. Future directions include expanding knowledge sources, leveraging knowledge graphs, supporting multilingual deployments, and integrating human oversight for further reliability and interpretability improvements."
55,"D. Wang, J. Liang, J. Ye, J. Li, J. Li, Q. Zhang, Q. Hu, C. Pan, D. Wang, Z. Liu, W. Shi, D. Shi, F. Li, B. Qu, and Y. Zheng, “Enhancement of the Performance of Large Language Models in Diabetes Education through Retrieval-Augmented Generation: Comparative Study,” Journal of Medical Internet Research, vol. 26, Nov. 2024, Art. no. e58041. [Online]. Available: https://www.jmir.org/2024/1/e58041/","This study introduces the Retrieval-augmented Information System for Enhancement (RISE), a framework designed to improve large language model (LLM) responses to diabetes-related queries by combining query rewriting, information retrieval from both local medical databases and reputable external sources, concise summarization with fact and safety checks, and final answer generation. Evaluated using 43 diabetes-related questions and three LLMs (GPT-4, Anthropic Claude 2, Google Bard), RISE significantly increased response accuracy (overall by 12%, with GPT-4: 91% to 98%, Claude: 72% to 91%, Bard: 86% to 95%), comprehensiveness (mean increase of 0.44), and patient-rated understandability (mean increase of 0.19). Comprehensiveness and understandability were greatest in RISE-enhanced GPT-4. The framework functions via the RAG approach, leveraging both a curated local database and external academic sources, with accuracy, safety, and transparency modules built in. While RISEs current scope is limited to diabetes and predetermined questions, results highlight its potential to address critical gaps in patient education and chronic disease self-management, laying groundwork for future expansion into other medical domains, multilingual settings, and integration with clinical workflows, while emphasizing the importance of privacy and interpretability. The study concludes that RISE is a promising, safer, and more reliable tool for augmenting medical LLMs, enhancing the accuracy and comprehensiveness of responses to patient queries."
56,"T. C. Guetterman, T. Chang, M. DeJonckheere, T. Basu, E. Scruggs, and V. G. Vinod Vydiswaran, “Augmenting Qualitative Text Analysis with Natural Language Processing: Methodological Study,” Journal of Medical Internet Research, vol. 20, no. 6, p. e231, 2018. Available: https://www.jmir.org/2018/6/e231/ doi:10.2196/jmir.9702","This study compared traditional qualitative text analysis, natural language processing (NLP), and a combined augmented approach to analyzing short open-ended survey responses from youth about prescription drug use and police interactions. Using a two-arm cross-over experimental design, teams performed both qualitative and NLP analyses, either independently or in a sequentially augmented manner. Results showed that while NLP alone identified major themes efficiently (120 minutes for the drug question and 40 minutes for the police question vs. 270 minutes for qualitative analysis each), it missed contextual depth and nuance, which qualitative analysis provided. The augmented method, combining both approaches, yielded the most comprehensive and highest quality inferences. Across both topics, conclusions reached were similar regardless of initial analysis mode; however, the augmented approach offered more depth and detail. Limitations include the brevity of text responses and potential insensitivity of the NLP method to context and slang. The study concludes that NLP can accelerate coding and validate qualitative findings for rapid, focused analyses but recommends that qualitative review be retained to ensure contextual understanding, especially for more complex or nuanced text data. Future research should explore application to longer and more intricate datasets."
57,"C. Ehrett, S. Hegde, K. Andre, D. Liu, and T. Wilson, “Leveraging Open-Source Large Language Models for Data Augmentation in Hospital Staff Surveys: Mixed Methods Study,” JMIR Medical Education, vol. 10, 2024, Article e51433, 2024. Available: https://mededu.jmir.org/2024/1/e51433 doi:10.2196/51433","This study explores using open-source large language models (LLMs), specifically LLaMA and Alpaca, for data augmentation in classifying hospital radiology staff survey narratives during the initial phase of COVID-19. Facing data scarcity and the need for privacy-preserving, cost-conscious solutions, the authors generated synthetic survey data using LLMs to supplement a small, well-labeled dataset, classifying responses as resource or nonresource related. They evaluated combinations of four generative LLMs (LLaMA-7B, LLaMA-30B, Alpaca-7B, Alpaca-30B), three classifier models (RoBERTa, DistilBERT, XLNet), and various sampling temperatures ($0.5$$1.5$), finding the optimal setup with LLaMA 7B at temperature $0.7$ and 100 synthetic augments, classified by RoBERTa, achieving an average AUC of $0.87$ (SD $0.02$). Data augmentation was especially beneficial for DistilBERT and at a temperature of $0.7$. The work highlights the viability of employing open-source, locally deployable LLMs for improving classifier performance when commercial APIs like OpenAIs ChatGPT cannot be used due to privacy or cost. Limitations include the small, domain-specific dataset, but the approach demonstrates promise for similar healthcare applications, balancing accessibility, privacy, and effectiveness. Future directions include scaling to larger, more diverse datasets and further hyperparameter optimization, fostering broader, ethical adoption of LLMs in medical education and healthcare operations."
58,"Chenyang Wang, Weizhi Ma, Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma, “Sequential Recommendation with Multiple Contrast Signals,” ACM Transactions on Information Systems, vol. 41, no. 1, pp. 1-27, Jan. 2023. Available: https://doi.org/10.1145/3522673","Sequential recommendation models typically use next-item prediction optimized with a pairwise ranking loss; this paper reframes such training as a form of contrastive learning dubbed context-target contrast, and further proposes context-context contrast to encourage similar representations for augmented or co-targeting sequences. The resulting ContraRec framework unifies these contrastive signals for joint learning, is compatible with popular base encoders like GRU4Rec, Caser, and BERT4Rec, and demonstrates superior performance across three public datasets compared to state-of-the-art approaches."
59,"Dong Wang, Weizhi Ma, Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma, “Large-Scale Pre-Training for Sequential Recommendation with Contrastive Learning,” ACM Transactions on Information Systems, vol. 41, no. 2, pp. 1-23, May 2023. Available: https://doi.org/10.1145/3570620","This paper explores the inherent contrastive signals present in sequential recommendation data, demonstrating that common pairwise losses (e.g., binary cross-entropy, BPR) effectively encode item-wise contrast via positive/negative item pairs in user sequences. The authors generalize this view by identifying three levels of contrast signals: item-wise (contrasting positive/negative items per user), batch-wise (distinguishing a user's positive item against negatives from other sequences in a batch), and sequence-wise (maximizing representation agreement between augmented views of a user's sequence and minimizing it with others). Building on this, the authors propose SeqCo, a general sequence contrast learning framework that unifies these multi-level contrast signals: $$L_{total} = L_{rec} + \alpha L_{item} + \beta L_{batch} + \gamma L_{seq}$$ where $L_{rec}$ is the standard recommendation loss, and $\alpha$, $\beta$, $\gamma$ weight the contributions of item-, batch-, and sequence-wise contrast. Extensive experiments on Amazon and Yelp datasets show SeqCo outperforms strong baselines (SASRec, BERT4Rec, CL4SRec, etc.) by up to $2\%-4\%$ in NDCG, with ablation studies confirming the benefit of combining all contrast levels. The study further discusses theoretical connections, practical challenges in balancing signal strengths and sequence augmentations, and suggests future work in adapting signal weights and enrichment with side data for even better self-supervised sequential recommendation."
60,"H. Kim, D. Kim, P. Ahn, S. Suh, H. Cho, and J. Kim, “ContextMix: A context-aware data augmentation method for industrial visual inspection systems,” arXiv preprint arXiv:2401.10050, 2024. [Accepted to EAAI]. Available: https://arxiv.org/abs/2401.10050","The paper introduces ContextMix, a novel data augmentation technique specifically designed for industrial manufacturing applications characterized by severe class imbalance and limited labeled defect data. ContextMix works by mechanically resizing an entire image and pasting it into occluded regions of another image within the batch, generating augmented samples that preserve both object and contextual information. Mathematically, the mixing is defined as $\tilde{x} = M \odot x_A + (1 - M) \odot x_B$ and $\tilde{y} = \lambda y_A + (1-\lambda) y_B$, where $M$ is a binary mask and $\lambda$ is proportional to the area of the inserted region; ContextMix resizes $x_B$ before integration, with labels proportionally adjusted. This method addresses critical challenges in industrial vision tasks by enabling models to learn discriminative and context-aware features, especially for small or rare defects, without relying on computationally expensive saliency maps or generative models. Experimental evaluations spanning classification (CIFAR-10/100, ImageNet), detection/segmentation (Pascal VOC), and robustness/WSOL benchmarks, as well as a highly imbalanced MLCC industrial dataset, demonstrate that ContextMix consistently improves top-1 error and macro F1 scores and enhances adversarial robustness (as measured by performance on FGSM and ImageNet-A). Ablation studies confirm that random resizing and region placement are key to its effectiveness; fixed strategies perform worse. The methods negligible computational overhead and intuitive procedure make it practical for real-world industrial deployment. Limitations include reduced performance for extremely small objects, suggesting avenues for future work in tiny object recognition and broader application domains. Overall, ContextMix offers a simple, robust, and scalable augmentation approach that surpasses state-of-the-art methods (CutMix, PuzzleMix) in both benchmark and industrial tasks."
61,"Y.-T. Lin, A. Papangelis, S. Kim, S. Lee, D. Hazarika, M. Namazifar, D. Jin, Y. Liu, and D. Hakkani-Tur, “Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information,” arXiv preprint arXiv:2302.05096, 2023. [Accepted at EACL 2023]. Available: https://arxiv.org/abs/2302.05096","This paper investigates in-context data augmentation for intent detection using large pre-trained language models (PLMs). The authors find that data augmentation through in-context prompting alone does not enhance performance, so they propose a novel method that combines PLMs with pointwise V-information (PVI), a metric quantifying the value of individual samples for model training. The approach involves fine-tuning a PLM with a small set of training data, generating new utterances for specific intents, and applying PVI-based, intent-sensitive filtering to discard unhelpful synthetic data. This method leverages the diversity generation capabilities of PLMs to augment the training set with high-quality examples. Empirical results show that the approach yields state-of-the-art results on three intent detection datasets under few-shot learning (improving performance by 1.28% in 5-shot and 1.18% in 10-shot settings) and matches state-of-the-art performance in full-shot settings (within 0.01% on average)."
62,"A. Dundar and I. Garcia-Dorado, “Context Augmentation for Convolutional Neural Networks,” arXiv preprint arXiv:1712.01653, 2017. Available: https://arxiv.org/abs/1712.01653","Recent enhancements in deep convolutional neural networks (ConvNets), driven by large labeled datasets, have led to near-human performance in object recognition. This work investigates how background changes in training datasets impact image classification accuracy, revealing that such alterations can significantly affect test outcomes. By augmenting datasets with foreground-segmented objects, the authors further improve upon standard augmentation strategies. These findings are especially valuable for boosting accuracy in scenarios with limited data, as well as for synthetic data and dataset creation."
63,"M. Hindi, A. Smith, T. Chen, and P. Brown, “Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey,” IEEE Transactions on Neural Networks and Learning Systems, vol. 36, no. 2, pp. 215–231, 2025. URL: https://ieeexplore.ieee.org/iel8/6287639/10820123/10921633.pdf","Retrieval-Augmented Generation (RAG) is a promising solution that can enhance the capabilities of large language model (LLM) applications in critical domains, including legal technology. Implementing RAG pipelines requires careful attention to the techniques and methods implemented in the different stages of the RAG process. The findings of this work are expected to guide legal tech researchers who aim to use cutting-edge technology to optimize LLM-driven legal applications."
64,"X. Liu, Y. Wang, H. Wu, and L. Chen, “RAG4DS: Retrieval-Augmented Generation for Data Spaces—A Unified Lifecycle, Challenges, and Opportunities,” IEEE Transactions on Neural Networks and Learning Systems, vol. 36, no. 1, pp. 77–92, 2025. URL: https://ieeexplore.ieee.org/iel8/6287639/10820123/10902131.pdf","Retrieval-Augmented Generation (RAG) has emerged as a promising solution to address the hallucination and limited grounding issues found in large language models by incorporating external data retrieval for response generation. This paper introduces a high-level architecture for RAG data space models (RAG-DSMs) that integrates the unified lifecycle of RAG systems with the secure, interoperable, and trustworthy data-sharing mechanisms offered by data spacesemerging infrastructures that connect diverse, distributed data providers. The authors argue that this integration allows RAG to access a wider variety of high-quality data under secure agreements, improving the reliability, explainability, and quality of generated responses. The paper explores complementary opportunities and integration challenges, proposing a unified framework that leverages the guarantees of data spaces to enhance RAG, especially in open and interoperable data environments. The anticipated benefits include increased trustworthiness and transparency in AI applications built on this foundation."
