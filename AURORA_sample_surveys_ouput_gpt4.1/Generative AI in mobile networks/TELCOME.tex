\documentclass[sigconf]{acmart}
 \usepackage{graphicx}      % For figures
\usepackage{booktabs}      % For professional tables
\usepackage{multirow}      % Multi-row cells in tables
\usepackage{array}         % Table column alignment
\usepackage{tabularx}      % Auto-adjust table widths
\usepackage{float}         % Force figure/table placement
\usepackage{algorithm}     % Algorithm environment
\usepackage{algpseudocode} % Algorithm pseudocode
\usepackage{amsmath}       % Math equations
\usepackage{hyperref}      % Clickable references
\usepackage{adjustbox}     % Resize tables/figures
\usepackage{xcolor} 
\usepackage{balance}% Color for highlighting text
\usepackage{algorithmicx}



\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{URL_TO_YOUR_ARTIFACTS}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 
\title{Survey: The Convergence and Advancement of AI, Generative AI, and Vector Databases in Modern Telecommunications and Wireless Networking}


\begin{document}
\maketitle
\section{Introduction}

\subsection{The Evolving Telecommunication Landscape and the Rise of AI}
\label{sec:intro_landscape}

The telecommunications sector is experiencing a profound transformation, characterized by the progression towards 5G, Beyond 5G (B5G), and the anticipated 6G networks \cite{ref7, ref9, ref17, ref34}. This evolution is amplified by the expansion of the Internet of Things (IoT) \cite{ref15, ref26}, the emergence of sophisticated vehicular networks \cite{ref25}, and the integration of cloud, edge, and fog computing paradigms \cite{ref3, ref14, ref25, ref31}. A direct consequence of these developments is a significant escalation in network complexity and heterogeneity \cite{ref6, ref9, ref17, ref25, ref27}, further intensified by the exponential increase in data generated and consumed across these modern infrastructures \cite{ref27}. This escalating complexity severely challenges traditional network management strategies, which typically rely on static counters, rule-based systems, and conventional communication protocols \cite{ref17, ref21, ref25, ref28, ref32}. Such legacy approaches struggle to adapt to the dynamism, scale, and stringent performance requirements of new services, hindering the consistent delivery of high network performance and Quality of Service (QoS) \cite{ref11}, elements crucial for user satisfaction and the viability of emerging applications \cite{ref1, ref11}.

In response to these formidable challenges, Artificial Intelligence (AI) and Machine Learning (ML) have emerged as critical enabling technologies \cite{ref12, ref17, ref20, ref25, ref26}. Over the last decade, ML techniques have increasingly been applied in wireless network research and operations, demonstrating substantial potential to automate and optimize network functions by learning from extensive data \cite{ref1, ref12, ref32}. The application domains for ML in wireless networks have broadened significantly \cite{ref1}, encompassing resource allocation, traffic engineering, QoS management, security enhancements \cite{ref12}, and the facilitation of intelligent information transmission and processing \cite{ref32}. Consequently, AI is widely recognized as indispensable for navigating the operational complexities inherent in 5G and future networks \cite{ref6, ref9, ref25}, automating tasks previously managed manually or through inflexible algorithms \cite{ref28}.

However, realizing the full vision for future networks requires AI capabilities that transcend conventional ML paradigms. A notable paradigm shift is currently underway, driven by the emergence of Generative AI (GenAI) \cite{ref2, ref9, ref19, ref20, ref26, ref31, ref33}, Foundation Models (FMs) \cite{ref16, ref17}, Large Language Models (LLMs) \cite{ref2, ref4, ref8, ref14, ref17, ref18, ref21, ref25}, Agentic AI \cite{ref35}, and supporting technologies like Vector Databases (VecDBs) \cite{ref8, ref14}. GenAI techniques, such as Generative Diffusion Models (GDMs) \cite{ref19, ref33}, offer transformative potential through their ability to generate novel content, synthesize realistic data, learn complex data distributions, and potentially revolutionize network optimization and security strategies \cite{ref5, ref9, ref19, ref20, ref26}. The proliferation of powerful AI-Generated Content (AIGC) platforms, exemplified by LLM-driven chatbots like ChatGPT and Bard \cite{ref2, ref26}, hints at substantial opportunities for diversifying network services and applications \cite{ref15, ref26, ref31}. Simultaneously, Semantic Communication (SemCom) is gaining traction as a novel paradigm focused on conveying meaning rather than achieving perfect bit replication, promising significant efficiency improvements, especially for AIGC transmission \cite{ref22, ref23}. These advanced AI approaches, combined with Agentic AI systems capable of autonomous perception, reasoning, and action \cite{ref35}, and FMs acting as versatile bases for various downstream tasks \cite{ref16, ref17}, are poised to redefine network intelligence.

\subsection{The Symbiotic Relationship between AI and Advanced Networks (5G/B5G/6G)}
\label{sec:intro_symbiosis}

The relationship between AI and advanced communication networks, including 5G, B5G, and 6G, is inherently symbiotic \cite{ref7}. AI, especially ML, has become crucial for realizing the demanding specifications of 5G and beyond, enabling complex applications such as autonomous driving, industrial automation, virtual and extended reality (VR/XR), and remote healthcare services, all of which necessitate unprecedented levels of reliability, low latency, and adaptability \cite{ref1}. Looking ahead to 6G, AI is anticipated to be not merely an optimization tool but a foundational element of the network architecture itself \cite{ref7, ref13, ref34}. GenAI, FMs, and LLMs are expected to assume increasingly significant roles in the design, configuration, operation, and optimization of these future wireless systems \cite{ref2, ref7, ref9, ref17}. This co-evolution mirrors a broader historical trend in wireless communications, transitioning from predominantly manual operations to more sophisticated, module-based systems, and now entering an era characterized by AI-driven intelligent information handling \cite{ref32}. Advanced networks provide the essential connectivity and data infrastructure for training and deploying large-scale AI models, while AI, in turn, delivers the intelligence required to manage network complexity and unlock the full potential of these sophisticated communication systems \cite{ref3, ref31}.

\subsection{Need for Advanced AI Capabilities in Modern Networks}
\label{sec:intro_need}

While existing AI applications have yielded considerable benefits, the pursuit of fully autonomous and intelligently managed future networks demands AI systems endowed with more advanced cognitive capabilities. There is a distinct and growing need for AI techniques capable of critical thinking, integrating sophisticated reasoning and planning functionalities \cite{ref6}. Although traditional symbolic AI approaches provide explicit reasoning mechanisms, they often struggle with the costs associated with knowledge acquisition and ensuring scalability. Conversely, the critical thinking capacity of emergent GenAI models is still under active investigation \cite{ref6}. Bridging this gap is essential for empowering AI to undertake complex decision-making tasks within dynamic network environments, thereby moving beyond pattern recognition and classification towards genuine network comprehension and predictive foresight \cite{ref6, ref35}.

\subsection{Motivation, Scope, Objectives, and Structure of the Survey}
\label{sec:intro_motivation}

The transformative potential of AI—spanning established ML methods to the rapidly advancing fields of GenAI, FMs, LLMs, and Agentic AI—underscores the need for a comprehensive understanding of its application within the telecommunications domain \cite{ref31, ref32, ref34, ref35}. This survey aims to provide a structured overview and critical analysis of the state-of-the-art regarding the application of these advanced AI techniques in modern communication networks.

The scope encompasses a broad spectrum of AI methodologies pertinent to telecommunications. This includes established ML techniques \cite{ref1, ref12}, GenAI paradigms like GDMs \cite{ref19, ref20, ref33}, LLMs and FMs \cite{ref2, ref8, ref16, ref17, ref18, ref26}, the enabling role of technologies such as VecDBs \cite{ref8, ref14}, the burgeoning field of SemCom \cite{ref22, ref23}, and the prospective impact of Agentic AI \cite{ref35}. We explore the deployment of these diverse AI techniques across critical network functions, including network operation and management \cite{ref6, ref9, ref25, ref28}, resource allocation and optimization \cite{ref9, ref11, ref12, ref19, ref21}, network security \cite{ref5, ref12, ref20, ref26, ref30}, customer service enhancement \cite{ref2}, IoT enablement \cite{ref15, ref26, ref29}, SemCom facilitation \cite{ref22, ref23, ref26}, promotion of network sustainability \cite{ref29}, realization of the AI-native Radio Access Network (AI-RAN) vision \cite{ref34}, and the development of sophisticated reasoning capabilities within network AI systems \cite{ref6, ref17, ref21, ref35}.

Furthermore, this survey critically examines the significant technical challenges, requisite infrastructure adaptations (spanning edge computing, cloud-edge synergy, architectural patterns, and specialized hardware) \cite{ref2, ref3, ref4, ref6, ref8, ref9, ref12, ref14, ref16, ref17, ref18, ref20, ref21, ref22, ref23, ref25, ref26, ref29, ref31, ref34, ref35}, and practical implementation considerations associated with deploying these advanced AI models in operational telecom environments. Finally, we synthesize our findings to identify key open research questions and delineate promising avenues for future investigation \cite{ref1, ref3, ref4, ref5, ref6, ref8, ref10, ref12, ref13, ref15, ref16, ref17, ref18, ref19, ref20, ref21, ref23, ref25, ref26, ref29, ref30, ref31, ref34, ref35}.

\subsection{Outline Structure}
\label{sec:intro_outline}

The remainder of this paper is organized as follows...
% [Section detailing the paper structure would follow here].

\section{Foundational Concepts: AI, Databases, and Communication Paradigms}

The convergence of artificial intelligence (AI), advanced database technologies, and novel communication paradigms is fundamentally reshaping the landscape of network design, operation, and management. This section elucidates the foundational concepts underpinning this transformation. We trace the evolution of AI applications within communications, detail key AI paradigms including traditional and generative approaches, and introduce critical enabling technologies such as vector databases and semantic communication.

\subsection{The Evolution Towards Intelligent Communications}

The trajectory of wireless communication systems reveals a dramatic shift from early manual operations, exemplified by Morse code, towards highly automated, module-based systems encompassing coding, modulation, and signal processing \cite{ref32}. This conventional research methodology—characterized by systematic problem analysis, model construction, algorithm design, and empirical verification—provided the foundation for modern communications. However, it faces inherent limitations when confronted with the escalating scale, complexity, and dynamism of contemporary and future networks \cite{ref32}. The advent of machine learning (ML) marked a pivotal transition, offering mechanisms to overcome the inefficiencies and constraints associated with traditional, model-dependent approaches \cite{ref32}. By enabling systems to learn intricate patterns and behaviors directly from vast amounts of operational network data, ML facilitates the development of more adaptive, efficient, and intelligent communication systems and algorithms. This progression has paved the way for AI-driven network automation, optimization, and intelligent information processing \cite{ref1, ref32}.

\subsection{Traditional AI/ML in Network Management}

Early adoptions of AI and ML in networking, particularly Artificial Neural Networks (ANNs) and subsequently Deep Learning (DL), demonstrated significant potential for enhancing network and service management tasks \cite{ref28}. These techniques leverage the capacity of deep neural architectures to perform representation learning, automatically discovering and extracting salient features from raw network data. This data-driven feature extraction often proves more effective than relying on pre-defined, manually engineered features \cite{ref28}. The practical application and success of DL in this domain have been significantly propelled by the confluence of two key factors: the availability of Big Data generated during routine network operations, providing the necessary substrate for training data-intensive models, and advancements in high-performance computing infrastructure, notably the widespread adoption of Graphics Processing Units (GPUs) that drastically accelerate computationally demanding training processes \cite{ref2, ref28}. A salient example involves predictive fault management within mobile core networks, such as forecasting specific errors during Voice over LTE (VoLTE) call establishment by analyzing historical event data and statistical features of message processing delays. This illustrates the utility of DL in proactively identifying and potentially mitigating network faults based on learned patterns from operational history \cite{ref28}.

\subsection{Discriminative AI and Machine Learning Paradigms}

Machine learning techniques have found broad application across diverse facets of networking \cite{ref12}, including network design optimization, dynamic resource allocation, sophisticated traffic engineering, Quality of Service (QoS) management, and network security enhancement \cite{ref1, ref12}. Many of these applications leverage \textit{discriminative} AI models, trained to discern decision boundaries between predefined categories or classes based on input data \cite{ref6}. The principal ML paradigms employed are:
\begin{itemize}
    \begin{itemize}
\item \textbf{Supervised Learning:} Models are trained on datasets where inputs are paired with known correct outputs (labels). Common applications include network traffic classification and identifying known security threats \cite{ref1}.
\end{itemize}
    \begin{itemize}
\item \textbf{Unsupervised Learning:} Algorithms identify inherent structures, patterns, or anomalies within unlabeled data. Uses encompass detecting novel network anomalies, clustering users based on behavior, and dimensionality reduction of network data \cite{ref1}.
\end{itemize}
    \begin{itemize}
\item \textbf{Reinforcement Learning (RL):} Agents learn optimal strategies or policies through direct interaction with the network environment, receiving feedback via rewards or penalties. RL is particularly suited for dynamic control problems like adaptive resource allocation, routing optimization, and autonomous network configuration \cite{ref1}.
\end{itemize}
\end{itemize}
Illustrative use cases demonstrating the value of these techniques include optimizing energy consumption in cellular base stations, implementing dynamic load balancing strategies for efficient traffic distribution, and detecting anomalous network behavior indicative of performance degradation or security breaches \cite{ref1, ref6}. Despite their proven utility, these predominantly discriminative approaches often encounter difficulties when faced with highly complex problems demanding critical thinking capabilities, such as nuanced analysis, systematic evaluation of multifaceted information, and sophisticated planning or reasoning that extends beyond pattern recognition \cite{ref6}.

\subsection{Symbolic AI Approaches}

As an alternative to purely data-driven methods, Symbolic AI concentrates on representing knowledge explicitly using symbols (e.g., facts, rules, logical predicates) interpretable by both humans and machines \cite{ref6}. This paradigm excels in tasks requiring structured reasoning and planning grounded in established knowledge bases \cite{ref6}. Potential networking applications include expert systems for troubleshooting, intent-based networking where high-level goals are translated into concrete configurations, and automated customer assistance based on documented procedures \cite{ref6}. However, the practical deployment of Symbolic AI within dynamic telecommunication environments confronts significant obstacles. These include the often prohibitive cost and manual effort involved in curating and maintaining comprehensive, accurate knowledge bases; potential challenges in scaling reasoning processes as knowledge bases expand; inherent brittleness when handling noisy, incomplete, or ambiguous real-world data; and the high computational complexity associated with many fundamental logical reasoning tasks \cite{ref6}.

\subsection{Generative AI (GenAI) Models and Principles}

Generative Artificial Intelligence (GenAI), encompassing technologies frequently associated with AI-Generated Content (AIGC), represents a significant paradigm shift within AI \cite{ref26, ref31}. Unlike conventional AI models (often discriminative) that primarily focus on analyzing, classifying, or predicting from existing data \cite{ref19, ref26}, GenAI models are fundamentally designed to \textit{create} novel, synthetic data instances exhibiting characteristics similar to their training data \cite{ref5, ref19, ref26}. By learning the underlying structure, patterns, and complex probability distributions within massive datasets, GenAI can produce diverse and original outputs, including text, computer code, images, audio, simulations, and synthetic datasets suitable for training other AI models \cite{ref5, ref9, ref22, ref23, ref24, ref26, ref31}. This core capability of content creation fundamentally distinguishes GenAI from conventional AI/ML systems \cite{ref19, ref26}.

The GenAI landscape is built upon several key technologies and model architectures:
\begin{itemize}
    \begin{itemize}
\item \textbf{Deep Generative Models (DGMs):} A broad category encompassing various deep learning-based generative techniques \cite{ref26}.
\end{itemize}
    \begin{itemize}
\item \textbf{Large Language Models (LLMs):} Specialized DGMs, typically based on the Transformer architecture, trained extensively on text and code, excelling at language understanding and generation \cite{ref26, ref29} (further detailed in Section 2.6).
\end{itemize}
    \begin{itemize}
\item \textbf{Generative Diffusion Models (GDMs / DMs):} Models recognized for their iterative process of adding noise to data and learning the reverse (denoising) process to generate high-fidelity samples, particularly effective in domains like image generation \cite{ref19, ref29, ref33}.
\end{itemize}
    \begin{itemize}
\item \textbf{Other Relevant Models:} Techniques such as Generative Adversarial Networks (GANs), Autoencoders (AEs), and Variational Autoencoders (VAEs) remain valuable for specific generative tasks, including image synthesis or anomaly detection via reconstruction errors \cite{ref5}.
\end{itemize}
\end{itemize}
From a learning perspective, many GenAI approaches function as powerful unsupervised learning techniques, capable of discerning intricate data characteristics and patterns to replicate or innovate upon existing data without requiring explicit labels \cite{ref20}. The transformative potential of GenAI is actively being explored across numerous communication and networking domains, such as intelligent network management and optimization, enhancing cybersecurity measures, enabling novel communication paradigms like semantic communication, and generating realistic synthetic network data for simulation and training purposes \cite{ref9, ref26, ref31}.

\subsection{Large Language Models (LLMs)}

Large Language Models (LLMs) constitute a particularly impactful category of GenAI and are increasingly viewed as foundational components for future intelligent systems \cite{ref16}. Architecturally, many state-of-the-art LLMs, including the GPT series, T5, and Llama \cite{ref8}, are based on the Transformer model \cite{ref2, ref8, ref9, ref18, ref25}. These models undergo extensive pre-training on vast and diverse corpora of text and code \cite{ref8, ref25}. This process endows them with remarkable capabilities in natural language processing (NLP), encompassing deep language understanding, context awareness, complex reasoning, and the generation of human-like text for tasks such as translation, summarization, question answering, and code generation \cite{ref9, ref21, ref25}. Adapting these general-purpose models for specific applications or domains typically involves techniques like prompt engineering (carefully formulating inputs to guide model behavior) and fine-tuning (supplementary training on smaller, task-specific or domain-specific datasets) \cite{ref8, ref18, ref25}. Despite their power, the sheer scale of LLMs presents significant practical challenges. Their training and inference demand substantial computational resources, resulting in high operational costs and potentially significant latency, which can be a critical bottleneck for real-time telecommunications applications \cite{ref14}.

\subsection{Agentic AI Paradigm}

Addressing the escalating complexity, dynamism, and scale inherent in modern telecommunication networks necessitates a shift towards more autonomous and intelligent systems \cite{ref35}. The Agentic AI paradigm provides a framework for developing such systems, envisioning autonomous agents capable of perceiving their environment (e.g., network state, user requests), reasoning about this perceived information, making informed decisions, and executing actions within the network \cite{ref35}. This approach, often leveraging LLMs for their reasoning and decision-making capabilities, aims to facilitate the creation of self-organizing, highly adaptive, and resilient network architectures capable of managing unforeseen situations and evolving demands \cite{ref35}. A critical enabler for effective agentic AI, particularly in complex telecom applications requiring planning, compliance adherence, and historical context (e.g., network planning, resource allocation, fault management), is the integration of sophisticated generative information retrieval mechanisms \cite{ref35}. Beyond simple keyword search, advanced strategies are essential, encompassing traditional retrieval, semantic search (based on meaning similarity), knowledge-based retrieval (querying structured knowledge bases like KGs), and emerging agentic contextual retrieval frameworks \cite{ref35}. These advanced retrieval capabilities empower agents to perform multi-hop reasoning (connecting disparate information pieces), cross-reference historical data and operational logs, and ensure adherence to complex, evolving standards and policies (such as those from 3GPP) during their decision-making processes \cite{ref35}.

\subsection{Enhancing GenAI/LLMs and Data Management Technologies}

While foundational GenAI models, particularly LLMs, exhibit impressive general capabilities, their direct application to specialized domains like telecommunications often requires significant adaptation and enhancement \cite{ref4, ref17, ref18}. General models may lack the nuanced understanding of specific network protocols, industry standards (e.g., 3GPP, O-RAN Alliance specifications), the physics governing wireless propagation, or the real-time operational context \cite{ref4, ref17}. Consequently, several key technologies and methodologies are employed to bridge this gap and augment GenAI/LLM capabilities for telecom applications, as summarized in Table~\ref{tab:genai_enhancements}.

\begin{table*}[htbp]
\centering
\caption{Key Techniques for Enhancing GenAI/LLMs in Telecommunications}
\label{tab:genai_enhancements}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}lp{6cm}p{5cm}@{}}
\toprule
Technique & Description/Purpose & Key Benefit / Use Case Example \\
\midrule
Prompt Engineering \& In-Context Learning & Structuring input prompts effectively and providing examples within the prompt to steer model output without parameter changes \cite{ref17, ref18}. & Guiding an LLM to generate configurations in a specific format or style. \\
Retrieval-Augmented Generation (RAG) & Dynamically integrating external knowledge during generation. The LLM queries an external source (e.g., VecDB, KG) for relevant, current information to incorporate into its response \cite{ref2, ref4, ref17, ref21, ref29}. & Answering queries about recent 3GPP standards; reducing hallucinations by grounding responses in factual documents. \\
Fine-tuning \& Domain-Specific Pre-training & Further training pre-trained LLMs on curated telecom datasets, or developing models trained primarily on telecom data \cite{ref17, ref18, ref25}. & Creating models specialized in understanding network logs or telecom standards (e.g., TeleRoBERTa for 3GPP docs \cite{ref18}). \\
Tool Usage Integration & Enabling LLMs to interact with external software tools, APIs, or simulators \cite{ref17}. & Allowing an LLM to query real-time network status via an API or trigger a simulation. \\
Multi-modal Capabilities & Extending models beyond text to process and reason about diverse data types (e.g., signal measurements, spectrum data, diagrams) \cite{ref17}. & Analyzing network performance based on both textual logs and signal strength charts. \\
Agentic Contextual Retrieval & Sophisticated retrieval strategies within agentic frameworks supporting complex, multi-step reasoning requiring dynamic information access \cite{ref35}. & Enabling an agent to plan network upgrades by retrieving technical specs, historical performance, and compliance rules. \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

Effective data management technologies are indispensable companions to these AI techniques. \textbf{Knowledge Graphs (KGs)} offer a structured representation of domain entities (e.g., network functions, protocols, devices) and their interrelationships. This provides explicit, organized knowledge that can be integrated with LLMs, often via RAG as indicated in Table~\ref{tab:genai_enhancements}, to enhance contextual understanding and logical reasoning \cite{ref4}. Concurrently, \textbf{Vector Databases (VecDBs)} have emerged as crucial infrastructure for managing the high-dimensional vector embeddings native to many modern AI models, including LLMs \cite{ref8}. VecDBs enable efficient storage, indexing, and large-scale similarity searching of these embeddings, thereby powering applications like semantic search, recommendation systems, and, critically, RAG \cite{ref8}. Their synergistic integration with LLMs helps mitigate core limitations such as hallucinations and reliance on potentially outdated internal knowledge by providing a mechanism to ground responses in current, domain-specific facts retrieved from the database \cite{ref8}. Furthermore, techniques like \textbf{vector caching}, which involves storing frequently accessed query-answer pairs or embeddings in an edge cache, can optimize performance by reducing latency and computational load for recurring requests \cite{ref14}. This combination of VecDBs and LLMs promises more scalable, cost-efficient, and capable systems for advanced data handling, knowledge extraction, and retrieval in demanding network environments \cite{ref8}. Overall, GenAI and LLMs possess the potential to significantly advance telecommunications by learning complex network dynamics directly from operational data, potentially reducing dependence on exhaustive, manually curated knowledge bases \cite{ref6, ref9}.

\subsection{Semantic Communication (SemCom)}

Shifting focus from AI mechanisms to the fundamental nature of information exchange, Semantic Communication (SemCom) introduces a paradigm distinct from traditional communication theory. The latter, following Shannon, primarily emphasizes the accurate reproduction of transmitted bits or symbols \cite{ref22, ref23, ref24}. SemCom, conversely, prioritizes the successful conveyance and interpretation of the underlying \textit{meaning} or \textit{semantic content} of the information \cite{ref22, ref23, ref24}. This approach inherently relies on shared background knowledge and contextual understanding between communicating entities (transmitter and receiver) to enable effective encoding and decoding of semantic intent \cite{ref23}. By concentrating on transmitting only the essential meaning rather than every bit, SemCom holds significant promise for dramatically improving communication efficiency (particularly regarding bandwidth and energy consumption) and enhancing reliability, especially in scenarios with noisy or constrained channels where perfect bit-level fidelity is challenging or superfluous \cite{ref22, ref23, ref24}. GenAI models are increasingly viewed as crucial enablers for practical SemCom systems, potentially driving advancements in semantic encoding/decoding, context reasoning, and the construction and utilization of the requisite shared knowledge bases \cite{ref22, ref24}.

\subsection{Game Theory in Networking}

Game theory provides a rigorous mathematical framework for analyzing situations involving strategic interactions among multiple rational entities (players) whose decisions mutually influence their outcomes \cite{ref21}. Fundamental concepts include identifying the players, the strategies available to each, the payoffs associated with strategy combinations, and the notion of equilibrium (e.g., Nash Equilibrium), representing a stable state where no player can unilaterally improve their outcome by altering their strategy \cite{ref21}. Within communication networks, game theory offers valuable tools for modeling and analyzing scenarios characterized by competition or cooperation over shared resources. Common applications include optimizing resource allocation (e.g., bandwidth, power) among competing users or services, designing efficient medium access control protocols, analyzing routing strategies, and modeling security interactions, such as intrusion detection games or jamming mitigation strategies \cite{ref21}. The integration of AI, particularly harnessing the reasoning and generation capabilities of GenAI/LLMs, shows potential for automating aspects of game theory application, such as formulating game models from natural language descriptions of networking problems or aiding in the computation of equilibrium solutions, thereby potentially lowering the barrier to entry for non-experts \cite{ref21}.

\subsection{Network and Service Management (NSM)}

Effective Network and Service Management (NSM) is indispensable for the reliable, efficient, and performant operation of modern communication infrastructures across their diverse forms – from mobile networks (including 5G and emerging 6G systems) and the Internet of Things (IoT) to vehicular networks (V2X) and complex cloud, fog, and edge computing environments \cite{ref25}. The primary goals of NSM are to maintain the health and optimal functioning of the network infrastructure while ensuring that services delivered meet predefined performance targets and Key Performance Indicators (KPIs) \cite{ref25}. Core functional areas of NSM traditionally encompass tasks such as: network monitoring (collecting and analyzing operational data, status, and metrics), network planning (topology design, capacity planning, technology evolution), deployment and configuration (installing, provisioning, and setting up elements and services), and continuous support (including fault management, performance optimization, security management, and maintenance) \cite{ref25}. The ever-increasing complexity, scale, and dynamism of modern networks render traditional, often manual or rule-based, NSM approaches inadequate. This underscores the urgent need for more intelligent, automated NSM solutions, where advanced AI paradigms, including the capabilities offered by LLMs for processing unstructured data and automating complex tasks, present significant opportunities for revolutionizing NSM practices \cite{ref25}.

\section{Applications of AI/GAI/LLMs/Agentic AI in Telecommunication Networks}
\label{sec:applications}

Artificial intelligence (AI), encompassing traditional machine learning (ML), generative AI (GAI), large language models (LLMs), and the nascent field of agentic AI, is fundamentally reshaping telecommunication networks. These technologies are transitioning from theoretical concepts to practical implementations, providing transformative solutions for network optimization, automation, security enhancement, and the creation of novel services, thereby paving the way for future network architectures such as 6G \cite{ref1, ref2, ref9, ref26}. This section delves into the diverse applications of these AI paradigms across the telecommunications landscape, highlighting their contributions to addressing contemporary challenges and enabling next-generation connectivity.

\subsection{Network Optimization and Management} \label{subsec:optimization}

Optimizing network performance and efficiently managing increasingly complex infrastructures represent critical challenges in modern telecommunications. AI, in its various forms, offers potent tools to address these issues, progressing from well-established ML techniques to sophisticated GAI-driven strategies.

\subsubsection{Overview of ML Applications in Network Management and Optimization} \label{subsubsec:ml_overview}
Traditional ML methods have been foundational in improving network operations. Techniques rooted in supervised, unsupervised, and reinforcement learning have been successfully applied to network design, traffic engineering, dynamic resource allocation, and Quality of Service (QoS) management \cite{ref1,ref12,ref34}. These approaches facilitate data-driven decision-making, enabling networks to learn from operational data and adapt accordingly. Specific applications include optimizing routing paths, allocating bandwidth efficiently, and ensuring service level agreements are met \cite{ref12}. For example, ML algorithms analyze historical traffic patterns to predict potential congestion, allowing for proactive traffic rerouting or adjustment of resource provisioning \cite{ref12,ref34}.

\subsubsection{ML for Network Traffic Analysis and Classification} \label{subsubsec:ml_traffic}
Understanding and classifying network traffic is essential for both performance optimization and security. ML models demonstrate significant capabilities in analyzing complex traffic data to identify applications and characterize traffic flows, often uncovering patterns imperceptible to conventional methods \cite{ref11}. Predictive traffic modeling, utilizing algorithms like Random Forest and Gradient Boosting Decision Trees (GBDT), enables forecasting of future network load and behavior. This foresight allows for proactive resource management and slice configuration \cite{ref11}, which is particularly crucial for dynamic network slicing in 5G and beyond, ensuring resources are allocated effectively based on anticipated demand for diverse services \cite{ref11}.

\subsubsection{ML-Driven Enhancements for 5G Networks} \label{subsubsec:ml_5g}
The inherent complexity and varied service requirements of 5G networks necessitate intelligent management systems, positioning ML as a critical enabler \cite{ref1}. ML finds application across numerous 5G domains, including location-based services, mobile edge caching optimization, context-aware networking, and big data analytics for performance monitoring \cite{ref1}. Furthermore, ML underpins advanced network traffic control and optimization strategies \cite{ref1}. Network slicing, a key 5G innovation reliant on Software-Defined Networking (SDN) and Network Functions Virtualization (NFV), benefits extensively from AI/ML \cite{ref11}. AI/ML enables intelligent slice management, efficient resource allocation across slices, and dynamic scaling based on real-time traffic analysis and prediction, ensuring tailored QoS for demanding applications such as industrial automation, autonomous vehicles, and virtual reality \cite{ref11}.

\subsubsection{GAI-Enabled Game Theory for Mobile Networking Optimization} \label{subsubsec:gai_gametheory}
Game theory provides a robust mathematical framework for analyzing strategic interactions in network optimization problems, such as resource allocation and security provisioning. However, its traditional application often requires substantial expertise in model formulation and solution techniques \cite{ref21}. GAI, particularly through LLM frameworks enhanced with Retrieval-Augmented Generation (RAG), offers a novel pathway to democratize and improve the application of game theory \cite{ref21}. These advanced frameworks can interpret natural language descriptions of networking challenges, propose suitable game-theoretic models, and potentially generate solution algorithms, thus reducing reliance on specialized human experts \cite{ref21}. The integration of RAG ensures that the models access up-to-date, specialized knowledge, thereby enhancing the accuracy and relevance of the generated solutions. The practical utility of this LLM-enabled approach has been demonstrated in a case study involving secured Unmanned Aerial Vehicle (UAV) networks, showcasing its potential for optimizing complex mobile networking scenarios \cite{ref21}.

\subsubsection{GenAI/LLM-based Network Optimization and Automation}\label{subsubsec:genai_optimization}
GAI and LLMs signify a considerable advancement over traditional AI/ML, introducing new capabilities for network optimization and automation \cite{ref9,ref26}. These models can discern complex network dynamics from extensive datasets, capturing intricate relationships that might be missed by conventional methods \cite{ref9}. GAI facilitates offline exploration and the generation of diverse, realistic network scenarios, supporting proactive optimization and rigorous testing of management strategies prior to deployment \cite{ref9}. This is especially advantageous for improving resource allocation and enhancing overall network performance and efficiency within dynamic operational environments \cite{ref9}. Moreover, GAI and LLMs are poised to automate a broad spectrum of network operations \cite{ref17,ref18,ref25}, potentially encompassing complex troubleshooting and predictive maintenance \cite{ref2,ref18,ref25}. They are transforming Network and Service Management (NSM) across various domains (e.g., mobile, vehicular, cloud/edge) by leveraging their ability to process unstructured data (logs, reports, technical documents), automate configuration tasks, enhance network monitoring and reporting, support AI-driven network planning and deployment, and enable continuous network support via advanced anomaly detection and predictive maintenance \cite{ref25}.

\subsubsection{Diffusion Model (DM) Applications in Mobile Communications}
\label{subsubsec:dm_mobile}
Generative Diffusion Models (DMs), a specific category of GAI, are emerging as effective tools for addressing complex optimization problems within mobile communications \cite{ref19, ref33}. Their capacity to model intricate data distributions and generate high-quality samples via iterative denoising processes makes them well-suited for optimization tasks where the solution space is complex or ill-defined \cite{ref19, ref33}. Synergies between DMs/GDMs and Deep Reinforcement Learning (DRL) are being explored, where DMs can potentially enhance the exploration strategies or policy generation capabilities of DRL agents \cite{ref19}. Demonstrated applications include incentive mechanism design for network participants and optimization within Internet of Vehicles (IoV) networks \cite{ref19}.

\subsubsection{Case Study: Diffusion-based GAI for Optimization in Non-Terrestrial Networks (NTNs)} \label{subsubsec:dm_ntn}
The applicability of diffusion models extends to optimizing novel network paradigms. A specific case study illustrates the use of diffusion-based GAI to tackle complex optimization problems such as load balancing, carrier aggregation, and backhauling optimization within the challenging operational context of Non-Terrestrial Networks (NTNs) \cite{ref9}.

\subsection{Automation, Customer Service, Enhanced Understanding, and Advanced Reasoning}
\label{subsec:automation_reasoning}

Beyond optimizing network infrastructure, AI and GAI are augmenting operational efficiency, enhancing customer interactions, and improving the cognitive capabilities applied within the telecommunications sector.

\subsubsection{Automated Customer Assistance and Troubleshooting} \label{subsubsec:customer_service}
GAI-powered chatbots and virtual assistants are revolutionizing customer service in the telecom industry \cite{ref2}. These systems adeptly handle customer inquiries, provide troubleshooting assistance for technical problems, and offer personalized recommendations, thereby improving user satisfaction while reducing operational overhead \cite{ref2}. An illustrative example includes the development of RAG-based chatbots capable of addressing specific technical queries related to complex standards, such as the O-RAN specifications, demonstrating tangible value in specialized business-to-business (B2B) contexts \cite{ref2}.

\subsubsection{Enhancing Understanding of Complex Standards} \label{subsubsec:standards_understanding}
The telecommunications domain relies heavily on extensive and intricate technical standards, like those produced by 3GPP. GAI, particularly LLMs fine-tuned on domain-specific corpora, can function as potent Question Answering (QA) assistants. These tools enable engineers and developers to rapidly locate and comprehend relevant information within these dense documents \cite{ref18}. Custom models, such as TeleRoBERTa, have shown that even smaller, specialized LLMs can achieve performance comparable to large foundation models for such tasks, thereby facilitating applications in troubleshooting, maintenance, operations, and software development \cite{ref18}.

\subsubsection{Enabling Critical Thinking and Automation with GenAI/Agentic AI}\label{subsubsec:agentic_ai}
Future network management necessitates capabilities extending beyond simple classification or prediction, requiring critical thinking skills like reasoning and planning \cite{ref6}. While traditional symbolic AI possesses these abilities, it often encounters challenges related to knowledge acquisition bottlenecks and computational complexity \cite{ref6}. GenAI and the emerging paradigm of Agentic AI present potential solutions \cite{ref6,ref35}. Agentic AI envisions autonomous agents endowed with capabilities for perception, reasoning, decision-making, and action execution within the network environment \cite{ref35}. These agents could employ sophisticated generative information retrieval strategies—spanning from traditional keyword-based methods to semantic, knowledge-based, and advanced agentic contextual retrieval—to acquire and process information for complex tasks such as intent-based networking automation and advanced QA \cite{ref6,ref35}. An agentic contextual retrieval framework, which integrates multi-source information, structured reasoning, and self-reflection mechanisms, has been proposed to enhance planning tasks in telecommunications by improving accuracy and the consistency of explanations \cite{ref35}.

\subsubsection{Role in Software Development} \label{subsubsec:sw_development}
The proficiency of LLMs in understanding and generating code, combined with their capacity to process technical documentation, positions them as valuable assets throughout the software development lifecycle in the telecom sector. They can assist developers with coding, testing, and documentation tasks, streamlining development processes \cite{ref18}.

\subsection{Security Enhancements}
\label{subsec:security}

The growing complexity and openness of modern networks, especially with the adoption of paradigms like 5G/6G and Non-Orthogonal Multiple Access (NOMA), introduce novel security vulnerabilities. AI and GAI offer promising methodologies for strengthening network defenses.

\subsubsection{ML for Network Security Applications}\label{subsubsec:ml_security}
ML techniques have been extensively deployed to bolster network security. Applications include intrusion detection, anomaly detection, malware analysis, and spam filtering, all of which rely on learning patterns indicative of malicious activity from network data \cite{ref12}.

\subsubsection{GenAI for Enhancing Communication Security}\label{subsubsec:genai_security}
GAI models, such as Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs), introduce new capabilities for communication security, particularly at the physical layer \cite{ref5, ref20}. They can address challenges where traditional AI may falter, such as adapting to dynamic channel conditions and countering sophisticated, evolving threats \cite{ref5, ref20}. GAI can be employed to enhance communication confidentiality, strengthen authentication protocols, and improve network availability, resilience, and integrity \cite{ref5}. Despite their power, GAI models can face limitations related to computational complexity and adaptability \cite{ref20}. The Mixture of Experts (MoE) architecture presents a potential mitigation strategy, combining multiple specialized GAI models orchestrated by a gating mechanism to enhance overall efficiency and adaptability \cite{ref20}. An MoE-enabled GAI framework demonstrated effectiveness in optimizing security tasks, exemplified in a case study involving cooperative friendly jamming to improve physical layer security \cite{ref20}. Furthermore, GAI's ability to generate realistic synthetic data helps overcome the scarcity of labeled malicious data needed for training robust security models \cite{ref5, ref26}.

\subsubsection{Security Challenges and AI Applications in Next-Generation Multiple Access (NGMA) / NOMA} \label{subsubsec:noma_security}
Advanced multiple access schemes like NOMA, intended to support massive connectivity in 6G, present unique security challenges owing to spectrum sharing and the increased potential for interference and eavesdropping \cite{ref30}. The open nature of the air interface and the intertwined transmission of NOMA user signals exacerbate these threats \cite{ref30}. Conversely, the inherent characteristics of NOMA, such as controlled interference, can potentially be exploited to design enhanced physical layer security mechanisms \cite{ref30}. AI, including GAI, is anticipated to be pivotal in developing future security solutions tailored to the specific vulnerabilities and opportunities presented by NGMA/NOMA systems \cite{ref30}.

\subsubsection{Security in GAI-Enabled Game Theory Applications} \label{subsubsec:gai_game_security}
As GAI enhances game theory applications for network optimization (e.g., in UAV networks \cite{ref21}), ensuring the security and robustness of these GAI-driven decision-making processes becomes paramount. This involves protecting the integrity of the GAI models themselves and the knowledge bases (e.g., those utilized in RAG) upon which they depend.

\subsection{Generative AI in the Internet of Things (IoT) / AIoT} \label{subsec:gai_iot}

The synergy between GAI and the expansive ecosystem of IoT devices, often termed the Artificial Intelligence of Things (AIoT), promises to deliver enhanced intelligence, richer functionality, and improved user experiences.

\subsubsection{The Convergence of Generative AI and IoT} \label{subsubsec:gai_iot_convergence}
GAI is poised to make a significant impact on the IoT landscape, extending capabilities beyond traditional discriminative AI tasks (e.g., classification) to enable content generation and more sophisticated device interactions \cite{ref15, ref26}. This convergence is driving the evolution of AIoT, where devices not only collect and analyze data but also autonomously generate relevant information, predictions, or content \cite{ref26}.

\subsubsection{Potential Applications of Generative AI in IoT Domains} \label{subsubsec:gai_iot_apps}
GAI applications within IoT are diverse and multifaceted \cite{ref15}. It can augment the capabilities of smart devices such as smartphones, wearables, and robots, facilitating more natural language interaction, personalized responses, and adaptive behaviors \cite{ref15}. GAI is particularly valuable for generating high-quality synthetic data to train and test IoT systems, especially in scenarios where real-world data is limited, sensitive, or challenging to acquire \cite{ref15}. Additionally, GAI can automate the creation of content for IoT user interfaces, such as generating summaries, explanations, or visualizations derived from device data \cite{ref15}.

\subsubsection{Multimodal Generative Models in IoT Contexts} \label{subsubsec:multimodal_iot}
IoT environments are inherently multimodal, involving data streams from various sensors (e.g., cameras, microphones, environmental sensors). Multimodal GAI models, capable of processing and generating content across different data types (e.g., text-to-image, sensor-data-to-text), are essential for realizing the full potential of GAI in complex IoT applications \cite{ref15}.

\subsection{Sustainability: Towards Low-Carbon AIoT}
\label{subsec:sustainability}

While AIoT offers substantial benefits, its widespread deployment raises concerns regarding energy consumption and the associated carbon footprint. GAI is being investigated as a potential tool to mitigate these environmental impacts.

\subsubsection{Energy Consumption and Carbon Emission Challenges in AIoT} \label{subsubsec:energy_challenges}
The exponential growth in connected devices, data traffic, and AI processing within AIoT systems contributes significantly to energy consumption and carbon emissions, presenting substantial sustainability challenges \cite{ref29}.

\subsubsection{GAI's Potential for Carbon Emission Reduction} \label{subsubsec:gai_carbon_reduction}
The advanced reasoning and generation capabilities of GAI can be harnessed to optimize energy usage and reduce the carbon footprint of AIoT systems \cite{ref29}. GAI can analyze complex operational data, model energy consumption patterns accurately, and identify effective strategies for efficiency improvements across network components and device operations \cite{ref29}.

\subsubsection{Proposed GAI-Enabled Solutions for Low-Carbon AIoT} \label{subsubsec:gai_lowcarbon_solutions}
Frameworks incorporating GAI are being developed to specifically address AIoT carbon emissions \cite{ref29}. LLMs, potentially augmented with RAG to access domain-specific knowledge (e.g., hardware energy profiles, renewable energy availability), can be utilized to formulate complex carbon emission optimization problems \cite{ref29}. Subsequently, Generative Diffusion Models (GDMs) can be employed to explore the vast solution space and identify optimal operational strategies that minimize the carbon footprint while maintaining required performance levels \cite{ref29}. Preliminary numerical results indicate that these GAI-enabled frameworks hold significant promise for facilitating greener AIoT deployments \cite{ref29}.

\subsection{Semantic Communications (SemCom) and GAI/LLM Integration}
\label{subsec:semcom}

Semantic communications (SemCom) signifies a paradigm shift in communication theory, moving focus from the accurate transmission of bits to the effective conveyance of meaning. GAI and LLMs are increasingly viewed as key enablers for realizing the potential of SemCom.

\subsubsection{Enabling Advanced Semantic Communications with GAI/LLMs}
\label{subsubsec:gai_semcom_enable}
GAI and LLMs can substantially enhance SemCom systems by providing the necessary intelligence for semantic encoding, decoding, and reasoning \cite{ref17,ref26}. These models possess the ability to learn the underlying semantics of information, allowing communication systems to transmit the intended meaning more efficiently compared to traditional methods focused solely on bit-level fidelity \cite{ref22,ref24}.

\subsubsection{GDM Use Cases in Semantic Communications} \label{subsubsec:gdm_semcom}
Generative Diffusion Models (GDMs) also find potential applications within the SemCom domain, possibly contributing to tasks such as the generation of semantic representations or assisting in the semantic encoding and decoding processes \cite{ref19}.

\subsubsection{Motivation Overcoming SemCom Limitations with GAI}\label{subsubsec:semcom_motivation}
Current SemCom approaches encounter significant challenges, notably in context-reasoning (understanding nuances across complex contextual fragments) and background knowledge provisioning (acquiring and managing the extensive knowledge required for accurate semantic interpretation) \cite{ref23}. GAI, with its inherent strengths in context understanding and knowledge synthesis, is particularly well-suited to address these limitations \cite{ref23}.

\subsubsection{Synergy and Mutual Benefits}\label{subsubsec:semcom_synergy}
The relationship between GAI and SemCom is characterized by strong synergy and mutual reinforcement \cite{ref22, ref23, ref24}. GAI enhances SemCom systems by aiding model pre-training and fine-tuning, facilitating the construction and augmentation of background knowledge bases, optimizing resource allocation for semantic transmission, and improving semantic context-reasoning capabilities. Conversely, SemCom can support demanding AIGC services by ensuring low-latency, high-reliability transmission of semantically dense content, employing semantic-aware encoding and data compression to conserve resources, and enabling knowledge- and context-based reasoning at the receiver end. This symbiotic relationship is summarized in Table~\ref{tab:gai_semcom_synergy}. Frameworks such as GAI-integrated SemCom networks (GAI-SCN) are being proposed, utilizing global and local GAI models within a cloud-edge-mobile architecture to enable efficient multimodal semantic processing \cite{ref23}.

\begin{table*}[htbp]
\centering
\caption{Synergistic Benefits of GAI and Semantic Communications Integration}
\label{tab:gai_semcom_synergy}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}lp{0.4\textwidth}p{0.4\textwidth}@{}}
\toprule
\textbf{Direction} & \textbf{Contribution} & \textbf{Examples/Mechanisms} \\
\midrule
\textbf{GAI $\rightarrow$ SemCom} & Enhances SemCom Intelligence \& Efficiency &
\parbox[t]{\linewidth}{
\begin{itemize} \itemsep0em
    \item Model pre-training/fine-tuning \cite{ref23}
    \item Background knowledge base construction/augmentation \cite{ref23}
    \item Resource allocation optimization for semantics \cite{ref22, ref23}
    \item Improved semantic context-reasoning \cite{ref23, ref24}
\end{itemize}
} \\
\addlinespace
\textbf{SemCom $\rightarrow$ GAI} & Supports Demanding AIGC Services &
\parbox[t]{\linewidth}{
\begin{itemize} \itemsep0em
    \item Low-latency, high-reliability semantic transmission \cite{ref22}
    \item Semantic-aware encoding/compression (resource saving) \cite{ref24}
    \item Enabling knowledge/context-based reasoning at receiver \cite{ref22, ref24}
\end{itemize}
} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\subsubsection{Potential SemCom Use Cases} \label{subsubsec:semcom_usecases}
The integration of GAI and SemCom is anticipated to enable sophisticated applications that depend on the efficient transmission of meaning. Examples include autonomous driving (exchanging complex situational awareness), smart cities (coordinating intricate urban systems), and the Metaverse (delivering rich, immersive experiences) \cite{ref22, ref24}. Other potential applications encompass distributed image synthesis, text generation, and even accelerating scientific discovery processes like drug discovery through more efficient knowledge dissemination \cite{ref23}.

\subsection{Role in Future Network Architectures (6G)}
\label{subsec:6g}

AI, and particularly GAI and LLMs, are widely regarded as foundational technologies for the development and operation of future 6G networks.

\subsubsection{Foundational Technology for AI-Native Networks}\label{subsubsec:6g_ainative}
6G is envisioned as an AI-native network architecture, where AI is not merely an auxiliary component but is deeply integrated into the network fabric from its inception \cite{ref7, ref9}. GAI and associated AI technologies are expected to provide the core intelligence required to manage the unprecedented complexity, scale, and dynamism characteristic of 6G systems \cite{ref7, ref9}.

\subsubsection{Supporting Diverse Service Demands and Quality of Service}\label{subsubsec:6g_qos}
GAI will be indispensable for dynamically managing network resources and configurations to meet the extremely diverse service demands and stringent QoS requirements anticipated in the 6G era. These range from holographic communication and immersive extended reality (XR) to massive-scale IoT deployments \cite{ref9}.

\subsubsection{The Essential Role of AI/ML in 6G Wireless Networks} \label{subsubsec:6g_essential}
AI/ML is considered crucial for nearly every facet of 6G wireless networks, enabling functionalities and performance levels that are unattainable with traditional network management approaches \cite{ref13}.

\subsubsection{Innovations in Communication Paradigms} \label{subsubsec:6g_paradigms}
Meeting 6G's demanding requirements—such as ultra-high spectral efficiency, ultra-low latency, and massive connectivity—necessitates innovations in fundamental communication paradigms, including multiple access schemes. NGMA/NOMA signifies a shift towards non-orthogonal resource sharing strategies, enabled and managed by AI, to push the boundaries of network capacity and efficiency \cite{ref30}.

\subsubsection{AI-Integrated Radio Access Networks (AI-RAN) for 6G}\label{subsubsec:6g_airan}
The concept of AI-RAN proposes the deep integration of AI within the Radio Access Network itself, moving beyond centralized AI control towards distributed intelligence embedded within RAN components \cite{ref34}. AI-RAN is expected to be a cornerstone of 6G, enabling real-time adaptation, optimization, and automation at the network edge to satisfy stringent performance demands \cite{ref34}.

\subsubsection{Proposed Frameworks The WirelessLLM Concept}
\label{subsubsec:6g_wirelessllm}
Research efforts are exploring specific frameworks for integrating large-scale AI models effectively into wireless network operations. The WirelessLLM concept, for instance, proposes adapting and enhancing LLMs specifically for the wireless domain \cite{ref17}. This involves incorporating principles like knowledge alignment, fusion, and evolution, and utilizing techniques such as prompt engineering, RAG, and domain-specific fine-tuning to address unique wireless challenges and enable advanced network automation and optimization \cite{ref17}.

\subsection{Integration with Emerging Technologies} \label{subsec:emerging_tech}

The applicability of GAI extends to synergistic integration with other emerging communication technologies, further enhancing network capabilities.

\subsubsection{GenAI in Non-Terrestrial Networks (NTNs)}\label{subsubsec:genai_ntn}
As previously highlighted in the optimization case study \cite{ref9}, GenAI is being actively applied to address the distinct challenges posed by NTNs (which include satellite, High-Altitude Platform Station (HAPS), and UAV networks). These challenges encompass dynamic network topologies, highly variable channel conditions, and complex resource management across integrated terrestrial and non-terrestrial segments.

\subsubsection{Consideration of Technologies like THz-RIS in NTNs} \label{subsubsec:thz_ris}
The integration of GAI into future networks, including NTNs, will also necessitate consideration of, and potentially interaction with, other advanced technologies. Examples include Terahertz (THz) communications and Reconfigurable Intelligent Surfaces (RIS), which are being explored to further augment capacity and coverage, particularly within NTN deployment scenarios \cite{ref10}. AI/GAI will likely assume a significant role in optimizing the joint operation of these diverse technologies to maximize overall network performance.

\section{Architectural Considerations, Infrastructure Optimization, and Data Management}

The integration of sophisticated Artificial Intelligence (AI), particularly generative AI (GenAI) and Large Language Models (LLMs), into communication networks and associated services introduces significant architectural, infrastructural, and data management challenges. Centralized cloud deployments often struggle with inherent latency, substantial computational demands, and high operational costs, necessitating innovative solutions that leverage distributed resources, optimize data flow, and embed AI principles directly into network design \cite{ref14, ref31}. This section examines architectural strategies for optimizing AI inference, managing data efficiently at the network edge, designing AI-native telecommunication systems, embedding responsibility within these architectures, and navigating the persistent challenges associated with edge deployment.

\subsection{AI Inference Optimization via Telco Infrastructure, Edge Computing, and Cloud-Edge Collaboration}

A primary obstacle to deploying real-time AI applications, especially those involving computationally intensive models like LLMs and GenAI, is the inference latency associated with exclusive reliance on centralized cloud infrastructure \cite{ref3, ref14, ref31}. To mitigate this bottleneck, research efforts are exploring architectures that distribute AI workloads closer to end-users by leveraging existing telecommunications infrastructure and edge computing resources.

One prominent strategy involves repurposing telecommunications operator (Telco) assets—such as regional data centers, Content Delivery Network (CDN) nodes, and near-Radio Access Network (RAN) edge sites—to create a hierarchical network of AI edges \cite{ref3}. This approach mirrors the success of CDNs in content distribution, envisioning a specialized delivery network tailored for AI inference embedded within the Telco fabric \cite{ref3}. Such a hierarchical structure enables tiered caching strategies, utilizing semantic and vector-based caches at various network levels to store frequently accessed AI results or intermediate computations. This caching mechanism significantly reduces the necessity for repeated, computationally expensive inferences \cite{ref3}. Latency can be further minimized through split-inference architectures, where segments of the AI model computation occur at the edge, while more demanding tasks are offloaded to the cloud or higher tiers within the edge hierarchy \cite{ref3}.

Leveraging edge computing, either independently or integrated with Telco assets, is crucial for delivering low-latency AI services \cite{ref14, ref25, ref31}. By positioning computational resources geographically nearer to users, edge computing directly addresses the network transmission delays inherent in purely cloud-centric models \cite{ref14, ref31}. However, the resource limitations frequently encountered at the extreme edge necessitate collaborative cloud-edge models as a pragmatic solution for demanding AI/GenAI workloads \cite{ref14, ref23, ref25, ref31}. These hybrid architectures seek to balance the low latency and data localization advantages of the edge with the extensive computational power available in the cloud \cite{ref31}. Compared to traditional cloud computing's reliance on centralized processing and Multi-access Edge Computing (MEC)'s sole focus on edge servers, cloud-edge collaboration presents a more flexible and potentially higher-performance paradigm \cite{ref31}. Nevertheless, designing distributed GenAI systems requires careful consideration of workload partitioning strategies, data synchronization mechanisms, and communication overhead minimization \cite{ref31}. Despite its promise, the deployment of GenAI services continues to face substantial infrastructure challenges, including managing immense computational demands, mitigating residual cloud dependency for certain tasks, and consistently achieving low latency across diverse network conditions \cite{ref31}.

\subsection{Vector Databases and LLM Optimization at the Edge}

For LLM-based services, the Quality of Service (QoS) challenges stemming from cloud deployment, primarily excessive response delays and high operational costs, are particularly pronounced \cite{ref14}. An effective, non-invasive method for optimizing LLM performance at the edge involves employing vector databases for caching \cite{ref14}. In contrast to techniques requiring modifications to the LLM architecture itself (e.g., quantization, pruning, knowledge distillation), vector database caching stores vector representations of historical request-response pairs (such as Questions and Answers) at the edge \cite{ref14}. When a similar request arrives, the system can potentially retrieve a cached response directly from the edge vector database. This approach significantly reduces response time and obviates the need for a computationally intensive LLM inference in the cloud \cite{ref14}, thereby minimizing delay and conserving resources without altering the underlying LLM.

To address the QoS optimization problem within such a cloud-edge context, the VELO (Vector database-assisted cloud-Edge collaborative LLM QoS Optimization) framework has been proposed \cite{ref14}. VELO explicitly utilizes an edge-based vector database to cache LLM results. The critical decision of whether to serve a user request from the edge cache or forward it to the cloud-hosted LLM is formulated as a Markov Decision Process (MDP). To solve this MDP and optimize request routing for improved QoS, VELO employs a Multi-Agent Reinforcement Learning (MARL) algorithm \cite{ref14}. This enables the system to dynamically learn optimal routing policies based on prevailing network conditions, cache hit rates, and user requirements. Further enhancements, including improvements to the MARL policy network and the integration of expert demonstrations, aim to refine feature extraction and accelerate the training convergence \cite{ref14}. Experimental validation within a real-world edge system confirmed that the VELO framework significantly enhances user satisfaction by reducing both delay and resource consumption for edge users interacting with LLMs \cite{ref14}.

\subsection{Architectural Design for AI-Powered Telecom Systems}

Beyond optimizing inference, AI is being integrated more fundamentally into the architectural blueprint of next-generation telecommunications systems. The concept of AI-integrated Radio Access Networks (AI-RAN) exemplifies this shift, aiming to embed AI capabilities directly within the RAN infrastructure to satisfy the stringent demands anticipated for 6G networks \cite{ref34}. Designing robust AI-RAN architectures involves addressing specific challenges related to the integration of network components, efficient resource management, and the development of novel spectrum allocation strategies driven by AI operations \cite{ref34}.

The emergence of Foundation Models (FMs), including LLMs, is further reshaping AI system architectures \cite{ref16}. An observable architectural evolution is underway, transitioning from systems where FMs function primarily as connectors between distinct components (FM-as-Connector) towards more integrated designs where the FM potentially subsumes multiple functionalities (FM-as-Monolithic) \cite{ref16}. This evolution introduces considerable design complexity, particularly concerning the dynamic nature of component boundaries and the evolution of interfaces as FM capabilities expand \cite{ref16}. Key design decisions must focus on optimally leveraging FM capabilities while effectively managing the complexity arising from these shifting architectural paradigms. To provide systematic guidance, pattern-oriented reference architectures are being developed to facilitate the design of robust and responsible FM-based systems \cite{ref16}.

Concurrently, the convergence of Generative AI (GAI) and Semantic Communication (SemCom) is introducing new architectural considerations \cite{ref22, ref23, ref24}. SemCom, which prioritizes transmitting the meaning or semantic content of information rather than ensuring perfect bit-level reproduction, holds promise for substantial efficiency gains, particularly for bandwidth-intensive AI-Generated Content (AIGC) services \cite{ref22, ref24}. GAI algorithms are fundamental to enabling SemCom by facilitating model pre-training, knowledge base construction, and semantic inference processes \cite{ref22, ref23, ref24}. Conversely, SemCom can provide low-latency, high-reliability delivery mechanisms for AIGC services through semantic-aware encoding and context-based reasoning \cite{ref22, ref23, ref24}. A proposed generic architecture for GAI-driven SemCom networks typically includes a data plane, physical infrastructure, and a network control plane \cite{ref22, ref24}. One specific instantiation, the GAI-integrated SemCom Network (GAI-SCN) framework, employs a cloud-edge-mobile architecture utilizing both global GAI models (hosted in the cloud/edge) and local GAI models (potentially deployed on end-user devices) \cite{ref23}. This architecture supports multimodal semantic content provisioning and employs semantic-level joint source-channel coding to optimize transmission efficiency and reliability \cite{ref23}. Key techniques encompass end-to-end system design, including transceiver optimization and semantic effectiveness calculation \cite{ref22, ref24}, alongside sophisticated generation-level strategies and knowledge management practices \cite{ref22, ref24}. Effective knowledge management—encompassing construction, updating, and sharing mechanisms—is crucial for ensuring accurate and timely knowledge-based reasoning within these advanced communication systems \cite{ref24}.

Furthermore, agentic AI paradigms are emerging as a powerful approach for intelligent communications and networking \cite{ref35}. Agentic AI involves autonomous agents capable of perceiving, reasoning, deciding, and acting within the network environment. To enhance decision-making capabilities for complex telecom tasks, such as network planning and resource allocation, agentic frameworks increasingly incorporate advanced generative information retrieval techniques. For example, an agentic contextual retrieval framework integrates multi-source retrieval, structured reasoning, and self-reflective validation mechanisms to improve planning accuracy and compliance, particularly when referencing complex technical standards like those developed by 3GPP \cite{ref35}.

\subsection{Building Responsible AI into the Architecture}

The integration of powerful AI models like FMs into system architectures raises significant concerns regarding responsible AI implementation \cite{ref16}. Establishing accountability becomes particularly complex due to the multiple stakeholders involved, including system owners, FM providers, and providers of external tools or plugins utilized by the FM \cite{ref16}. Consequently, architectural design must explicitly incorporate mechanisms for robust traceability. This involves enabling the comprehensive recording and auditing of inputs and outputs associated with FMs, constituent system components, and integrated external tools \cite{ref16}. Addressing the inherent opacity of many large-scale AI models is also paramount for building trustworthiness. Architectural patterns and specific safeguards are essential to mitigate risks associated with potentially unpredictable or unexplainable model behavior \cite{ref16}.

\subsection{Challenges in Deploying AI at the Network Edge and in Edge-Cloud Systems}

Despite significant architectural advancements, effectively deploying AI—especially GenAI—within edge and collaborative edge-cloud environments remains fraught with challenges. Overarching concerns include the general resource constraints prevalent at the edge (e.g., limited compute power, memory, and energy budgets) and the persistent imperative to minimize latency \cite{ref14,ref31}. Specific implementation hurdles encompass the development of efficient mechanisms for cache synchronization across distributed edge nodes, managing the distribution and updates of potentially large AI models to numerous edge locations, ensuring robust privacy preservation when processing sensitive data at the edge, and effectively leveraging hardware acceleration capabilities within resource-constrained edge devices \cite{ref3}. Moreover, ensuring the scalability and overall operational efficiency of distributed edge-cloud GenAI systems as user demand grows presents a significant ongoing technical and logistical challenge \cite{ref31}. Addressing these multifaceted obstacles is critical to fully realizing the potential of edge AI and collaborative edge-cloud architectures in future communication networks.

\section{Challenges and Implementation Considerations}
\label{sec:challenges}

The transformative potential of Generative AI (GenAI) within the telecommunications sector is substantial; however, its realization is contingent upon navigating a complex landscape of challenges. These hurdles encompass theoretical limitations, practical deployment constraints, intrinsic model weaknesses, and critical ethical considerations. Successfully addressing these issues is paramount for leveraging AI-driven network automation, optimization, and service innovation effectively. This section delineates the principal challenges and implementation factors associated with integrating GenAI, particularly Large Language Models (LLMs) and related advanced AI paradigms, into the telecommunications ecosystem.

\subsection{Bridging Theory and Practical Deployment}
\label{sec:theory_practice_gap}

A notable gap persists between the demonstrated theoretical capabilities of GenAI and its effective application within the demanding operational realities of telecommunications networks \cite{ref2}. While research highlights GenAI's potential across diverse use cases \cite{ref2, ref9}, translating these concepts into robust, field-deployable solutions encounters significant practical obstacles \cite{ref2, ref26}. Much of the existing literature emphasizes the visionary potential of GenAI for telecom, occasionally understating the intricate details and barriers inherent in real-world deployment \cite{ref2}. The broader application of Machine Learning (ML) in networking already grapples with challenges concerning data quality, model complexity, and the necessity for continuous adaptation within dynamic network environments \cite{ref12}. GenAI introduces additional layers of complexity, especially regarding the stringent reliability and trustworthiness required for managing critical network functions \cite{ref6}. A key unresolved challenge lies in maturing GenAI algorithms to exhibit dependable critical thinking, reasoning, and planning capabilities adequate for complex, high-stakes decision-making processes in mobile networks \cite{ref6}. Therefore, overcoming these practical deployment hurdles and ensuring GenAI models are sufficiently robust and validated for telecom operations represent critical prerequisites for widespread adoption \cite{ref26}.

\subsection{Intrinsic Limitations and Reliability of AI Models}
\label{sec:intrinsic_limitations}

Beyond the gap between theory and practice, contemporary GenAI and LLM technologies exhibit inherent limitations that impede their straightforward application in mission-critical systems. A primary concern involves the phenomenon of "hallucination," wherein models generate outputs that appear plausible but are factually incorrect or nonsensical \cite{ref8, ref17}. This issue partly arises from training on vast datasets that may contain biases or outdated information, resulting in factual inaccuracies and an inability to access real-time knowledge \cite{ref8, ref17}. Consequently, LLMs may lack the deep, domain-specific understanding required for telecommunications—for instance, concerning the complex physics governing wireless propagation or intricate protocol specifications—potentially leading them to propose solutions that violate fundamental operational constraints \cite{ref17}. Memory limitations, including "catastrophic forgetting" where models lose previously learned information upon acquiring new knowledge, further undermine their reliability for tasks demanding consistent performance over extended periods \cite{ref8}. Moreover, the training data itself can embed societal or imitative biases, introducing concerns about fairness and equity in AI-driven network management and service delivery \cite{ref8}. The capacity of current GenAI for genuine critical thinking also remains circumspect, limiting confidence in their ability to reliably handle complex reasoning and planning tasks \cite{ref6}. Ensuring the accuracy and robustness of solutions generated or assisted by LLMs—such as formulating and solving game-theoretic models for network optimization \cite{ref21} or generating network configurations—necessitates rigorous validation processes and potentially novel methodologies for enhancing model reliability and grounding outputs in factual knowledge \cite{ref17}. While techniques like Retrieval-Augmented Generation (RAG), knowledge graphs (KGs), or improved training strategies aim to mitigate these intrinsic limitations, they introduce their own set of complexities, as discussed further in Section~\ref{sec:technical_hurdles}.

\subsection{Cost, Latency, Computational Efficiency, and Scalability}
\label{sec:cost_latency_efficiency}

The deployment of sophisticated AI models, particularly large foundational models, imposes significant practical constraints related to cost, latency, and computational resource requirements. Inference latency represents a critical bottleneck, especially for real-time network control or interactive user applications, often rendering purely cloud-based deployments unsuitable for numerous telecommunications use cases \cite{ref3, ref31}. High response delays and the associated operational costs can negatively impact the Quality of Service (QoS) experienced by end-users interacting with cloud-hosted LLMs \cite{ref14}. The substantial computational demands for both model training and, critically, inference operations translate into high operational expenditures, particularly for commercial applications or deployments involving customized models \cite{ref3, ref8, ref31}. This situation compels an exploration of trade-offs between centralized cloud inference and distributed edge inference \cite{ref3, ref31}. Leveraging existing telecommunications infrastructure for edge caching (e.g., utilizing vector databases) and performing distributed inference can help mitigate latency and reduce costs \cite{ref3, ref14, ref31}. However, this approach introduces new challenges related to cache synchronization, efficient model distribution, and managing heterogeneous edge hardware resources \cite{ref3}. Foundational model inference itself presents unique scaling difficulties \cite{ref3}. Ensuring the scalability and computational efficiency of GenAI, LLM, GAI, and agentic AI solutions is crucial as network complexity continues to increase \cite{ref6, ref9, ref20, ref25, ref31, ref35}. Scalability must be addressed across diverse scenarios, including complex network optimization problems (e.g., those employing game theory) \cite{ref21, ref25}, managing large-scale Network and Service Management (NSM) tasks \cite{ref25}, supporting hybrid edge-cloud GenAI architectures \cite{ref31}, and deploying GAI for security applications where resource efficiency is paramount \cite{ref5}. Resource constraints are particularly acute for LLMs applied to NSM \cite{ref25} and for GAI-driven Semantic Communications (SemCom), where balancing semantic fidelity with resource consumption is essential \cite{ref22, ref23, ref24}. Furthermore, the significant energy consumption and associated carbon emissions resulting from training and operating large AI models present considerable sustainability challenges, especially within the context of AIoT and large-scale AI-driven network deployments \cite{ref29, ref31, ref34, ref35}. Optimizing GAI models for resource efficiency and reduced environmental impact is therefore a critical design consideration \cite{ref5, ref25, ref29}.

\subsection{Technical Hurdles in Deployment, Integration, and Adaptation}
\label{sec:technical_hurdles}

Successfully deploying, integrating, and adapting GenAI technologies within the complex telecommunications ecosystem necessitates overcoming numerous technical hurdles. Implementing edge AI and managing edge-cloud collaborative systems introduces significant challenges in resource management, latency optimization, and consistent model deployment across geographically distributed infrastructure \cite{ref3, ref14, ref31} (further discussed in Section 4.5). A central challenge lies in achieving sufficient domain specificity: general-purpose LLMs often lack the specialized knowledge required for nuanced telecom applications, demanding deep expertise in cellular protocols, evolving standards (e.g., 3GPP releases, O-RAN specifications), and the underlying physics of network operations \cite{ref4, ref17, ref18, ref25}. Adapting these models effectively to the telecom domain is non-trivial. While fine-tuning is one potential approach, it can be computationally expensive, may suffer from catastrophic forgetting, struggles to keep pace with the rapid evolution of standards, and carries the risk of overfitting to specific training data \cite{ref4, ref8}. Alternative methods, such as RAG, KGs, vector databases (VecDBs), and agentic retrieval frameworks, offer mechanisms to inject domain-specific, up-to-date knowledge without retraining the entire foundational model \cite{ref4, ref8, ref35}. However, the effective integration of these external knowledge sources presents its own architectural, data management, and retrieval accuracy challenges \cite{ref4, ref8}. Handling the diverse and often multi-modal data prevalent in wireless systems—including sensor readings, signal measurements, network logs, and textual reports—requires sophisticated data ingestion and processing capabilities \cite{ref17, ref22, ref23}. This is particularly relevant for tasks like NSM, which deals with large volumes of unstructured network data \cite{ref25}, or GAI-SemCom, which processes various content types \cite{ref22, ref23}. Defining the specific network infrastructure requirements needed to adequately support emerging GAI applications is another essential prerequisite \cite{ref9}. Specific challenges also arise within the Internet of Things (IoT) domain, demanding tailored GenAI solutions optimized for resource-constrained devices and diverse communication patterns \cite{ref15}. Data acquisition for building comprehensive background knowledge bases and for training specialized models, particularly for niche areas like SemCom, remains a significant bottleneck \cite{ref23}. Achieving sufficient context reasoning capabilities is vital for advanced applications like SemCom and agentic AI, enabling models to understand complex situations and make nuanced, context-aware decisions \cite{ref23, ref35}. Ensuring mechanisms for continuous adaptation and knowledge evolution is critical within the highly dynamic telecom environment, allowing AI models to remain current with new technologies, evolving standards, and changing network states \cite{ref17, ref25}. Key research challenges also persist in areas such as AI-RAN, specifically concerning optimal spectrum allocation strategies, efficient architectural designs, and intelligent resource management within AI-native radio access networks \cite{ref34}.

\subsection{Benchmarking and Evaluation}
\label{sec:benchmarking}

The application of GenAI in telecommunications is still nascent, suffering from a significant lack of standardized benchmarks and robust performance evaluation methodologies \cite{ref18}. Establishing common metrics, representative datasets, and standardized testing procedures is crucial for objectively assessing the capabilities and limitations of different GenAI models and implementation approaches (e.g., comparing fine-tuned models versus RAG-based systems) within the specific context of relevant telecom use cases \cite{ref18}. Without such robust benchmarking frameworks, rigorously comparing proposed solutions and systematically tracking progress in the field remains exceedingly difficult.

\subsection{Responsible AI, Security, and Reliability}
\label{sec:responsible_ai_security}

Integrating powerful AI systems like GenAI into critical telecommunications infrastructure demands an unwavering focus on responsible AI principles, comprehensive security measures, and overall system reliability. Given the sensitivity of network operational data and control functions, security and privacy concerns are paramount \cite{ref12, ref25, ref31, ref34, ref35}. Addressing responsible AI challenges—including accountability, traceability, trustworthiness, and explainability—is essential for building confidence in AI-driven decision-making processes, particularly as these systems gain greater autonomy \cite{ref16}. The inherently opaque nature ("black box" problem) of many large AI models can make tracing decision pathways and assigning accountability for errors or failures challenging \cite{ref16}. Furthermore, GenAI itself can introduce novel security vulnerabilities; these models could potentially be exploited to generate malicious inputs, craft sophisticated phishing attacks targeting users or network personnel, or manipulate network behavior in unintended ways [Implicit extension of \cite{ref20}]. Specific security challenges also emerge in advanced communication schemes like Non-Orthogonal Multiple Access (NOMA), necessitating the development of tailored security protocols and countermeasures \cite{ref30}. Ensuring the fundamental reliability of AI-driven network functions is absolutely critical for maintaining service continuity, network stability, and a positive user experience \cite{ref31, ref34, ref35}. This encompasses not only the robustness and predictability of the AI models themselves but also the resilience of the surrounding infrastructure, control loops, and fail-safe mechanisms.

\subsection{Strategic and Ecosystem Considerations}
\label{sec:strategic_ecosystem}

Beyond the purely technical hurdles, various strategic and ecosystem-level factors significantly influence the successful deployment and adoption of GenAI in the telecommunications industry. Establishing viable and mutually beneficial partnership models between telecommunications operators and AI technology providers (including hyperscalers, startups, and research institutions) is crucial for effectively leveraging complementary expertise, data resources, and infrastructure capabilities \cite{ref3}. Addressing the pressing need for standardization and interoperability is also vital. This ensures the seamless integration of AI components sourced from different vendors and facilitates the development of a cohesive, interoperable AI-driven network ecosystem \cite{ref34}. Concerted efforts towards practical implementation guidelines and standardization, particularly for emerging concepts like AI-RAN, are necessary to transition promising research prototypes into widespread, commercially viable deployments \cite{ref34}.

\subsection{Concluding Remarks on GAI Applicability}
\label{sec:challenges_conclusion}

In summary, while GenAI presents unprecedented opportunities for innovation within mobile and wireless networking, its practical realization is contingent upon addressing a multitude of interconnected challenges, as outlined throughout this section and summarized in Table~\ref{tab:genai_challenges_summary}. Overcoming the limitations related to model reliability, operational cost, system scalability, domain-specific adaptation, security vulnerabilities, and responsible deployment is essential. Addressing these multifaceted issues comprehensively is necessary to unlock the full potential of GAI technologies and facilitate their widespread, effective applicability across the telecommunications sector \cite{ref26}.

\begin{table*}[htbp]
\centering
\caption{Summary of Key Challenges in Implementing Generative AI in Telecommunications}
\label{tab:genai_challenges_summary}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|p{0.2\linewidth}|p{0.4\linewidth}|p{0.3\linewidth}|}
\hline
\textbf{Category} & \textbf{Specific Challenges} & \textbf{Key Implications / Examples} \\
\hline
\hline
Theoretical \& Practical Gap & Difficulty translating research potential to robust operational systems \cite{ref2, ref26} & Ensuring reliability for critical functions; Overcoming deployment intricacies \cite{ref6, ref12} \\
\hline
Intrinsic Model Limitations & Hallucinations, factual incorrectness, lack of real-time knowledge \cite{ref8, ref17}; Memory issues (catastrophic forgetting) \cite{ref8}; Embedded biases \cite{ref8}; Limited critical reasoning \cite{ref6} & Reduced trustworthiness; Potential violation of domain constraints \cite{ref17}; Need for rigorous validation; Fairness concerns \\
\hline
Resource Constraints & High computational cost (training \& inference) \cite{ref3, ref8, ref31}; Inference latency \cite{ref3, ref14, ref31}; Scalability issues \cite{ref6, ref9, ref20, ref25, ref31, ref35}; Energy consumption \cite{ref29, ref31, ref34, ref35} & Barriers to real-time applications; High operational costs; QoS impact; Need for edge/distributed solutions \cite{ref3, ref14}; Sustainability concerns \\
\hline
Technical Integration \& Adaptation & Domain specificity mismatch \cite{ref4, ref17, ref18, ref25}; Challenges in fine-tuning vs. RAG/KG integration \cite{ref4, ref8, ref35}; Multi-modal data handling \cite{ref17, ref22, ref23, ref25}; Continuous adaptation requirement \cite{ref17, ref25}; Edge-cloud system complexity \cite{ref3, ref14, ref31} & Need for specialized telecom knowledge; Difficulty keeping models current; Data processing bottlenecks; Infrastructure management \\
\hline
Evaluation \& Standardization & Lack of standardized benchmarks and evaluation metrics \cite{ref18} & Difficulty comparing solutions objectively; Hindrance to progress tracking \\
\hline
Responsible AI \& Security & Security vulnerabilities (new attack vectors) [Implicit extension of \cite{ref20}]; Data privacy concerns \cite{ref12, ref25, ref31, ref34, ref35}; Accountability, trustworthiness, explainability deficits \cite{ref16} & Risk to critical infrastructure; Need for robust security measures; Building user/operator trust \\
\hline
Strategic \& Ecosystem & Need for viable partnerships \cite{ref3}; Standardization and interoperability requirements \cite{ref34} & Enabling collaborative innovation; Ensuring seamless integration across vendors \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\section{Future Research Directions and Trends} \label{sec:future_directions}

The integration of Artificial Intelligence (AI), particularly Generative AI (GenAI) and Large Language Models (LLMs), into telecommunication networks is poised to revolutionize network design, operation, and service delivery. Realizing this transformative potential, however, necessitates addressing numerous research challenges and exploring emergent trends. Future research must encompass advancements in core AI capabilities tailored for networks, mitigation of inherent AI limitations, enhancement of specific applications and architectures, exploration of novel technological paradigms, establishment of robust standards and benchmarks, and resolution of practical deployment hurdles.

\subsection{Advancing AI Capabilities for Future Networks}

A primary research thrust involves enhancing the fundamental capabilities of AI models to meet the unique demands of future wireless networks. Machine Learning (ML) is already recognized as pivotal for realizing Beyond 5G (B5G) systems, aiming to progress beyond current applications toward more autonomous and intelligent network functions \cite{ref1}. The anticipated scale and complexity of 6G networks demand the development and adaptation of large-scale AI models, including GenAI and LLMs, for sophisticated tasks such as network design, dynamic configuration, and real-time operational management \cite{ref2, ref13}. Efficiently deploying these computationally intensive models requires innovative infrastructure solutions. Optimizing hierarchical edge AI frameworks within telco networks represents a key area, potentially leveraging existing assets like regional data centers and near-RAN sites as tiered inference caches to mitigate latency \cite{ref3, ref31}. This optimization includes developing effective vector database caching strategies at the network edge to enhance the Quality of Service (QoS) for LLM-based services \cite{ref14}.

Furthermore, continued progress in domain-specific AI techniques is essential, as generic models often lack the nuanced understanding required for the telecommunications sector \cite{ref4, ref17}. Promising research avenues include refining hybrid approaches like Knowledge Graph-Retrieval Augmented Generation (KG-RAG) to ground LLM responses in verified domain knowledge \cite{ref4}, developing specialized models such as WirelessLLM capable of handling multi-modal wireless data and understanding physical layer constraints \cite{ref17}, and creating tailored Telecom LLMs \cite{ref25} or more compact, efficient models like TeleRoBERTa for specific tasks like querying 3GPP documentation \cite{ref18}. Maturing GenAI capabilities for reliable critical thinking, encompassing reasoning and planning, is crucial for automating complex decision-making processes in network operations, potentially bridging the gap between the structure of symbolic AI and the flexibility of GenAI \cite{ref6}. Optimizing the synergy between LLMs and Vector Databases (VecDBs) is another vital direction, focused on addressing issues like knowledge staleness and enabling the efficient retrieval of relevant information for context-aware generation \cite{ref8, ref14}.

Advancing ML techniques specifically geared towards increased network automation remains a core objective \cite{ref12}, covering diverse areas from resource allocation to anomaly detection. This necessitates ensuring the scalability and seamless integration of GenAI across different network tiers and functions \cite{ref14, ref15, ref25, ref31}. Exploring the expanding application scope of Generative Diffusion Models (GDMs) and Diffusion Models (DMs) holds promise for network optimization tasks, potentially integrated with techniques like Deep Reinforcement Learning (DRL) or semantic communications \cite{ref19, ref33}. The development of hybrid approaches, combining LLMs with other AI/ML techniques (e.g., DRL, symbolic AI, GDMs \cite{ref25}), Mixture of Experts (MoE) architectures \cite{ref20}, game theory \cite{ref21}, or agentic frameworks \cite{ref35}, will be vital for creating robust and versatile solutions. Finally, advancements in agentic AI, coupled with sophisticated generative retrieval mechanisms, are required to enable autonomous agents capable of complex reasoning and decision-making within dynamic telecom environments \cite{ref35}.

\subsection{Addressing Core AI Challenges}

Despite their significant potential, AI models, particularly GenAI and LLMs, exhibit intrinsic limitations that must be rigorously addressed for trustworthy deployment in critical network infrastructure. Sustained research is imperative to mitigate fundamental issues such as inherent bias, deficits in explainability, security vulnerabilities, lack of robustness, susceptibility to factual inaccuracies (hallucinations), and overall trustworthiness \cite{ref8, ref16, ref17, ref31, ref34, ref35}. The propensity of LLMs to generate plausible yet incorrect information, or to lack understanding of physical constraints, poses considerable risks in network control scenarios \cite{ref8, ref17}.

Security and privacy concerns are paramount as AI assumes greater control within telecommunication networks. Research must prioritize the development of robust security measures specifically for AI-driven network functions \cite{ref12}, techniques for securing distributed edge-cloud AI deployments \cite{ref31}, protection against AI-specific threats within novel architectures like AI-RAN \cite{ref34}, and methods to ensure the security and privacy of emergent agentic AI systems \cite{ref35}. A specific focus is also required on leveraging AI/GAI to enhance physical layer security \cite{ref5, ref30}, while concurrently ensuring the security of the AI models themselves against adversarial attacks \cite{ref20}.

Future networks are expected to generate vast quantities of multi-modal data, encompassing sensor readings, signal measurements, text logs, and more. Developing efficient techniques for fusing and processing this diverse data is crucial, as current models like LLMs, primarily trained on text, often struggle with such heterogeneity \cite{ref17, ref23}. Managing the computational complexity and ensuring the scalability of increasingly large AI models remains a significant challenge \cite{ref20, ref25, ref31}. This demands innovations in model optimization (e.g., employing MoE strategies \cite{ref20}), distributed computing frameworks \cite{ref31}, and intelligent resource management within network architectures \cite{ref34}. Lastly, the substantial energy consumption associated with training and operating large AI models necessitates dedicated research into energy-efficient AI algorithms and deployment strategies. This is particularly critical in edge-cloud and AI-RAN contexts \cite{ref29, ref31, ref34, ref35} and is essential for contributing to the sustainable evolution of future networks.

\subsection{Enhancing Specific Applications and Architectures}

Beyond core capabilities and challenges, research efforts must target the advancement of AI/GAI within specific network applications and architectural paradigms. Significant opportunities exist across various domains, as summarized in Table~\ref{tab:app_research}. Key areas include enhancing GAI for security applications, which involves improving model robustness, enabling deployment across diverse scenarios, optimizing resource efficiency, and exploring secure semantic communication avenues \cite{ref5}, with techniques like MoE-enabled GAI showing promise \cite{ref20}. Continued investigation into AI's role in physical layer security, especially for complex schemes like Non-Orthogonal Multiple Access (NOMA), remains critical \cite{ref30}.

\begin{table*}[htbp]
\centering
\caption{Selected AI/GAI Research Thrusts in Specific Telecommunication Applications and Architectures}
\label{tab:app_research}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lll}
\hline
\textbf{Application/Architecture} & \textbf{Key AI/GAI Techniques} & \textbf{Primary Research Focus} \\ \hline
Network Security & GAI (incl. MoE-enabled), PhySec AI & Robustness, resource efficiency, secure semantic comm. \cite{ref5, ref20, ref30} \\
Semantic Communication & GAI, Multimodal AI & Context awareness, semantic reasoning, efficiency (GAI-SCN) \cite{ref23} \\
Game Theory in Networks & GAI & Simplified modeling, solving complex strategic interactions \cite{ref21} \\
Network \& Service Mgmt (NSM) & LLMs & Automation (monitoring, planning, deployment, support) \cite{ref25} \\
Low-Carbon AIoT & GAI & Energy efficiency optimization, carbon emission reduction \cite{ref29} \\
AI-RAN & AI/GAI & Spectrum mgmt., novel architectures, resource allocation \cite{ref34} \\
Edge-Cloud Systems & GenAI, Distributed AI & Collaborative training, inference offloading, synchronization \cite{ref3, ref14, ref31} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table*}

Further exploration of GAI-integrated Semantic Communication (GAI-SCN) is needed to unlock its potential for efficient, context-aware communication by leveraging GAI's ability to handle multimodal content and enhance semantic reasoning \cite{ref23}. Similarly, continued research into leveraging GAI to extend game theory applications in networking can simplify the modeling and resolution of complex strategic interactions \cite{ref21}. Advancing LLMs specifically for Network and Service Management (NSM) tasks—covering monitoring, planning, deployment, and support—represents a significant opportunity for increased automation \cite{ref25}. Open research directions also exist for applying GAI to enable low-carbon AIoT systems, optimizing for reduced energy consumption and carbon emissions \cite{ref29}. Maturing the AI-RAN concept requires focused research on AI-driven spectrum management, novel network architectures incorporating AI, and intelligent resource allocation strategies \cite{ref34}. Finally, advancing edge-cloud GenAI systems necessitates addressing challenges in collaborative training protocols, efficient inference offloading mechanisms, model distribution strategies, and maintaining synchronization across distributed components \cite{ref3, ref14, ref31}.

\subsection{Exploring New Paradigms and Technologies}

Future research should extend beyond refining existing approaches to explore novel paradigms and integrate emerging technologies. This includes advancing technical solutions tailored for specific, demanding communication domains, such as the integration of Terahertz (THz) communications with Reconfigurable Intelligent Surfaces (RIS) in Non-Terrestrial Networks (NTNs) \cite{ref10}. A significant trend is the continued exploration and development of hybrid AI approaches. Combining the complementary strengths of different techniques—such as symbolic AI, GenAI, VecDBs, agentic AI, and various ML methods—is anticipated to yield more powerful, reliable, and versatile systems for network control and optimization \cite{ref6, ref8, ref20, ref21, ref23, ref25, ref29, ref35}.

Addressing the research opportunities presented by GenAI specifically for the Internet of Things (IoT) is also critical, given the proliferation of connected devices and the potential for GenAI to enhance IoT applications, manage complex device interactions, and optimize resource usage \cite{ref15, ref26, ref29}. Ultimately, a key goal is the seamless integration of diverse AI techniques—spanning discriminative models, generative models, reasoning systems, and retrieval mechanisms—to achieve holistic network optimization and management across various network layers and operational domains \cite{ref31, ref34, ref35}.

\subsection{Standards, Benchmarking, and Responsible AI}

The successful integration, interoperability, and trustworthiness of AI in telecommunications critically depend on robust standardization and rigorous evaluation methodologies. Establishing standardized benchmarks and evaluation metrics specifically designed for telecom GenAI applications is essential for objectively comparing the performance and suitability of different models and approaches \cite{ref18}. There is a pressing need to advance the practical implementation of responsible AI principles, ensuring accountability, traceability, trustworthiness, and explainability are inherent in AI-driven network functions \cite{ref16}. This involves designing network architectures and AI systems that embed responsibility from the ground up \cite{ref16}, while actively addressing inherent model limitations related to trust, fairness, and bias \cite{ref8, ref17}. Promoting standardization and ensuring interoperability are fundamental prerequisites for fostering multi-vendor ecosystems and facilitating widespread adoption of AI technologies in telecom networks \cite{ref34}. Practical implementation guidelines and standardization efforts are particularly needed for emerging frameworks like AI-RAN to ensure consistent deployment and performance \cite{ref34}.

\subsection{Open Challenges and Facilitating GAI Applicability}

Despite rapid progress, numerous open challenges continue to impede the practical, large-scale deployment of GAI in mobile and wireless networks \cite{ref26}. These encompass the previously discussed issues of model robustness, scalability limitations, security vulnerabilities, ensuring trustworthiness, managing energy consumption, and the critical need for domain-specific adaptation and grounding \cite{ref2, ref4, ref5, ref6, ref8, ref15, ref17, ref21, ref25, ref29, ref31, ref34, ref35}. Facilitating broader GAI applicability requires concerted and collaborative research efforts across all these dimensions. Bridging the gap between the theoretical potential of AI/GAI and its tangible, real-world operational value in next-generation communication systems remains the overarching goal. Addressing these multifaceted challenges collectively will pave the way for the realization of truly intelligent, automated, and efficient future networks.

\section{Conclusion}
\label{sec:conclusion}

This paper has explored the profound and accelerating integration of Artificial Intelligence (AI), encompassing Machine Learning (ML), Generative AI (GenAI), Large Language Models (LLMs), and Agentic AI, into the fabric of modern telecommunications. The discourse has progressed beyond viewing AI and wireless communications as disparate fields \cite{ref1}, now recognizing AI not merely as an enhancement but as a fundamental enabler for the complex, dynamic, and demanding requirements of current and future networks, including 5G, Beyond 5G (B5G), and 6G \cite{ref1, ref6, ref12, ref17, ref28}. The following subsections synthesize the key findings, challenges, and future trajectory of this transformative convergence.

\subsection{Summary of AI's Integral Role and Transformative Potential}
\label{subsec:summary_role}

AI, in its diverse manifestations, has become indispensable for managing the escalating complexity and varied service demands inherent in modern telecommunication networks \cite{ref1, ref3, ref5, ref11, ref12, ref28}. Foundational ML techniques continue to enhance critical functions such as network optimization, resource allocation, traffic control, and security \cite{ref1, ref5, ref11, ref12, ref30}. More recently, the advent of GenAI, LLMs, and Agentic AI heralds a new wave of innovation, driving unprecedented capabilities \cite{ref2, ref4, ref8, ref9, ref16, ref18, ref19, ref20, ref21, ref22, ref23, ref24, ref25, ref26, ref29, ref31, ref32, ref33, ref34, ref35}. GenAI and LLMs, specifically, offer potent tools for automating intricate operational tasks, generating novel solutions for network management, enabling sophisticated customer interactions, and improving overall efficiency \cite{ref2, ref4, ref8, ref9, ref19, ref26, ref32}. This integration permeates the entire network stack, influencing domains from physical layer security \cite{ref5, ref16} and semantic communications (SemCom) \cite{ref24, ref34} to Radio Access Network (RAN) intelligence (AI-RAN) \cite{ref25} and the management of the Internet of Things (IoT) \cite{ref22, ref29}. Ultimately, this deep embedding signifies a paradigm shift towards intelligent, data-driven network operations equipped to handle the scale and dynamism required by future services, including autonomous systems, immersive experiences, and massive IoT deployments \cite{ref1, ref17}.

\subsection{Recapitulation of Key Applications, Benefits, Shifts, Challenges, and Opportunities}
\label{subsec:recap_apps_benefits_challenges}

Throughout this work, diverse applications leveraging AI for substantial benefit have been highlighted. Notable examples include network slicing optimization \cite{ref11}, dynamic resource allocation \cite{ref1, ref8}, enhanced Quality of Service (QoS) management \cite{ref11, ref13, ref14}, predictive maintenance, anomaly detection \cite{ref1, ref9}, advanced security measures \cite{ref5, ref16, ref22, ref30}, intelligent customer service via chatbots \cite{ref2}, and comprehensive network automation leading to efficiency gains \cite{ref1, ref2, ref8, ref26, ref31}. The primary advantages manifest as reduced operational expenditures, elevated network performance, improved user experience, and the capacity to support innovative and demanding services \cite{ref1, ref2, ref8, ref31}. Concurrently, AI integration is catalyzing significant architectural shifts. Key developments include the migration towards edge intelligence, utilizing telecommunications infrastructure for low-latency AI inference and caching \cite{ref3, ref13, ref31}, the formulation of AI-native RAN architectures (AI-RAN) \cite{ref25}, and the exploration of foundation model-based network systems \cite{ref21}. Nevertheless, realizing this potential faces considerable hurdles. These encompass the requirement for vast high-quality datasets, the computational expense and latency associated with large AI models \cite{ref4, ref7, ref9, ref14}, challenges in model adaptability and domain-specific tuning \cite{ref4, ref9, ref18, ref26}, ensuring robust security and privacy \cite{ref3, ref5, ref16, ref30}, managing inherent network complexity \cite{ref1, ref28}, achieving model explainability and trustworthiness \cite{ref21}, and bridging the gap between research advancements and practical deployment \cite{ref2}. Crucially, these challenges simultaneously represent substantial opportunities for innovation in algorithms, architectures, and operational methodologies \cite{ref6, ref9, ref26, ref28, ref29, ref30, ref31, ref32, ref34, ref35}.

\subsection{Recap of Novel Approaches and Frameworks}
\label{subsec:recap_novel_approaches}

This paper surveyed several innovative approaches and frameworks designed to harness AI's capabilities while mitigating associated challenges within the telecommunications domain. Prominent examples include: the VELO framework, which employs vector database caching at the network edge to optimize LLM QoS by reducing latency and cost without modifying the core LLM \cite{ref14}; the application of advanced GenAI models like Generative Diffusion Models (GDMs) and Diffusion Models (DMs) for complex network optimization and enhancing semantic communications \cite{ref15, ref19, ref33}; the investigation of Foundation Model (FM) architectures and their integration complexities \cite{ref21}; the utilization of GenAI within IoT and AIoT ecosystems for tasks such as data generation and enabling low-carbon operations \cite{ref22, ref29}; the development of GAI-enabled game theory for intelligent strategic decision-making in mobile networks \cite{ref23}; the creation of GAI-Integrated Semantic Communication Networks (GAI-SCN) leveraging GenAI for context reasoning and efficient knowledge management \cite{ref24, ref34}; the specialization of LLMs for specific Network and Service Management (NSM) tasks \cite{ref26}; the conceptualization and advancement of AI-RAN for intelligent radio resource management \cite{ref25}; and the emergence of Agentic AI, utilizing autonomous agents capable of perception, reasoning, and action for sophisticated network control and planning, often augmented by generative information retrieval techniques \cite{ref35}. These frameworks signify the forefront of research, propelling the field towards more capable, efficient, and autonomous communication systems.

\subsection{Reiterating Major Challenges and Open Research Questions}
\label{subsec:reiterating_challenges}

Despite considerable progress, significant challenges and unresolved research questions persist. A primary area of concern, particularly with LLMs and GenAI, involves their propensity for hallucination, reliance on potentially outdated training data, and difficulties in adapting to highly specialized and rapidly evolving domains like telecommunications standards \cite{ref4, ref7, ref9, ref18, ref26}. Mitigation techniques such as Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG) integration show promise but necessitate further refinement \cite{ref4, ref7}. The substantial computational and energy costs associated with training and deploying large AI models present significant obstacles, demanding focused research into model compression, energy-efficient hardware, effective edge deployment strategies, and low-carbon AI solutions \cite{ref3, ref9, ref13, ref14, ref29, ref31}. Ensuring the security and privacy of data used for AI training and operation, alongside protecting AI models from adversarial attacks, remains paramount, especially in critical areas like physical layer security \cite{ref5, ref16, ref30}. Standardization efforts for AI integration in telecommunications, particularly for GenAI and AI-RAN, are still in their nascent stages \cite{ref2, ref18, ref25}. Developing AI systems, notably Agentic AI, endowed with robust critical thinking, reasoning, and planning capabilities comparable to human experts represents an ongoing and complex challenge \cite{ref9, ref35}. Furthermore, issues concerning scalability, seamless integration with legacy systems \cite{ref10}, ensuring fairness, accountability, and transparency (responsible AI) \cite{ref21}, and devising effective methods for multi-modal data fusion in wireless environments \cite{ref18} require sustained investigation \cite{ref6, ref8, ref12, ref15, ref17, ref20, ref22, ref23, ref24, ref31, ref32, ref34}.

\subsection{Final Outlook: Towards Fully Intelligent and Autonomous Networks}
\label{subsec:final_outlook}

The trajectory points unequivocally towards increasingly intelligent, autonomous, and self-optimizing communication networks \cite{ref9, ref12, ref17, ref26, ref31}. This evolution is propelled by the synergistic integration of advanced AI/ML, GenAI, LLMs, and Agentic AI paradigms \cite{ref9, ref35} with next-generation wireless systems (B5G/6G) \cite{ref1, ref6, ref12, ref17}, sophisticated network architectures embracing edge and cloud intelligence \cite{ref3, ref13, ref31}, and novel communication concepts such as SemCom \cite{ref24, ref34}. We envision future networks capable of predictive management, dynamic adaptation to fluctuating conditions and user requirements, automated service provisioning, and resilient operation achieved with minimal human intervention \cite{ref9, ref15, ref35}. While substantial challenges related to AI robustness, efficiency, trustworthiness, and domain-specific intelligence must be addressed \cite{ref7, ref9, ref12, ref21, ref26}, the continuous advancements in AI algorithms, computational infrastructure, and their tailored application within telecommunications promise to revolutionize how networks are designed, deployed, and operated. This progress ultimately paves the way for realizing the vision of truly intelligent networked systems \cite{ref13, ref32}.

 

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}