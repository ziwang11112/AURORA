Index,Citation,Summary
1,"G. Franceschelli and M. Musolesi, ""Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges,"" Journal of Artificial Intelligence Research, vol. 79, pp. 851-903, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15278","This survey reviews the intersection of generative artificial intelligence (AI) and reinforcement learning (RL), discussing key concepts, methodologies, and open research questions. Generative AI models, such as VAEs, GANs, transformers, and diffusion models, have achieved impressive results but often struggle to encode complex objectives and alignment with human values. RL offers solutions by enabling generative models to work with non-differentiable rewards and embedding desired characteristics. The paper systematically explores three RL application classes: (1) RL as an alternative for mere generationenabling sequential tasks like text generation (e.g., SeqGAN, MaskGAN, CodeRL), (2) RL for objective maximizationdirectly optimizing non-differentiable metrics like BLEU, ROUGE for text, molecular properties for molecule generation, or subjective image scores for diffusion models, and (3) RL for incorporating characteristics not easily captured with explicit metricsprimarily via reward modeling and reinforcement learning from human/AI feedback (RLHF), crucial for aligning large language models (LLMs) and generative systems with nuanced human preferences. Challenges highlighted include managing exploration in vast action spaces, dealing with computational demands, preventing overfitting to sparse or flawed metrics (Goodharts Law), ensuring robust, fair, and diverse reward modeling, and safeguarding against reward hacking and adversarial attacks such as LLM jailbreaks. The authors advocate for deeper integration of advanced RL techniques, improved robustness, diverse alignment, and preventive strategies against vulnerabilities in next-generation generative AI models. The work comprehensively charts state-of-the-art applications across chemistry, vision, music, text, and code, using RL paradigms such as on-policy, off-policy, hierarchical, and reward-weighted optimization, and concludes that RL is both an enabler and a new frontier of challenges in generative AI."
2,"K. Sowa and A. Przegalinska, ""From Expert Systems to Generative Artificial Experts: A New Concept for Human-AI Collaboration in Knowledge Work,"" Journal of Artificial Intelligence Research, vol. 82, pp. 1-31, 2025. [Online]. Available: https://doi.org/10.1613/jair.1.17175","This paper introduces the concept of Generative Artificial Experts (GAEs), a novel class of generative AI agents specifically designed for collaborative knowledge work between humans and AI. GAEs are characterized by seven defining traits, including specialized domain expertise, bounded autonomy, synthetic personas, and multimodal generative capabilities, distinguishing them from other generative AI systems through a proposed taxonomy. Utilizing literature review and abductive reasoning, the authors establish GAEs as an evolutionary step beyond traditional expert systems, driven by advancements in human-AI collaboration and generative AI technologies. Although technical development is not detailed and practical implementations are just emerging, the paper offers conceptual analysis and illustrative examples to showcase potential applications and the transformative role GAEs could play in the future of knowledge work."
3,"L. Lin, H. Mu, Z. Zhai, M. Wang, Y. Wang, R. Wang, J. Gao, Y. Zhang, W. Che, T. Baldwin, X. Han, and H. Li, ""Against The Achilles' Heel: A Survey on Red Teaming for Generative Models,"" Journal of Artificial Intelligence Research, vol. 82, pp. 191-256, 2025. [Online]. Available: https://doi.org/10.1613/jair.1.17654","Generative AI models, such as LLMs and VLMs, present significant vulnerability surfaces due to their flexible capabilities, and the field of red teaming has rapidly evolved to address these concerns. This comprehensive survey covers 129 papers, introducing a taxonomy of attack strategies grounded in intrinsic model capabilitiessuch as compliance, indirection, generalization, and manipulationand systematizes both attack and defense pipelines, including evaluation benchmarks. Automated red teaming, reconceptualized as a search problem with state space, goals, and operations, enables diverse prompt generation via optimization, genetic algorithms, and deep learning, while also highlighting limitations in diversity and coverage. The study emphasizes novel and still-under-addressed risks in multilingual, multimodal scenarios, and overkill in safety training, where benign prompts are mistakenly refused. Benchmarks and metrics remain fragmented, impeding standardized assessment, and despite advances in defense mechanismsranging from training-time to inference-time and ensemble methodstrade-offs between safety and helpfulness persist, with vulnerabilities due to application-layer attacks and model manipulation. The discussion underscores the need for unified benchmarking, systematic prompt exploration, robust defenses, and cross-disciplinary approaches, especially as GenAI agents, tool integration, and multilingual deployments expand; open collaboration and transparency are positioned as key to enhancing reliability and trustworthiness for future GenAI systems."
4,"Z. Qiao, W. Nie, A. Vahdat, T. F. Miller III, and A. Anandkumar, ""State-specific protein–ligand complex structure prediction with a multiscale deep generative model,"" Nature Machine Intelligence, vol. 6, pp. 195–208, 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00792-z","NeuralPLexer is a deep generative modeling framework designed to directly predict protein-ligand complex structures and their conformational changes at atomic resolution, using only protein sequence and ligand molecular graph inputs. It integrates both auto-regressive and diffusion models, with biophysical inductive biases to capture the cooperativity between proteins and small molecules. Key components include a graph-based encoder for representing atoms and residues, a contact prediction module (CPM) leveraging attention networks to estimate intermolecular distances and contact maps, and an equivariant structure denoising module (ESDM) that generates atomic coordinates using structured diffusion while maintaining geometric constraints. The model is trained on a large, curated dataset (PL2019-74k) and utilizes a composite loss function with cross-entropy, translation-rotation-invariant denoising, and geometry-regularization terms. Benchmarked on tasks like blind docking and flexible binding site recovery, NeuralPLexer achieves state-of-the-art results, including 78% higher ligand pose accuracy than the best prior methods on PDBBind2020 and high TM-scores (average $0.906$), even for proteins with significant conformational changes. Compared to AlphaFold2 and RosettaLigand, it provides more accurate predictions and confidence estimates. Its end-to-end differentiable architecture enables efficient, scalable applications to structural biology, drug discovery, and protein engineering, and it is amenable to further improvements through richer datasets, integration of experimental data (such as binding affinities), and extension to more challenging biological scenarios. The code is publicly available at https://github.com/zrqiao/NeuralPLexer."
5,"Z. Zhang, W. X. Shen, Q. Liu, and M. Zitnik, ""Efficient generation of protein pockets with PocketGen,"" Nature Machine Intelligence, vol. 6, pp. 1382–1395, 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00920-9","PocketGen is a deep generative model designed to co-generate both the residue sequence and atomic structure of protein ligand-binding pockets, addressing key challenges in achieving sequencestructure consistency and modeling complex multiscale proteinligand interactions. PocketGen employs an equivariant bilevel graph transformer to encode interactions at atom, residue, and ligand levels, along with a sequence refinement module based on a protein language model augmented by structural adapters for cross-modal consistency. Training uses combined sequence, coordinate, and structure losses, and initialization is performed by interpolating known scaffolds. Benchmarking on CrossDocked and Binding MOAD datasets, PocketGen achieves an amino acid recovery rate (AAR) of 63.4%, outperforms baselines with a 97% success rate (higher binding affinity than references), yields high geometric fidelity ($\mathrm{scRMSD}$, $\mathrm{scTM}$, pLDDT, and KullbackLeibler divergence of geometric features), and runs over ten times faster than diffusion-based methods. PocketGen generalizes to novel proteins, various pocket sizes, and ligands, demonstrating high stability in designs (predicted $\Delta\Delta G$), with successful recapitulation and extension of experimentally validated pockets. The method's efficiency (44.2 sec for 100 pockets), direct full-atom coordinate prediction, modular neural architecture, and adaptability to larger or unseen cases mark a significant advance over existing physics-based, template, or traditional deep learning approaches. Future work aims to expand design scope, integrate biochemical priors, and validate designs experimentally. Code and data are freely available for further research and application."
6,"Y. Du, A. R. Jamasb, J. Guo, T. Fu, C. Harris, Y. Wang, C. Duan, P. Liò, P. Schwaller, and T. L. Blundell, ""Machine learning-aided generative molecular design,"" Nature Machine Intelligence, vol. 6, pp. 589–604, 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00843-5","Machine learning has provided a means to accelerate early-stage drug discovery by combining molecule generation and filtering steps in a single architecture that leverages the experience and design preferences of medicinal chemists. However, designing machine learning models that can achieve this on the fly to the satisfaction of medicinal chemists remains a challenge owing to the enormous search space. Researchers have addressed de novo design of molecules by decomposing the problem into a series of tasks determined by design criteria. Here we provide a comprehensive overview of the current state of the art in molecular design using machine learning models as well as important design decisions, such as the choice of molecular representations, generative methods and optimization strategies. Subsequently, we present a collection of practical applications in which the reviewed methodologies have been experimentally validated, encompassing both academic and industrial efforts. Finally, we draw attention to the theoretical, computational and empirical challenges in deploying generative machine learning and highlight future opportunities to better align such approaches to achieve realistic drug discovery end points."
7,"D. H. Hagos, R. Battle, and D. B. Rawat, ""Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives,"" arXiv preprint arXiv:2407.14962, accepted for publication in IEEE Transactions on Artificial Intelligence, 2024. [Online]. Available: https://arxiv.org/abs/2407.14962","The paper provides a comprehensive overview of the advances and current landscape of Generative Artificial Intelligence (AI) and Large Language Models (LLMs) in Natural Language Processing (NLP). It details the technical foundations underlying generative models including approaches such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models, and explores the evolution and architectures of LLMs like GPT, BERT, and their successors. A clear comparison of state-of-the-art LLMs is presentedperformance tabulated as: 
\[
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Parameters (B)} & \textbf{Notable Features} \\
\hline
GPT-3 & 175 & Few-shot learning, Zero-shot \\
PaLM 2 & 540 & Multilingual, Reasoning \\
LLaMA & 65 & Academic availability \\
\hline
\end{tabular}
\]
The paper emphasizes the wide range of LLM applications, from code generation to healthcare, while critically discussing persistent challengesincluding bias and fairness, interpretability, data privacy, computational cost, deepfake generation, limited context windows, and responsible deployment. Future directions are outlined, focusing on advanced bias detection, explainable AI, improved domain adaptation, human-AI collaboration, and long-term memory solutions. Ultimately, the authors argue for a responsible, ethical integration of generative AI into society and identify their work as a roadmap for researchers and practitioners aiming to harness these technologies' potential while mitigating risks."
8,"S. Daniotti, J. Wachs, X. Feng, and F. Neffke, ""Who is using AI to code? Global diffusion and impact of generative AI,"" arXiv preprint arXiv:2506.08945, 2025. [Online]. Available: https://arxiv.org/abs/2506.08945","This study investigates the adoption and impact of generative AI tools in coding by training a neural classifier to detect AI-generated Python functions across 80 million GitHub commits from 200,000 developers worldwide (20182024). By December 2024, AI wrote 30.1% of US Python functions, compared to 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4% in Russia, and 11.7% in China. Adoption is especially strong among newer GitHub users (41% AI code share vs. 28% for veterans), with no gender differences observed. Fixed-effects analyses show ramping up AI usage to 30% lifts quarterly commits by 2.4%, and fosters innovation, evidenced by 2.2% more new libraries and 3.5% more new library combinations used per developer. Economic modeling, combining wage and occupation data, estimates the annual US value of AI-assisted coding at $9.6$14.4 billion ($33B$96B for higher productivity estimates from RCTs). While results are robust and suggest widespread diffusion of generative AI, the authors note uneven uptake, methodological challenges in measuring true usage, and limitations in generalizabilityespecially for China and closed-source contexts. The study concludes that intensity, not just access, to AI tools is key for productivity and innovation gains, and that generative AI for programming is likely at the early stages of a transformative, general-purpose technology diffusion."
9,"N. Holzner, S. Maier, and S. Feuerriegel, ""Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis,"" arXiv preprint arXiv:2505.17241, 2025. [Online]. Available: https://arxiv.org/abs/2505.17241","Generative artificial intelligence (GenAI) is increasingly adopted to aid human creative tasks, but its real effect on creativity has been unclear. This meta-analysis synthesizes data from 28 studies ($n=28$, $m=8214$ participants) to evaluate GenAI's impact. Using standardized effect sizes with Hedges' $g$, the analysis compares (i) GenAI's creativity, (ii) the creativity of humans supported by GenAI, and (iii) the diversity of ideas in such collaborations. GenAI alone is no more creative than humans ($g = -0.05$), but humans working with GenAI outperform those working unaided in creative ideation tasks ($g = 0.27$). However, GenAI collaboration significantly reduces idea diversity ($g = -0.86$). The study further examines variations across different GenAI models, task types, and participant populations, concluding that GenAI serves best as a creative assistant rather than a replacement for human creativity."
10,"S. A. Obead, R. Freij-Hollanti, T. Westerbäck, and C. Hollanti, ""Private Linear Computation for Noncolluding Coded Databases,"" IEEE Journal on Selected Areas in Communications, vol. 40, no. 3, pp. 825–838, Mar. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9678319","This paper addresses the problem of private linear computation (PLC) in distributed storage systems (DSS) where $n$ noncolluding databases store $r$ messages encoded with an $[n, k]$ linear code. The objective is to allow a user to reliably compute any linear combination $y = \lambda_1 x_1 + \ldots + \lambda_r x_r$ of the messageswhere $\lambda_i \in \mathbb{F}_q$without revealing information about the function or message identities to any database, satisfying an information-theoretic privacy constraint. The key result is the exact capacity for PLC, given by $C_{PLC}(n, k, r) = \frac{1}{1 + \frac{k}{n-k} + \cdots + \frac{k^{r-1}}{(n-k)^{r-1}}}$, which generalizes the capacity for private information retrieval (PIR) to any linear function. Explicit query schemes based on index coding and coset codes achieve this capacity, demonstrating correctness and privacy. The work reveals that for MDS codes, as $n$ increases, capacity nears 1 and download cost per bit vanishes; for replicated storage ($k=1$), classical PIR capacities are recovered. The discussion highlights connections to index coding, explains the necessity of code structure for optimality, and notes that as function complexity increases (larger $r$), rate falls for fixed $n$. Future research directions include handling collusion, Byzantine databases, private computation of nonlinear functions, and seamless PLC integration with practical DSS protocols. The paper thus lays the foundations for secure, private, and efficient distributed function computation in coded storage environments, with complete information-theoretic characterization and matching constructions."
11,"T. P. Raptis, A. Passarella, M. Conti, A. Zanni, and R. Bruno, ""Distributed Data Access in Industrial Edge Networks,"" IEEE Journal on Selected Areas in Communications, vol. 38, no. 5, pp. 915–927, May 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9039732","This paper addresses the challenges of distributed data access in multi-hop wireless industrial edge networks, particularly relevant for Industry 4.0 environments where vast amounts of sensor data strain traditional centralized data management. The authors formulate the distributed data access (DDA) and caching problem, prove its NP-completeness, and propose a two-step algorithm involving computation of feasible path sets (via their ComputePathSets algorithm) and a DataCacheAccess step to maximize network lifetime while meeting data delay constraints. They design both a centralized (DCA+) and a fully distributed Proportionally Fair Rotation (PFR) scheme, conducting experiments on a real IoT-LAB testbed and extensive simulations. Results show that their DCA approach closely matches the relaxed optimum, while the distributed PFR method vastly prolongs network lifetimeby orders of magnitudeby relying solely on energy-efficient local wireless links, especially compared to centralized schemes (which incur high communication costs from global status exchanges). The methodologies respect delay constraints for all consumers and ensure balanced energy consumption, firmly supporting the use of decentralized edge-centric solutions in smart industrial settings. The studys findings stress the trade-off between centralized optimality and the scalability, efficiency, and practicality of distributed approaches and point towards future work in dynamic, real-world deployments and cross-layer optimizations."
12,"Q. Hu, G. Zhang, Z. Qin, Y. Cai, G. Yu, and G. Y. Li, ""Robust Semantic Communications with Masked VQ-VAE Enabled Codebook,"" IEEE Transactions on Wireless Communications, vol. 22, no. 12, pp. 8707–8722, Dec. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10101778/","This paper addresses the robustness of semantic communication (SC) systems against semantic noise, which arises from adversarial perturbations or task-unrelated disturbances impairing the mapping between transmitted and received semantic symbols and causing downstream task failure. The authors introduce a comprehensive framework combining adversarial training with weight perturbations, masking strategies targeting frequently noisy input regions, and a masked vector quantized-variational autoencoder (VQ-VAE) that utilizes a discrete codebook shared between transmitter and receiver. Additionally, a feature importance module (FIM) is developed to select and transmit only crucial, task-relevant codebook indices, thus reducing transmission overhead and increasing robustness. Extensive experiments on datasets such as CIFAR-10 and ImageNet show that the masked VQ-VAE with FIM delivers significant gains in task accuracy for classification and retrieval under both white-box and black-box adversarial attacks compared to traditional end-to-end SC methods. The approach demonstrates notable resilience in suppressing noise-sensitive features and maintains higher intra-class semantic similarity, as evidenced by accuracy curves and semantic similarity evaluations (see Tables VII, VIII, IX, XIII). The proposed method offers efficient, robust semantic communication by combining codebook-based feature representation with intelligent masking and feature selection, laying the groundwork for future research in adaptive SC under evolving attack models and resource constraints."
13,"J. Shin, Y. Kang, and Y.-S. Jeon, ""Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems,"" IEEE Wireless Communications Letters, vol. 13, no. 9, pp. 2382–2386, Sept. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10403736/","This paper introduces a deep-learning-based channel state information (CSI) feedback method for massive MIMO systems that utilizes a vector-quantized variational autoencoder (VQ-VAE) with shape-gain vector quantization to provide efficient, finite-bit representations of latent vectors. The proposed method separately quantizes the magnitude using a non-uniform scalar codebook based on a clipped $\mu$-law transformation and the direction using a trainable Grassmannian codebook under unit-norm constraints, substantially reducing computational complexity from $O(2^B)$ to $O(2^{\max\{B_\text{mag}, B_\text{dir}\}})$ where $B_\text{mag}$ and $B_\text{dir}$ are the bit allocations for gain and shape, respectively. A multi-rate codebook strategy is presented, using a nested codebook and an adjusted loss function to support different bit-rate feedbacks while sharing codebooks. Simulation resultsusing the COST2100 channel dataset with $N_t=32$, $N_c=1024$, and feedback overheads such as 512 bitsdemonstrate that this approach outperforms previously proposed scalar and vector quantization DL-based feedback methods in terms of normalized mean squared error (NMSE), with further improvements from the multi-rate design (e.g., at 512 bits indoor, NMSE: VQ-VAE $=-14.17\,\text{dB}$, Proposed multi-rate $=-14.93\,\text{dB}$). The methods design makes codebook storage and search more efficient and allows for practical adaptation to varying channel distributions."
14,"J. Cai, C. Wen, C. K. Wen, and S. Jin, ""Hybrid Precoding Architecture for Massive Multiuser MIMO With Dissipation: Sub-Connected or Fully Connected Structures?"" IEEE Transactions on Wireless Communications, vol. 17, no. 3, pp. 1606–1621, Mar. 2018. [Online]. Available: https://ieeexplore.ieee.org/document/8383712/","This paper investigates hybrid precoding structuresspecifically sub-connected and fully-connectedover limited feedback channels in massive multiuser MIMO systems by rigorously modeling the dissipative losses in practical analog networks using actual component characteristics and circuit theory. The system model considers a downlink scenario where a base station with $N_t$ antennas and $N_{\mathrm{RF}}$ RF chains serves $K$ single-antenna users, and hybrid precoding splits beamforming between a digital precoder $\mathbf{F}_{\mathrm{BB}}$ and an analog precoder $\mathbf{F}_{\mathrm{RF}}$. Both continuous and finite-bit phase shifter quantization, as well as limited CSIT feedback, are addressed. Analytical and simulation results reveal that while fully-connected structures theoretically provide higher flexibility, in practice their higher dissipation often results in lower spectral efficiency compared to sub-connected structures when lossy hardware components are considered. Closed-form expressions for achievable sum-rate are derived; in both Rayleigh and mmWave models, extensive simulations show that the sub-connected architecture outperforms the fully-connected structure when practical losses are present, especially with increasing antenna count and reduced quantization bits. The study also finds that mixed-ADC architectures are beneficial under hardware constraints. The paper concludes with guidelines for analog component selection and notes that practical sub-connected hybrid designs are preferable for future massive MIMO deployments."
15,"S. Ahn, S. Kim, J. Lee, and T. Kim, ""Data Embedding Scheme for Efficient Program Behavior Modeling With Neural Networks,"" IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 6, no. 6, pp. 982-993, Dec. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9772749/","As modern programs grow in size and complexity, the importance of program behavior modeling is emerging in various areas."
16,"N. Alabbasi, O. Erak, O. Alhussein, I. Lotfi, L. Da Xu, and M. Debbah, ""TeleOracle: Fine-Tuned Retrieval-Augmented Generation With Long-Context Support for Networks,"" IEEE Internet of Things Journal, Early Access, 2025. [Online]. Available: https://ieeexplore.ieee.org/document/10935354/","LongRAG addresses limitations in traditional retrieval-augmented generation (RAG) frameworks, where short retrieval units (e.g., 100-word passages) overload the retriever and risk contextual loss. By grouping documents into long retrieval units (typically 4K tokens or more), LongRAG drastically reduces the number of required retrieval units (from 22M to 600K for NQ) and leverages long-context LLMs as readers, enabling efficient, high-integrity retrieval. On benchmarks, LongRAG achieves EM scores of 62.7% on NQ and 64.3% on HotpotQA, matching or exceeding fully-supervised models, and delivers F1 scores of 25.9% on Qasper and 57.5% on MultiFieldQA-en, outperforming passage-based retrieval baselines. Optimal performance is observed at around 30K context tokens, with fewer retrieved units reducing distractors and hard negatives. The main challenge remains efficiently encoding long documents for retrieval, often approximated by max pooling over chunk embeddings, and fully realizing the reasoning capabilities of long-context LLMs. LongRAG does not require retriever or reader fine-tuning, and paves the way for future scalable, high-integrity RAG systems as embedding models and long-context LLMs continue to improve."
17,"E. Cadet, O. S. Osundare, H. O. Ekpobimi, Z. Samira, Y. W. Weldegeorgise, ""Cloud migration and microservices optimization framework for large-scale enterprises,"" Open Access Research Journal of Engineering and Technology, vol. 7, no. 2, pp. 046–059, 2024. [Online]. Available: https://oarjpublication.com/journals/oarjet/sites/default/files/OARJET-2024-0059.pdf","The paper addresses the digital transformation of large-scale enterprises through a comprehensive framework for cloud migration and microservices optimization. Recognizing the complexity and risks of transitioning monolithic legacy systems to agile, scalable cloud-native architectures, the authors present a phased approach encompassing assessment, strategy formulation, design, migration, and post-migration optimizationeach phase supported by governance and automation. Drawing on industry best practices (e.g., AWS, Microsoft, CNCF), literature review, and large-enterprise case studies, the framework introduces systematic asset cataloging, business case development, architectural redesign using microservices (often leveraging domain-driven design), container orchestration, and continuous integration/delivery automation. Empirical results show improvements such as up to 35% reduction in migration lead time, 27% fewer incidents, and 22% cost savings post-migration. Yet, challenges persist in decomposing legacy systems, achieving optimal service granularity, ensuring secure compliance across distributed systems, and overcoming cultural barriers to DevOps adoption. The discussion highlights solutions like phased pilots, rigorous governance, alignment of IT and business, and investment in observability. Future directions include AI/ML-driven orchestration, more robust multi-cloud strategies, advanced telemetry, and exploration of serverless paradigms. The authors conclude that their reference framework equips IT leaders and architects for efficient, resilient digital transformation, with ongoing evolution needed to address persistent operational and technological hurdles."
18,"S. Prasad, V. Kumar, ""Enhancing customer experience through AI-driven language processing in telecommunications,"" Open Access Research Journal of Engineering and Technology, vol. 7, no. 1, pp. 102–114, 2024. [Online]. Available: https://oarjpublication.com/journals/oarjet/sites/default/files/OARJET-2024-0027.pdf","This paper reviews how AI-driven language processing, including chatbots, virtual assistants, sentiment analysis, and large language models (LLMs), is transforming customer experience (CX) within the telecommunications sector. Driven by the demand for personalized and efficient customer interactions, telecom providers are adopting NLP tools to automate support, interpret unstructured data, and resolve routine inquiries. Literature reviews and case studies show measurable improvements: AI chatbot deployment raised first-contact resolution rates, reduced average handling time, and increased customer satisfaction, while sentiment analysis tools helped in real-time escalation of dissatisfied customers and reduced churn. Advanced LLMs, such as GPT-based agents, enabled automation of more complex queries, further increasing automation containment rates without sacrificing the quality of customer experience. Key challenges include adapting NLP to telecom-specific language, providing multilingual support, ensuring seamless legacy system integration, and meeting security and data privacy requirements (e.g., GDPR compliance). The discussion emphasizes that customer acceptance is highest when AI systems allow effortless escalation to humans and provide transparency in their decision-making. The paper concludes that, despite integration and trust hurdles, AI-driven language processing delivers operational gains and added customer value, recommending focused investments in domain-specific NLP training, privacy standards, and hybrid human-AI workflows to maximize impact in telecommunications."
19,"B. Basu, A. Sharma, P. Patil, ""Pioneering digital innovation strategies to enhance financial performance in satellite telecommunications using data analytics,"" Open Access Research Journal of Engineering and Technology, vol. 7, no. 1, pp. 126–141, 2024. [Online]. Available: https://oarjpublication.com/journals/oarjet/sites/default/files/OARJET-2024-0029.pdf","This paper investigates digital innovation strategies in the satellite telecommunications industry, focusing on how big data, machine learning, and advanced analytics improve financial performance and operational efficiency. Employing a mixed-methods approach that includes literature synthesis, benchmarking KPIs, and a real-world case study, the authors demonstrate that advanced analytics can decrease churn rates by 1520%, yield annual cost savings of around 12% through predictive maintenance, and increase average revenue per user (ARPU) by about 9% via digital revenue streams such as dynamic bandwidth allocation. However, challenges like integrating legacy systems, high initial investments, workforce upskilling, data security, and change resistance persist. The discussion highlights that strong ROI can be achieved if companies prioritize change management, leadership buy-in, and an agile innovation culture. The paper concludes that a comprehensive digital roadmap and investment in analytics are crucial for sustained profitability, with future work suggested in AI-driven optimization, predictive customer analytics, blockchain for contracts, and open data standards to advance industry transformation."
20,"G. M. Yilma, J. A. Ayala-Romero, A. Garcia-Saavedra, and X. Costa-Perez, ""TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs,"" arXiv preprint arXiv:2406.07053, Jun. 2024. [Online]. Available: https://arxiv.org/pdf/2406.07053","The paper introduces TelecomRAG, a modular, open-source Retrieval Augmented Generation (RAG) framework tailored for the domain of mobile telecommunications standards, specifically targeting Release 18 of 3GPP. Designed to aid R&D professionals and regulators navigating vast and evolving technical standards, TelecomRAG employs a domain-adapted, multi-vector retrieval system using ColBERT, integrated with state-of-the-art large language models (LLMs) for robust question answering. The framework is evaluated against a curated benchmark of 383 industry-authored technical queries, showing that multi-vector retrieval achieves a Top-5 recall of 70%substantially outperforming single-vector methods (48%)and that careful domain chunking nearly reaches 90% recall on certain question types. Integration with leading LLMs such as GPT-4-Turbo, Gemini 1.5, and Claude-Opus demonstrates improved technical summary quality, although challenges persist with zero-grounding and cross-document queries, ambiguity, and figure/table retrieval. The discussion emphasizes the necessity of fine-grained evaluation and user feedback, while acknowledging difficulties posed by evolving standards and cross-document reasoning. TelecomRAGs codebase, curated standards, and benchmark queries are publicly released, facilitating trustworthy and efficient AI-powered navigation of telecom standards, with future work addressing continual updates, user-adaptive retrieval, and enhanced interpretability and multimodality. Resources are available at https://github.com/telecom-gpt/TelecomRAG."
21,"F. Jiang, W. Zhu, L. Dong, K. Wang, K. Yang, C. Pan, and O. A. Dobre, ""CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model,"" arXiv preprint arXiv:2502.18763, Feb. 2025. [Online]. Available: https://arxiv.org/abs/2502.18763","Large Language Models (LLMs) are poised to play a critical role in 6G communications, but conventional LLMs face challenges due to inadequate domain data, restricted input modalities, and difficulties with knowledge retrieval. To address these, the authors introduce CommGPT, a domain-specialized, multimodal LLM for communications. The system leverages CommDataa curated dataset of protocols, standards, patents, papers, code, and Wikipediafor pretraining and instruction-based fine-tuning. CommGPT incorporates a multimodal encoder utilizing BLIP (for semantic image features) and QOCR (for low-level text in infographics/tables), and introduces a Graph and Retrieval-Augmented Generation (GRG) framework which jointly uses Knowledge Graphs (KGs) for global, structured retrieval and Retrieval-Augmented Generation (RAG) for localized document retrieval. Experiments with the Gemma 2-9b-instruct base model show substantial gains: baseline accuracy rose from 37% to 54% with domain-specific data, and to 91% when combining RAG+KG (the GRG framework). Comparative results indicate that while both open- and closed-source generic LLMs lag behind, CommGPTs architecture, especially its GRG module, allows it to outperform all tested competitors on domain Q&A. Thus, CommGPT demonstrates that open-source models, when tailored with specialized data and multi-scale retrieval mechanisms, can achieve and even surpass performance of proprietary counterparts in complex, multimodal communication domains."
22,"A. Karapantelakis, M. Thakur, A. Nikou, F. Moradi, C. Olrog, F. Gaim, H. Holm, D. D. Nimara, and V. Huang, ""Using Large Language Models to Understand Telecom Standards: Challenges and Lessons Learned,"" arXiv preprint arXiv:2404.02929, Apr. 2024. [Online]. Available: https://arxiv.org/abs/2404.02929","This paper investigates the use of Large Language Models (LLMs) as question answering (QA) assistants for navigating the complex and vast 3GPP telecom standards. The authors present three main contributions: (1) a benchmark and methodology for evaluating LLM QA performance, (2) data preprocessing and fine-tuning strategies to enhance LLM accuracy, and (3) the introduction of TeleRoBERTa, a lightweight model with performance comparable to larger foundation LLMs yet requiring significantly fewer parameters. They explore both fine-tuning and Retrieval Augmented Generation (RAG) methods for adaptation to the telecom domain, as well as quantization approaches for deployment on resource-constrained devices. Experimental results, measured via metrics such as BERTScore and a GPT-4-based reference evaluation, demonstrate that TeleRoBERTa and similar LLMs can serve as credible reference tools for telecom documents, supporting applications in troubleshooting, operations, and product development. Challenges identified include the misinterpretation of technical jargon, difficulty with tabular information and cross-references, and the need for improved domain adaptation. The study concludes that LLMs, especially with advanced preprocessing and fine-tuning like Supervised Fine Tuning (SFT), hold promise as digital assistants for the 3GPP specifications, and future work aims to expand their utility to broader operations and software development contexts in telecommunications."
23,"G. Lan, et al., “Communication-Efficient Federated Learning for Resource-Constrained Edge Devices,” IEEE Transactions on Machine Learning in Communications and Networking, 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10233897/","This paper addresses the communication bottleneck in federated learning (FL) for resource-constrained edge devices connected via imperfect wireless links, proposing a hierarchical wireless FL system that uses over-the-air computation (AirComp) for efficient model update aggregation at the cell edge. To further reduce communication overhead, the authors introduce low-rank tensor decompositions (such as CANDECOMP/PARAFAC or Tucker) to compress DNN parameter updates, and they develop a joint optimization framework for allocating communication, computation, and model compression resources to maximize convergence rate under runtime constraints. The system further employs a lattice-coding strategy to improve AirComp robustness to channel noise and interference. Experimental results on standard datasets (MNIST, CIFAR10) and popular DNN architectures demonstrate that this framework can achieve model-update compression ratios up to 100 with negligible accuracy loss, and overall FL speedups of 35 compared to vanilla wireless FL, all while being robust to wireless noise. The study discusses practical trade-offs, implementation challenges for transmission of large-scale model updates, and dynamic resource allocation strategies, highlighting the crucial role of compression and robust aggregation for scalable wireless FL. Future directions include privacy-preserving compression, adaptive and secure aggregation techniques, and real-world system deployments supporting non-IID and time-varying data."
24,"D. Huang, et al., “Physical Layer Spoof Detection and Authentication for IoT Devices Using Deep Learning Methods,” IEEE Transactions on Machine Learning in Communications and Networking, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10568158/","In this work, we propose an authentication method based on radio frequency fingerprinting (RFF) using deep learning. This method can be implemented on the base station side and is capable of detecting spoofed Internet of Things (IoT) devices at the physical layer. Deep learning models are used to extract unique RFF features from the received signals, enabling the identification and authentication of legitimate devices while detecting and blocking attackers attempting to spoof device identities."
25,"J. Wang, S. Liu, S. Liu, C. Yuen, Y. Zhang, and Z. Han, ""Generative Artificial Intelligence Assisted Wireless Sensing: Human Flow Detection in Practical Communication Environments,"" IEEE Journal on Selected Areas in Communications, vol. 42, no. 10, pp. 2737–2753, Oct. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10557650","This paper introduces G-HFD, a generative AI-assisted human flow detection system designed to enhance wireless sensing using channel state information (CSI). G-HFD estimates the velocity and acceleration of human-induced reflections (HIR) and employs a proposed Unified Weighted Conditional Diffusion Model (UW-CDM) for robust denoising and accurate detection of the number of moving human targets. By leveraging CSI from a uniform linear array with wavelength spacing, the system estimates time of flight (ToF) and direction of arrival (DoA), solving ambiguity in DoA spectra through UW-CDM, even when antenna spacing exceeds half a wavelength. Clustering observed parameters (velocity, DoA, ToF) via affinity propagation and K-means enables identification of human subflows and subflow sizes. Evaluations with realistic wireless communication signals demonstrate a subflow size detection accuracy of up to 91%, outperforming traditional AI methods in noise resilience and fine-grained detection. While detection accuracy declines with increased target numbers or subflow sizes, it improves with higher packet rates, and further gains are anticipated with more antennas and faster diffusion model inference. The proposed methods directly address ambiguity in DoA estimation and denoising in challenging real-world conditions, confirming the significant potential of generative AI in fine-grained wireless sensing and setting the stage for future real-time, hardware-informed applications."
26,"L. Bariah, M. Debbah, M.-S. Alouini, A. Mohammadi, and H. Yanikomeroglu, ""Large Generative AI Models for Telecom: The Next Big Thing?,"" IEEE Journal on Selected Areas in Communications, vol. 42, no. 4, pp. 917–939, Apr. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10384630","This article examines the transformative potential of large generative AI (GenAI) models in the telecommunications (Telecom) domain, arguing that their integration could enable fully autonomous, self-evolving wireless networks. The authors introduce the concept of Large Telecom Models (LTMs), multimodal foundation models pre-trained on diverse Telecom data and fine-tuned for a wide range of downstream tasks, thus superseding single-task AI models and significantly improving efficiency, adaptability, and resource usage in future 6G networks. Key envisioned applications of GenAI include super-resolution 3D wireless environment reconstructions, predictive channel state information (CSI) estimation via FDD, optimized resource allocation, emergent protocol learning through multi-agent reinforcement learning, semantic communications, and the realization of collective intelligence across distributed on-device models. The article highlights that moving from model-per-task solutions to highly generalizable and context-aware multimodal models can facilitate the realization of AGI-empowered networks, integrating reasoning, planning, and world knowledge directly into Telecom operations. The authors discuss core challenges including the need for RF-adapted architectures, explainability, distributed training, and efficient on-device deployment, and outline open research directions, such as the development of Telecom-specific GenAI architectures, distributed aggregation protocols, interpretable AI, and optimization for energy, latency, and reliability. Overall, the paper presents a forward-looking vision where large GenAI models underpin the next leap in wireless network intelligence and autonomy."
27,"R. Schroeder, J. He, G. Brante, and M. Juntti, ""Two-stage channel estimation for hybrid RIS assisted MIMO systems,"" IEEE Transactions on Communications, vol. 70, no. 7, pp. 4793–4806, July 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9791830","Reconfigurable intelligent surfaces (RISs) are central to enabling smart radio environments in 6G, but efficient channel estimation (CE) remains challenging due to the large number of elements and the passive nature of conventional RIS, especially at mmWave/THz frequencies. This paper proposes a two-stage CE method leveraging a hybrid RIS, where a subset of RIS elements is equipped with active RF chains. In the first stage, active elements estimate channels between the base station (BS), RIS, and users using standard MIMO methods; their co-location with passive elements allows passive element characteristics to be inferred. In the second stage, the RIS operates in passive mode, and the overall channel is estimated by exploiting linear relationships between received signals and effective channel gains, employing a two-step least squares solution. Compared to conventional fully passive RIS CE, the hybrid scheme dramatically improves normalized mean square error (NMSE) and spectral efficiency (SE), with performance approaching the CramerRao lower bound (CRLB) as the number of pilots increases. Simulations highlight the robustness and efficiency gains, showing the impact of the number and location of active elements and quantifying the tradeoff between estimation performance and hardware cost. The study concludes that only a small number of active RF chains is sufficient to approach optimal performance, paving the way for practical large-scale RIS deployment. Open challenges include hardware cost, calibration, placement strategies, and scaling, with future work targeting optimal active element design, efficient protocols, and implementation in broader real-world scenarios."
28,"M. Wasilewska, K. Brzostowski, and A. Kliks, ""Artificial Intelligence for Radio Communication Context-Awareness: State of the Art, Challenges, and Opportunities,"" IEEE Transactions on Communications, vol. 69, no. 9, pp. 5533–5550, Sept. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9568860","This paper surveys Artificial Intelligence (AI) approaches for enhancing context-of-operation awareness in radio communication nodes, links, and networks, focusing on the importance of context information and the applicability of Machine Learning (ML) techniques for enriching such context. It provides a detailed analysis of various context features and evaluates the suitability of specific ML methods to different facets of context learning, presenting a synthesized framework for context-information processing, sharing, and management in radio communication networks. The paper also delineates a network-embedded subsystem for managing context information and offers recommendations for future AI/ML-based radio communication system architectures."
29,"Z. Zhao, X. Li, X. Wang, Y. Chen, and K. Wong, ""As the permeation of artificial intelligence (AI) in wireless applications continues, a cross-layer and cross-domain collaboration is essential,"" IEEE Transactions on Communications, vol. 68, no. 11, pp. 6827–6840, Nov. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9190121","As the permeation of artificial intelligence (AI) in wireless applications, some data-driven and computing-intensive services are emerging, such as mobile."
30,"G. Di Caro and M. Dorigo, ""AntNet: Distributed Stigmergetic Control for Communications Networks,"" Journal of Artificial Intelligence Research, vol. 9, pp. 317–365, 1998. [Online]. Available: https://www.jair.org/index.php/jair/article/view/10217","This paper presents AntNet, an adaptive routing algorithm for IP networks inspired by ant colony optimization (ACO) principles and stigmergy, wherein distributed mobile agents (""ants"") asynchronously explore network paths to build routing tables. AntNet operates in a fully distributed, robust way, employing lightweight agents for active path sampling, supporting multipath routing, and spreading data loads efficiently. Experimental results from both realistic simulations and small physical network tests show AntNet consistently outperforms six state-of-the-art routing algorithmsthose based on distance-vector and link-state methodsacross a variety of network sizes and traffic patterns, demonstrating superior adaptability and robustness. The algorithms strengths stem from its decentralized, collective, and flexible approach, making it highly responsive to dynamic network conditions. Notable software implementations include OmNet++, NS-2, and QualNet, and foundational references are provided, highlighting AntNets status as a major benchmark in swarm intelligence-based routing research."
31,"D. Lesaint, P. Chaslot, F. Fages, F. Jaubert, and R. Lesaint, ""Developing Approaches for Solving a Telecommunications Feature Subscription Problem,"" Journal of Artificial Intelligence Research, vol. 37, pp. 445–477, 2010. [Online]. Available: https://www.jair.org/index.php/jair/article/view/10654","Call control features, such as call-divert and voice-mail, enable users to personalize their telecommunications services by configuring feature subscriptionsselections and sequences of features from a catalog. However, these configurations are constrained to avoid undesirable feature interactions at runtime. When a requested subscription is inconsistent with the constraints, the task is to find an optimal relaxation of the subscription, which generalizes the feedback vertex set problem for directed graphs and is NP-hard. The paper presents multiple formulations for solving this problem, including constraint programming, partial weighted maximum Boolean satisfiability, and mixed integer linear programming, and compares these approaches experimentally using randomly generated instances of the feature subscription problem."
32,"D. V. Pynadath and M. Tambe, ""The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories and Models,"" Journal of Artificial Intelligence Research, vol. 16, pp. 389–423, 2002. [Online]. Available: https://www.jair.org/index.php/jair/article/view/10304","Despite notable advancements in multiagent teamwork, prior work does not rigorously address whether its methods are optimal or quantify the complexity of coordination. This paper introduces COM-MTDP (COMmunicative Multiagent Team Decision Problem), a unified framework combining decentralized partially observable Markov decision processes and economic team theory, enabling analysis of both performance optimality and computational complexity. COM-MTDP permits breakdowns of complexity for team construction across observability and communication cost regimes, and it encodes and compares different instantiations of joint intentions theory. The authors further establish a domain-independent optimal communication criterion and apply a novel, reusable COM-MTDP-based software package to empirically evaluate coordination strategies, illustrating this with example domains that compare joint intentions approaches with respect to optimal communication policy."
33,"A. Shahid, A. Kliks, A. Al-Tahmeesschi, A. Elbakary, A. Nikou, A. Maatouk, A. Mokh, A. Kazemi, A. De Domenico, A. Karapantelakis, B. Cheng, B. Yang, B. Wang, C. Fischione, C. Zhang, C. Ben Issaid, C. Yuen, C. Peng, C. Huang, C. Chaccour, C. K. Thomas, D. Sharma, D. Kalogiros, D. Niyato, E. De Poorter, E. Mhanna, E. C. Strinati, F. Bader, F. Abdeldayem, F. Wang, F. Zhu, G. Fontanesi, G. Geraci, H. Zhou, H. Purmehdi, H. Ahmadi, H. Zou, H. Du, H. Lee, H. H. Yang, I. Poli, I. Carron, I. Chatzistefanidis, I. Lee, I. Pitsiorlas, J. Fontaine, J. Wu, J. Zeng, J. Li, J. Karam, J. Gemayel, J. Deng, J. Frison, K. Huang, K. Qiu, K. Ball, K. Wang, K. Guo, L. Tassiulas, L. Gwenole, L. Yue, L. Bariah, L. Powell, M. Dryjanski, M. A. C. Galdon, M. Kountouris, M. Hafeez, M. Elkael, M. Bennis, M. Boudjelli, M. Dai, M. Debbah, M. Polese, M. Assaad, M. Benzaghta, M. Al Refai, M. Djerrab, M. Syed, M. Amir, N. Yan, N. Alkaabi, N. Li, N. Sehad, N. Nikaein, O. Hashash, P. Sroka, Q. Yang, Q. Zhao, R. Nikbakht Silab, R. Ying, R. Morabito, R. Li, R. Madi, S. E. El Ayoubi, S. D'Oro, S. Lasaulce, S. Shalmashi, S. Liu, S. Cherrared, S. B. Chetty, S. Dutta, S. A. R. Zaidi, T. Chen, T. Murphy, T. Melodia, T. Q. S. Quek, V. Ram, W. Saad, W. Hamidouche, W. Chen, X. Liu, X. Yu, X. Wang, X. Shang, X. Wang, X. Cao, Y. Su, Y. Liang, Y. Deng, Y. Yang, Y. Cui, Y. Sun, Y. Chen, Y. Pointurier, Z. Nehme, Z. Nezami, Z. Yang, Z. Zhang, Z. Liu, Z. Yang, Z. Han, Z. Zhou, Z. Chen, Z. Chen, Z. Shuai, et al., ""Large-Scale AI in Telecom: Charting the Roadmap for Innovation, Scalability, and Enhanced Digital Experiences,"" arXiv, preprint arXiv:2503.04184 [cs.NI], Mar. 2025. Available: https://arxiv.org/abs/2503.04184","This white paper discusses the role of large-scale AI in the telecommunications industry, with a specific focus on the potential of generative AI to revolutionize network functions and user experiences, especially in the context of 6G systems. It highlights the development and deployment of Large Telecom Models (LTMs), which are tailored AI models designed to address the complex challenges faced by modern telecom networks. The paper covers a wide range of topics, from the architecture and deployment strategies of LTMs to their applications in network management, resource allocation, and optimization. It also explores the regulatory, ethical, and standardization considerations for LTMs, offering insights into their future integration into telecom infrastructure. The goal is to provide a comprehensive roadmap for the adoption of LTMs to enhance scalability, performance, and user-centric innovation in telecom networks."
34,"X. Lin, L. Kundu, C. Dick, M. A. Canaveras Galdon, J. Vamaraju, S. Dutta, and V. Raman, ""A Primer on Generative AI for Telecom: From Theory to Practice,"" arXiv, preprint arXiv:2408.09031 [cs.NI], Aug. 2024. Available: https://arxiv.org/abs/2408.09031","The rise of generative artificial intelligence (GenAI), particularly large language models (LLMs), is transforming the telecom industry by driving innovation, enhancing efficiency, and improving customer service. This paper overviews GenAI theory and practice for telecom, including a review of models, applications, technology enablers, and best practices. It highlights the significance of retrieval augmented generation (RAG) for connecting LLMs with domain-specific telecom data to improve accuracy. The authors present a RAG-based chatbot capable of answering open radio access network (O-RAN) questions, which has attracted significant industry interest and is publicly available on GitHub."
35,"F. Zhu, X. Wang, C. Zhu, and C. Huang, ""Liquid Neural Networks: Next-Generation AI for Telecom from First Principles,"" arXiv, preprint arXiv:2504.02352 [cs.IT], Apr. 2025. Available: https://arxiv.org/abs/2504.02352","Artificial intelligence (AI) is revolutionizing next-generation wireless networks by optimizing performance, data processing, and decision-making through advanced algorithms and machine learning. However, present AI models suffer from robustness issues and lack interpretability, especially in dynamic environments where data distributions change, resulting in degraded performance and raising concerns about safety, transparency, and fairness. To address these challenges, liquid neural networks (LNNs) have been developed with the goal of improving both robustness and interpretability. This paper investigates the prospects of LNNs for telecommunications, explains their operational mechanisms, compares their advantages over conventional neural networks, and explores the opportunities they present for future wireless systems. The authors also discuss design challenges and directions for implementing LNNs and summarize their performance using two illustrative case studies."
36,"C. Chaccour, W. Saad, M. Debbah, and H. V. Poor, ""Joint Sensing, Communication, and AI: A Trifecta for Resilient THz User Experiences,"" IEEE Transactions on Wireless Communications, vol. 23, no. 9, pp. 11444-11460, Sept. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10489861","In this paper, a novel joint framework integrating sensing, communication, and artificial intelligence (AI) is introduced to optimize extended reality (XR) experiences over terahertz (THz) wireless systems. The approach comprises three key components: (1) a tensor decomposition method exploits THz channel sparsity and quasi-opticality to extract unique sensing parameters from uplink communication signals, enabling shared waveform, spectrum, and hardware for both communication and sensing; the Cramer-Rao lower bound is derived to evaluate parameter estimation accuracy. (2) A non-autoregressive multi-resolution generative AI framework with an adversarial transformer is proposed to predict missing and future sensing data, providing robust historical and anticipatory insights adaptable to dynamic and unknown user and environmental conditions. (3) Control of reconfigurable intelligent surface (RIS) subarrays handover policies is achieved via a multi-agent deep recurrent hysteretic Q-neural network, using rich sensing information to minimize handover costs, maximize the quality of personal experiences (QoPEs), and enhance the reliability and resilience of THz links. Simulations demonstrate that the unsupervised generative AI framework maintains high generalizability across user behavior and velocities, yielding a 61% improvement in instantaneous reliability over channel state information-based schemes."
37,"Q. Hou, M. Zorzi, T. Palpanas, M. Rossi, D. Zordan, and D. Reforgiato Recupero, ""Automatic AI Model Selection for Wireless Systems: Online Learning via Digital Twinning,"" IEEE Transactions on Wireless Communications, vol. 24, no. 1, pp. 411–426, Jan. 2025. [Online]. Available: https://ieeexplore.ieee.org/document/10844034","This paper presents a methodology for online optimization of automatic model selection (AMS) mappings in O-RAN wireless network settings, where context-aware AI apps (e.g., for power control) must be rapidly and robustly deployed. Traditional AMS calibration using only real (physical twin, PT) data is slow due to the vast context space; employing a digital twin (DT) to simulate data can accelerate this but introduces bias due to real-to-sim gaps. The authors propose and analyze DT-powered AMS (DT-AMS), which corrects simulator bias by adjusting simulated-data loss estimates using limited real data, and Adaptive DT-AMS (A-DT-AMS), which adaptively balances bias and variance with online-tuned hyperparameters to optimize calibration efficiency. Experimental results for a GNN-based power control app show that while all DT-powered methods converge to the WMMSE benchmark with high-fidelity simulation, correction techniques (DT-AMS, A-DT-AMS) provide much faster and more robust convergence, especially under model misspecification and limited simulation budgets. The main challenges include simulator bias, simulation cost, hyperparameter tuning, and context correlation. This approach substantially reduces online calibration time and data requirements, enabling practical AMS in fast-evolving wireless environments. Future avenues include joint app selection, reduced PT-DT communication, transformer-based AMS, and co-evolving DT context distributions. Key equations include minimizing the expected context loss via
\[
\min_{g}~\mathbb{E}_{c}[\ell_{c}(g(c))]
\]
where $g$ is the AMS mapping, $c$ is context, and $\ell_{c}$ is per-context loss, with DT-AMS correcting simulated loss by estimating and adding bias:
\[
\hat{\ell}_{DT-AMS}(c) = \ell_{DT}(c) + (\ell_{PT}(c) - \ell_{DT}(c))
\]"
38,"A. K. Gizzini, O. Tork, A. Ghazal, S. -E. Elayoubi, and M. Debbah, ""Towards Explainable AI for Channel Estimation in Wireless Communications,"" IEEE Transactions on Wireless Communications, vol. 22, no. 11, pp. 8248–8264, Nov. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10368353/","This paper addresses the core problem of explainability in AI-driven channel estimation for future 6G networks, proposing an explainable AI-based channel estimation (XAI-CHEST) scheme specifically for feed-forward neural network (FNN) estimators in doubly-selective OFDM channels. Traditional deep learning models, like Bayesian neural networks (BNNs) and FNNs, have shown high accuracy and robustness in channel estimation but are hindered by their black-box nature, undermining trust in mission-critical applications. The novel XAI-CHEST framework generates interpretability by systematically perturbing input subcarriers with noise and learning noise masks that classify subcarriers as relevant or irrelevant according to their impact on the utility model's mean squared error (MSE). The custom loss function for training the noise model is given by $L_\mathrm{N} = \min_{\theta_\mathrm{N}} [L_\mathrm{U} - \lambda \log(B)]$, where $L_U$ is the MSE for channel estimation and $B$ are the learned noise weights. Simulations using vehicular channel models (VTV-US and VTI-US) demonstrate that: (i) STA-FNN and TRFI-FNN estimators outperform conventional methods, (ii) using only relevant subcarriers, as identified by the XAI-CHEST noise masks, improves BER by up to 2\,dB for STA-FNN and 1\,dB for TRFI-FNN at $BER = 10^{-4}$, while omitting irrelevant subcarriers does not degrade performance despite them being more numerous, and (iii) relevant features are concentrated at sharp channel variation points. The XAI-CHEST approach therefore not only reveals internal model logic, addressing challenges of trust and transparency, but also enables reduced model input size for lower complexity. The methodology is adaptable to other physical-layer tasks, with future work focusing on threshold optimization, extension to other domains, and pilot/FNN architecture optimization guided by interpretability."
39,"J. Wang, X. Mu, Y. Liu, M. Di Renzo, and J. Wang, ""Interplay Between RIS and AI in Wireless Communications: Fundamentals, Architectures, Applications, and Open Research Problems,"" IEEE Journal on Selected Areas in Communications, vol. 39, no. 7, pp. 1936–1971, July 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9451999/","Future wireless communication networks face significant challenges in meeting the demanding requirements of a highly digital and data-driven society. Two promising technologiesreconfigurable intelligent surfaces (RIS) and artificial intelligence (AI)have garnered attention for their potential to create controllable, intelligent, and programmable wireless environments, especially for sixth-generation (6G) networks. This paper explores how AI-enabled technologies can be integrated into RIS-based frameworks to enhance the practicality of RIS and enable smart radio propagation environments. The discussion spans from fundamental RIS concepts to the current research status, analyzing the inevitability of merging RIS with AI. Special focus is given to recent advances in RIS architectures that incorporate AI, detailing both the intelligent metamaterial structures and AI-embedded RIS-assisted wireless communication systems. The challenges and potential future directions of this interdisciplinary field are also reviewed."
40,"J. Wang, H. Du, J. Zhu, X. Xie, D. Fang, and H. Wang, ""Generative Artificial Intelligence Assisted Wireless Sensing: Human Flow Detection in Practical Communication Environments,"" IEEE Journal on Selected Areas in Communications, vol. 42, no. 10, pp. 2737–2752, Oct. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10557650","This paper introduces G-HFD, a generative artificial intelligence (GAI)-assisted human flow detection system leveraging wireless communication signals for fine-grained detection of human targets and subflows. The system uses channel state information (CSI) to estimate the velocity and acceleration of human-induced reflection (HIR) path length changes; these noisy estimates are denoised using a unified weighted conditional diffusion model (UW-CDM). By employing a uniform linear array (ULA) with wavelength-level antenna spacing, G-HFD applies the UW-CDM to resolve ambiguities in direction of arrival (DoA) spectral estimationespecially challenging when antenna spacing exceeds half a wavelengthenabling accurate identification of both time of flight (ToF) and DoA. Clustering techniques are subsequently used to determine subflow numbers and sizes, i.e., the count of people in each motion subflow. Tested in real-world scenarios (corridors and meeting rooms) with downlink signal data, G-HFD achieves up to 91% accuracy in subflow size detection. This demonstrates the promise of GAIspecifically diffusion modelsfor enhancing wireless sensing and overcoming challenges faced by traditional AI in denoising, parameter estimation, and spectrum ambiguity, and sets the stage for broader uses of GAI in future signal processing and sensing applications."
41,"A. K. Gizzini, V. Labeau, S. Clavier, ""Towards Explainable AI for Channel Estimation in Wireless Communications,"" IEEE Transactions on Vehicular Technology, vol. 73, no. 5, pp. 7389-7394, May 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10368353/","This paper addresses the critical need for explainable AI (XAI) in 6G wireless communication systems, particularly for AI-driven channel estimationa key function in mission-critical applications such as autonomous driving. The authors propose the XAI-CHEST scheme, a novel, model-agnostic, perturbation-based method that provides interpretable insights into feed-forward neural network (FNN) channel estimators used in doubly-selective channels. The scheme relies on training an auxiliary interpretability model which generates a noise mask $B$ (via sigmoid activation) to selectively induce noise on FNN inputs. The model aims to identify relevant inputs by minimizing a customized loss $L_N = \min_{\theta_N} [ L_U - \lambda \log(B)]$, where $L_U$ is the utility model's loss (mean squared error), and $\lambda$ encourages noise on irrelevant features. Simulation results show that, by using only the relevant subcarriers identified by the XAI model, the bit error rate (BER) performance improves (by up to 2 dB at $10^{-4}$ BER for some estimators) and computational complexity drops, compared to using all subcarriers. These results reveal that XAI-CHEST not only unveils the logic behind black-box AI estimatorstransforming them into transparent, white-box modelsbut also guides input optimization for improved performance. Challenges remain in empirically setting noise thresholds and adapting XAI methods developed for other fields to wireless communication. Future work entails refining subcarrier classification, exploring optimal pilot pattern design, and tuning FNN structures for efficiency. Overall, XAI-CHEST offers a practical step towards trustworthy, interpretable AI for next-generation wireless networks."
42,"Y. He, J. Ren, G. Yu, J. Yuan, ""Importance-Aware Data Selection and Resource Allocation in Federated Edge Learning System,"" IEEE Transactions on Vehicular Technology, vol. 69, no. 11, pp. 13593-13605, Nov. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9163301","To address challenges in federated edge learning (FEEL), this paper develops an importance-aware joint data selection and resource allocation scheme. A novel metric is proposed to quantify the importance of data samples, enabling edge devices to select and upload the most critical data for local model updates. Additionally, a resource allocation algorithm is designed to optimally assign resources to each device based on data importance and device capability. Experimental results demonstrate that the proposed approach effectively reduces training latency and enhances learning accuracy compared to conventional algorithms."
43,"A. Kabacı, M. Başaran, H. A. Çırpan, ""Low-Complex AI-Empowered Receiver for Spatial Media-Based Modulation MIMO Systems,"" IEEE Transactions on Vehicular Technology, vol. 73, no. 10, pp. 13276-13288, Oct. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10252036/","Next-generation communication systems are expected to integrate artificial intelligence (AI) techniques into multi-antenna setups to improve system performance. One of the most important use cases of AI in wireless communications is multiple-input multiple-output (MIMO) systems, due to the superior AI capability of learning the best possible decision-making in complex transceiver structures. In this study, we propose a deep neural network (DNN) receiver structure for spatial media-based modulation (SMBM)-MIMO systems, which detects the symbols in an end-to-end manner. Instead of a conventional two-stage approach, which handles channel estimation and symbol detection separately, the proposed DNN receiver recovers the transmitted symbols directly, utilizing an offline training process. It is demonstrated that while the proposed DNN receiver structure and conventional maximum likelihood (ML) receiver utilizing linear minimum mean-square error (LMMSE)-based channel estimation, perform similarly for a single receive antenna case, DNN is superior for multiple receive antenna cases. We conclude that using DNNs in SMBM with multi-antenna receivers can provide higher performance and thus permits higher data transmission rates in addition to the reduced receiver complexity due to the removing channel estimation process."
44,"Thai-Hoc Vu, Senthil Kumar Jagatheesaperumal, Minh-Duong Nguyen, Nguyen Van Huynh, Sunghwan Kim, and Quoc-Viet Pham, ""Applications of Generative AI (GAI) for Mobile and Wireless Networking: A Survey,"" accepted in IEEE Internet of Things Journal, 2024. [Online]. Available: https://arxiv.org/abs/2405.20024","This paper provides a comprehensive tutorial and survey on the integration of Generative Artificial Intelligence (GAI)notably models like VAEs, GANs, Diffusion Models, and multi-modal/meta-learning architecturesinto mobile and wireless networking for the evolving Internet-of-Things (IoT) landscape. It reviews the evolution and core mechanisms of GAI, then systematically analyzes its state-of-the-art applications in network management, wireless security, and semantic communication, presenting summarized findings in compact tables with technical details such as methods, models, limitations, and outcomes. Key applications include GAI-enabled SDN, network slicing, resource allocation, channel estimation, privacy-preserving and adversarial security systems, knowledge abstraction, and distributed learning frameworkseach with tabulated lessons learned. The discussion highlights critical open challenges, including high model complexity, scalability in mobile/IoT deployments, privacy/trust/security issues, and lack of standardization, presented with suggested future directions and a detailed tabular taxonomy of open problems and potential solutions. The conclusion emphasizes both the opportunities and unresolved barriers for GAI adoption in mobile networking, calling for lightweight, distributed, and secure GAI architectures, as well as standardized operation and benchmarking procedures to accelerate research and practical deployment."
45,"Ijaz Ahmad, Shahriar Shahabuddin, Tanesh Kumar, Erkki Harjula, Marcus Meisel, Markku Juntti, Thilo Sauter, and Mika Ylianttila, ""Challenges of AI in Wireless Networks for IoT,"" arXiv preprint arXiv:2007.04705, 2020. [Online]. Available: https://arxiv.org/abs/2007.04705","The Internet of Things (IoT), first introduced by Ashton, refers to the extension of network connectivity to physical devices such as actuators, sensors, and mobile devices, allowing them to interact, communicate, and be monitored or controlled remotely. Regarded as the catalyst for the next industrial revolution, IoT is expected to fundamentally change the way we perceive and interact with the physical systems around us, already demonstrating significant impact across various domains including health care, smart homes, manufacturing, commerce, and education."
46,"Medhat Elsayed and Melike Erol-Kantarci, ""AI-enabled Future Wireless Networks: Challenges, Opportunities and Open Issues,"" arXiv preprint arXiv:2103.04536, 2021. [Online]. Available: https://arxiv.org/abs/2103.04536","A plethora of demanding services and use cases mandate a revolutionary shift in the management of future wireless network resources. Indeed, when tight quality of service demands of applications are combined with increased complexity of the network, legacy network management routines will become unfeasible in 6G. Artificial Intelligence (AI) is emerging as a fundamental enabler to orchestrate the network resources from bottom to top. AI-enabled radio access and AI-enabled core will open up new opportunities for automated configuration of 6G. On the other hand, there are many challenges in AI-enabled networks that need to be addressed. Long convergence time, memory complexity, and complex behaviour of machine learning algorithms under uncertainty as well as highly dynamic channel, traffic and mobility conditions of the network contribute to the challenges. In this paper, we survey the state-of-art research in utilizing machine learning techniques in improving the performance of wireless networks. In addition, we identify challenges and open issues to provide a roadmap for the researchers."
47,"W. Shi, M. He, H. Wu, and X. Shen, ""Split Learning Over Wireless Networks: Parallel Design and Resource Management,"" IEEE Journal on Selected Areas in Communications, vol. 41, no. 4, pp. 1051–1066, Apr. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10040976","Split learning (SL) enables collaborative AI model training between devices and edge servers by splitting models at a cut layer, but standard SL suffers from high training latency due to sequential processing across many devices. This paper introduces Cluster-based Parallel SL (CPSL), which first partitions devices into clusters for parallel device-side training and aggregation, then sequentially trains the full model across clusters to reduce latency. Additionally, the authors propose a resource management algorithm that minimizes CPSLs training latency under device heterogeneity and wireless network dynamics by stochastically optimizing cut layer selection (on a large timescale), device clustering, and radio spectrum allocation (on a smaller timescale). Simulation results on non-i.i.d. data show that this two-timescale optimization approach substantially reduces training latency compared to current SL methods and adapts to network changes."
48,"X. Lyu, et al., “Objective-driven Differentiable Optimization of Traffic …,” IEEE Transactions on Machine Learning in Communications and Networking, vol. ?, no. ?, pp. ?, 2024. [Online]. Available: https://ieeexplore.ieee.org/iel8/9882533/10070384/10646623.pdf","The paper introduces an objective-driven, end-to-end differentiable optimization framework designed to jointly optimize traffic prediction and resource allocation for split AI inference tasks in edge networks. Leveraging neural networks for future traffic estimation, the framework integrates these predictions with resource allocation mechanismssuch as computation and bandwidth assignmentto minimize latency or maximize service quality. The optimization problem is formulated with differentiable surrogate loss functions and soft constraints, enabling gradient-based training even in the presence of complex, real-world constraints. Empirical evaluations on both synthetic and real network traces demonstrate significant latency reductions and improved resource utilization compared to conventional, decoupled methods, with rapid adaptation to non-stationary traffic patterns. The approach is highly flexible, supporting custom objectives and scalable to larger deployments, though it relies on quality traffic prediction and sufficient training data. The authors acknowledge challenges including the coupling of prediction and allocation under differentiability constraints, and robust performance in dynamic settings. Future work targets extensions to multi-hop networks, reinforcement learning integration, and security/privacy enhancements. Overall, this study provides a principled, adaptive methodology for optimizing split AI inference in practical edge intelligence applications."
49,"K. B. Letaief, Y. Shi, J. Lu, J. Lu, and S. Sun, ""Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications,"" IEEE Journal on Selected Areas in Communications, vol. 39, no. 12, pp. 3335–3374, Dec. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9606720","The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from ""connected things"" to ""connected intelligence"". However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems."
