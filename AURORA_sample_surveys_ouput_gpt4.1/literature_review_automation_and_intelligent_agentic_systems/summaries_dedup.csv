Index,Citation,Summary
1,"B. Das, M. Majumder, S. Phadikar, and S. A. Ahmed, ""Automatic question generation and answer assessment: a survey,"" Research and Practice in Technology Enhanced Learning, vol. 16, no. 5, pp. 1-42, 2021. [Online]. Available: https://rptel.apsce.net/index.php/RPTEL/article/download/2021-16005/29/62","This paper surveys recent advances in automatic question generation (QG) and answer assessment methodologies for online learning environments, emphasizing their importance for scalable and effective learner evaluation. It outlines that QG can automate the creation of objective (e.g., multiple-choice, cloze, gap-fill) and subjective (e.g., short/long answer) questions from both textual and pictorial learning resources. The authors methodically review 78 articles spanning rule-based, NLP, and machine learning techniques for QG, as well as semantic similarity, vector space, and deep learning methods for answer assessment. Key datasets such as SQuAD, MS MARCO, RACE, NewsQA, TriviaQA, and LearningQ are discussed in terms of their relevance and limitations for automated assessment. The survey highlights major challenges: reliable sentence selection for QG, coherent question formulation from larger text units, accurate automatic grading of written responses, lack of standardized assessment frameworks, and a research gap in multimedia (audio, video) QG/assessment. Progress in deep learning and NLP is noted as promising, though the field still requires robust systems for subjective and multimedia QG. The paper concludes by proposing future research directions, including robust sentence extraction, multimodal QG from video/audio sources, and standardization of answer assessment methodologies, ultimately stressing that while MCQs are prevalent due to scoring convenience, they are insufficient alone for comprehensive assessment of reasoning and writing skills."
2,"D. Pugh, M. D. O'Reilly, and C. Jasper, ""Can automated item generation be used to develop high-quality multiple-choice questions for medical education? A comparative study,"" Research and Practice in Technology Enhanced Learning, vol. 15, no. 12, pp. 1-22, 2020. [Online]. Available: https://rptel.apsce.net/index.php/RPTEL/article/download/2020-15012/68/141","This study compared the quality of multiple choice questions (MCQs) developed through automated item generation (AIG) versus traditional methods, with blinded expert raters evaluating 102 MCQs (51 from each method) on six quality metrics and cognitive domain (recall vs application). Results indicated no statistically significant differences between AIG and traditional items on any quality metric, as shown by Wilcoxon two-sample tests with Holm-Bonferroni correction. Both item types overwhelmingly assessed higher-order skills (AIG: 96.1%, traditional: 91.2%), and inter-judge agreement was high. The findings demonstrate that AIG produces MCQs of comparable quality to traditional methods, capable of assessing higher-order cognitive skills, provided the cognitive model and author training are robust. The results support the use of AIG for generating high-quality assessment items, with implications for enhancing exam item efficiency and mitigating challenges like item overuse or sharing. Future research should investigate whether AIG benefits less-experienced item writers in producing higher-order or high-quality items."
3,"R. Nakamoto, A. Iwasawa, N. Takahashi, and R. Oka, ""Unsupervised techniques for generating a standard explanatory sentence for self-explanatory mathematics,"" Research and Practice in Technology Enhanced Learning, vol. 19, no. 16, pp. 1-15, 2024. [Online]. Available: https://rptel.apsce.net/index.php/RPTEL/article/download/2024-19016/2024-19016","Mathematical self-explanation tasks, which prompt learners to articulate the validity of each derivation step, foster deeper understanding but are burdensome for teachers to prepare due to the need for detailed, stepwise explanations. This paper presents an unsupervised approach to automate the generation of standard explanatory sentences for math derivations by labeling each step with a knowledge component (KC)such as ""distributive law""and mapping these to natural language explanation templates. The methodology includes unsupervised clustering and summarization of self-explanatory sentences from student data or open-access textbooks to extract KCs and build a KC-to-template database; new derivation steps are matched to KCs using pattern matching and heuristic formula analysis, then combined with templates to generate fluent explanations. Evaluation on high-school math quizzes demonstrates that over 90% of derivation steps can be automatically covered with substantive, human-readable explanations, with accuracy exceeding 85% where KCs are well matched. Key challenges include handling multi-rule or ambiguous steps and ensuring template variety and contextual adaptation, while broader future work involves integrating advanced NLP methods, leveraging large educational corpora, and transferring the approach to other STEM domains. The system significantly reduces the time required by teachers for preparing self-explanation references and is shown to generalize to other procedural solution domains involving formulas and derivations."
4,"T. Steuer, A. Filighera, T. Tregel, and A. Miede, “Educational Automatic Question Generation Improves Reading Comprehension in Non-native Speakers: A Learner-Centric Case Study,” Frontiers in Artificial Intelligence, vol. 5, 2022, Art. no. 900304. Available: https://www.frontiersin.org/articles/10.3389/frai.2022.900304/full","This study investigates whether automatically generated adjunct questions (AQG) can enhance learning outcomes in reading comprehension, addressing the gap where manually authored questions are often unavailable. In a quasi-experimental design involving 48 college students using science texts, participants exposed to AQG showed significantly higher related posttest scores ($M = 0.76$ for treatment vs. $M = 0.62$ for control, $p = 0.049$) without a significant decrease in unrelated content retention ($p = 0.40$), indicating no definitive adverse effects. Treatment and control groups were balanced for language skills and prior knowledge, but time-on-task was higher for the AQG group. Notably, even questions recognized as computer-generated improved learning, suggesting full human-like quality is not required for educational benefit. These findings reinforce that AQG can effectively scaffold learners during reading, especially where traditional question resources are lacking. Limitations include the small sample size, online delivery, and focus on definition-heavy texts; future work aims to generalize AQG by refining content selection and understanding what makes text passages suitable for question generation."
5,"C. R. Guerra-Tamez, K. Kraul Flores, G. M. Serna-Mendiburu, D. Chavelas Robles, and J. Ibarra Cortés, “Decoding Gen Z: AI's influence on brand trust and purchasing behavior,” Frontiers in Artificial Intelligence, vol. 7, 2024, Art. no. 1323512. Available: https://www.frontiersin.org/articles/10.3389/frai.2024.1323512/full","This study investigates how Artificial Intelligence (AI) influences Generation Z's consumer behaviors in fashion, technology, beauty, and education sectors, utilizing survey data from 224 Mexican university students aged 1826. Key constructsAI exposure, attitude toward AI, and perceived AI accuracywere measured using validated 7-point Likert scales and analyzed via confirmatory factor analysis and structural equation modeling. Results reveal these AI-related variables significantly enhance brand trust, which in turn positively influences purchasing decisions; notably, ""flow experience"" partially mediates the path from brand trust to purchase intent. The sample predominantly consisted of women (67.1%), students (88.8%), and digital natives with medium to high tech-savviness, with most participants purchasing fashion and technology products online. The model exhibits strong reliability and validity ($NFI=0.931$, $SRMR=0.071$, $GoF=0.49$), and all hypothesized relationships are statistically significant. The study acknowledges its scope limitation (single cohort, single country, self-reported data) and advocates for broader, longitudinal, and cross-cultural research. Overall, findings emphasize the centrality of AI-driven trust in shaping Gen Zs digital purchasing, suggesting marketers should leverage AI enhancements and transparent practices to build strong brand relationships in emerging consumer landscapes."
6,"M. Esposito, S. Sarbazvatan, T. Tse, and G. Silva-Atencio, “The use of artificial intelligence for automatic analysis and reporting of software defects,” Frontiers in Artificial Intelligence, vol. 7, 2024, Art. no. 1443956. Available: https://www.frontiersin.org/articles/10.3389/frai.2024.1443956/full","This study explores the integration of artificial intelligence (AI) into software defect detection, motivated by the increased demand for efficient, cost-effective, and high-quality software solutions in the post-COVID-19 era. Employing a qualitative, exploratory, and descriptive methodology based on a documentary review of 79 bibliometric sources, the research identifies regression testing and automated log files within machine learning (ML) and robotic process automation (RPA) environments as key techniques that optimize the detection of software failures. Notably, the term ""artificial intelligence"" appeared in 95% of searches, but only 0.0014% included combinations with automation testing and software defects, highlighting a research gap. The analysis stresses that AI, particularly when harnessed through regression testing $($which leverages known outputs from prior system versions$)$, can automate defect analysis and reporting, reduce manual effort, and improve agility, though its broader application faces challenges such as the need for reliable ""oracle"" mechanisms and standardized methodologies. Recommendations focus on robust data collection and validation, effective model training (supervised or unsupervised), systematic documentation of test results, and defined protocols for root cause analysis. The study concludes that organizations adopting AI-driven models in software quality assurance can reap significant efficiency gains and cost reductions, despite the need for standardized industry methodologies."
7,"M. Imran and N. Almusharraf, ""Google Gemini as a next generation AI educational tool: a review of emerging educational technology,"" Smart Learning Environments, vol. 11, Art. no. 22, 2024. [Online]. Available: https://slejournal.springeropen.com/articles/10.1186/s40561-024-00310-z","This report surveys Google Gemini, a multimodal generative AI developed by DeepMind and designed to process and generate content across text, image, audio, video, and PDF modalities, making it uniquely versatile for educational technology applications. Gemini is available in three versionsNano, Pro, and Ultrato cater to varying user demands, from efficient on-device use to the most advanced AI capabilities. Unlike traditional unimodal AI models, Gemini handles diverse input types, performs complex reasoning, and adapts communication styles for personalized and engaging learning experiences. It excels at interpreting and generating multimodal content, offering tailored feedback and dynamic assessment, and assisting both teachers and learners with content generation, differentiation, visualization, and multilingual support. Its generative AI engine allows up-to-date, context-sensitive responses accessed through Google Search, while strong performance on benchmarks such as MMLU underscores its problem-solving power, for instance, reasoning over 100,000 lines of code. However, challenges remain including the lack of ethical guidelines for deployment in education, difficulties with complex visual scenarios, inconsistent scoring, potential biases, and risks surrounding data privacy, reliability, and academic integrity. Rigorous monitoring, ethical safeguards, and transparent practices are recommended to ensure equitable and responsible integration. If thoughtfully implemented, Google Gemini has the potential to transform personalized learning experiences by combining its multimodal reasoning and generation strengths with a human-centered, ethical approach."
8,"H. Ferreira, G. P. de Oliveira, R. Araújo, F. Dorça, and R. Cattelan, ""Technology-enhanced assessment visualization for smart learning environments,"" Smart Learning Environments, vol. 6, Art. no. 14, 2019. [Online]. Available: https://slejournal.springeropen.com/articles/10.1186/s40561-019-0096-z","In Smart Learning Environments, the OSM-V Open Student Model with Information Visualization was developed to enhance self-regulated learning and enable both students and instructors to monitor academic performance. Integrated into the Classroom eXperience platforma multimedia-enabled smart classroom systemOSM-V combines probabilistic modeling (using Bayesian Networks) and semantic inference (using ontologies and SWRL rules) to provide adaptive, customizable feedback about student progress, strengths, and weaknesses. Its architecture comprises Probabilistic, Semantic, Activity Management, and Visualization modules, exchanging data via JSON, and calculates knowledge with the formula $K = \frac{M}{Q-1} (Q-N)$, where $K$ is the knowledge probability, $M$ is the maximum probability, $Q$ is the number of quiz alternatives, and $N$ the attempt in which the student succeeded, with $K$ always ranging from 0.1 to 0.9. Two empirical studies demonstrated that OSM-V improves students perception of utility and changes study behaviors, with longer study sessions correlating to higher grades (Kruskal-Wallis test: $H(2)=7.063, p<0.05$). While best suited to factual or conceptual knowledge, and having been tested mainly with tech students, OSM-Vs results endorse the models potential benefit in adapting educational content and supporting both students and instructors, with future work aimed at broader deployment and enhanced visualization techniques."
9,"C.-C. Lin, A. Y. Q. Huang, and O. H. T. Lu, ""Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review,"" Smart Learning Environments, vol. 10, Art. no. 41, 2023. [Online]. Available: https://slejournal.springeropen.com/articles/10.1186/s40561-023-00260-y","Sustainable education is vital for ensuring a future that balances environmental, social, and economic needs, but it faces challenges such as insufficient infrastructure, unequal access, limited resources, lack of teacher ICT skills, engagement issues in virtual settings, and concerns over privacy, bias, and transparency. This review applies the PRISMA framework and draws on qualitative research to systematically analyze literature around the integration of Artificial Intelligence (AI) and Information Technology (IT) in education, identifying 8 key sustainable education challenges and reviewing 29 papers on AI-driven Intelligent Tutoring Systems (ITS) and Technology-Enhanced Learning (TEL). AI can enable personalized learning by adapting to individual student needs and providing teachers with actionable, data-driven insights into student performance and engagement, but it also brings issues such as data privacy and bias, demanding robust infrastructure and thoughtful design. The literature highlights technologys proven benefits in subject-specific and general educational contexts (such as flipped classrooms for mathematics and chemistry), yet emphasizes the need for equitable access and human-centered, interpretable AI solutions (XAI, HCAI). Future research should focus on robust solution architectures, portability, improved sample spaces for machine learning, and maintaining fairness and transparency. The paper concludes that achieving sustainable, equitable education through AI depends not just on technological solutions, but also ongoing research addressing pedagogical, ethical, and infrastructure gaps, thereby maximizing the positive educational transformation for diverse learner populations. Key abbreviations include AI, ITS, ICT, TEL, XAI, and PRISMA."
10,"Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang, ""AutoSurvey: Large Language Models Can Automatically Write Surveys,"" arXiv preprint arXiv:2406.10252 [cs.IR], 2024. Available: https://arxiv.org/abs/2406.10252","AutoSurvey is an automated methodology leveraging large language models (LLMs) to efficiently generate comprehensive literature surveys in rapidly evolving fields like AI. Traditional surveys are challenged by the overwhelming volume of new research, as illustrated by over 4,000 LLM-related arXiv submissions in four months of 2024, making efficient literature synthesis essential. AutoSurvey proceeds through four phases: embedding-based retrieval and outline generation, parallel LLM-driven subsection drafting with attentive citation, section integration and refinement, and rigorous multi-LLM and human-calibrated evaluation using metrics such as citation recall/precision and content coverage/structure/relevance. Experimentally, AutoSurvey produces 73.59 surveys/hour for 64k tokens (vs. 0.07 for humans), with citation recall/precision of 82.25%/77.41% and content metrics approaching human levels (e.g., coverage 4.66/5), and strong alignment ($\rho$ up to 0.5429) between LLM and human evaluations. Key innovations include hierarchical outline-driven generation, reflective refinement, and robust citation checking. However, challenges such as citation misalignments (misinterpretation, overgeneralizationmost common at 51%), context and knowledge window limitations, and reliance on arXiv data persist, and all content is meant for reference, not direct replacement of expert synthesis. The method scales to other domains, with future work focusing on better benchmarks, knowledge retrieval, and integrative citation rationalization. Generating a 32k token survey costs between \$0.89 (Claude) and \$33.48 (GPT-4), and societal/ethical considerations recommend careful use of outputs as reference material."
11,"Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, and Zhiyu Li, ""SurveyX: Academic Survey Automation via Large Language Models,"" arXiv preprint arXiv:2502.14776 [cs.CL], 2025. Available: https://arxiv.org/abs/2502.14776","Large Language Models (LLMs) have potential for automating survey generation but face notable hurdles, including limited context windows, reliance on outdated internal knowledge, and weak reference management. SurveyX introduces a two-phase pipeline inspired by human authorship, with a Preparation Phase for advanced reference retrieval (offline arXiv and online Google Scholar), using keyword expansion and a two-step filter (embedding + LLM) and a novel AttributeTree to structure extracted reference data, forming a RAG-ready knowledge base. In the Generation Phase, SurveyX sequentially builds survey outlines (aided by LLMs and guided hints) and generates content for each section, followed by post-processing that enriches citations and incorporates figures/tables generated programmatically or extracted multimodally. Experiments over 20 LLM-focused academic topics show SurveyX closely approaches human-written survey quality, outperforming strong baselines (AutoSurvey, Naive RAG) across content quality (score: 4.590 vs human 4.754), citation F1 (81.52%), and reference relevance metrics. Ablations show each major system componentadvanced retrieval, AttributeTree structuring, outline optimization, RAG-based rewritingsignificantly boosts quality. SurveyX thus sets a new state-of-the-art for automated survey generation, with ongoing plans to further enhance retrieval, visual elements, and organizational structure via the AttributeTree framework."
12,"Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, and Lei Bai, ""SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing,"" arXiv preprint arXiv:2503.04629 [cs.CL], 2025. Available: https://arxiv.org/abs/2503.04629","SurveyForge is an automated framework designed to generate high-quality survey papers, addressing the shortcomings of existing Large Language Model (LLM)-based systems in outline coherence, reference relevance, and content quality. It operates in two main stages: a heuristic outline generation phase, which leverages domain-specific research and survey outline databases for structured, semantically rich outlines, and a memory-driven content writing phase, where a scholar navigation agent retrieves high-quality literature using advanced retrieval and reranking strategies. Evaluation using the SurveyBench benchmarkcomprising 100 human-written surveys across 10 topicsdemonstrates that SurveyForge substantially outperforms previous systems like AutoSurvey on reference coverage (improving from 0.23 to 0.40, for example), outline quality (82.25 to 86.58), and content quality (73.87 to 76.34), as reinforced by both automated metrics and human expert assessments. Cost analysis reveals the system's efficiency, producing a 64k-token survey for under \$0.50 in about 10 minutes. Despite these advancements, challenges persist in deep knowledge synthesis and citation accuracy, motivating future work to enhance cross-publication reasoning and knowledge association through citation graphs and core entity extraction. SurveyForge thus sets a new standard in automated survey writing, potentially lowering the barrier for researchers entering new domains and supporting cross-disciplinary scholarship."
13,"S. Hu, C. Lu, and J. Clune, ""Automated Design of Agentic Systems,"" arXiv preprint arXiv:2408.08435 [cs.AI], 2025. [Online]. Available: https://arxiv.org/abs/2408.08435","Researchers are exploring the Automated Design of Agentic Systems (ADAS), aiming to automate the creation of powerful agentic systems by inventing novel building blocks or combining existing ones in innovative ways. This area leverages the idea that, just as in past eras of machine learning, hand-crafted solutions may be outperformed and eventually replaced by those devised through automated, learning-driven processes. The paper introduces a novel approach within ADAS whereby agents are defined and generated in code, with a meta agent autonomously programming improved agents using the expressive power of Turing Complete languagesenabling, in principle, the automatic discovery of any agentic system, including new prompts, tool integrations, and workflows. The authors present Meta Agent Search, an algorithm in which a meta agent continually develops new agents by building on an expanding archive of prior discoveries. Extensive experiments in domains such as coding, science, and math reveal that this method invents agents with novel designs that significantly outperform the best manually crafted counterparts, and these improvements generalize robustly across tasks and model architectures. The findings highlight the promise and generality of ADAS, suggesting a compelling new direction for safely automating the discovery of effective agentic systems to advance human objectives."
14,"F. Bousetouane, ""Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents,"" arXiv preprint arXiv:2501.00881 [cs.MA], 2025. [Online]. Available: https://arxiv.org/abs/2501.00881","This paper analyzes the transformative role of agentic systemsAI architectures built around Large Language Models (LLMs)in addressing the evolving, complex demands faced by modern industries. Highlighting the limitations of traditional SaaS platforms, which emphasize generalizability over domain specialization, and context-aware systems that adapt to dynamic inputs but still rely on rule-based automation, the paper introduces vertical AI agents as a solution offering domain-expert intelligence, real-time adaptability, and end-to-end workflow automation. The proposed framework decomposes agentic systems into core modules, including Memory, Reasoning Engine, Tools, and a novel Cognitive Skills module that embeds domain-specific, compliance, and ethical reasoning. Architectures such as RAG Agent Routers, orchestrated multi-agent systems, and human-in-the-loop approaches are presented with successful use cases spanning e-commerce, healthcare, finance, legal, and logistics, demonstrating improvements in automation, accuracy, compliance, and adaptability. The discussion underscores challenges such as lack of standardization, the need for ongoing domain expertise, ethical concerns, and robust human-agent collaboration. The conclusion envisions agentic systems facilitating unprecedented scalability, operational efficiency, and responsible AI integration, signifying a paradigm shift in how intelligent workflows and automation are designed and deployed across industries."
15,"E. Miehling, K. Natesan Ramamurthy, K. R. Varshney, M. Riemer, D. Bouneffouf, J. T. Richards, A. Dhurandhar, E. M. Daly, M. Hind, P. Sattigeri, D. Wei, A. Rawat, J. Gajcin, and W. Geyer, ""Agentic AI Needs a Systems Theory,"" arXiv preprint arXiv:2503.00237 [cs.AI], 2025. [Online]. Available: https://arxiv.org/abs/2503.00237","This paper argues that advancing the reasoning capabilities and agency of AI requires a holistic, systems-theoretic perspectivemoving beyond a narrow focus on individual model performance to understand emergent behaviors resulting from agent, human, and environmental interactions. The authors define functional agency as the capacity for goal-directed action, outcome modeling, and behavioral adaptation, placing agents (LLMs, multimodal models, humans, etc.) along a spectrum by their degrees of agency. They identify key mechanisms through which agentic systems gain emergent cognitive abilities: embodied and multimodal interaction with the environment enhances cognition; predictive modeling under uncertainty enables causal reasoning; and metacognitive awareness arises through feedback, reflection, and social (agent-agent) interaction. Drawing on interdisciplinary literature, the paper theorizes that advanced capabilitiessuch as generalization, causal reasoning, and self-monitoringcan emerge when simpler agents interact within carefully structured networks, but warns that such behaviors also amplify risks like deception, self-exfiltration, and subgoal misalignment. Open challenges include determining the right balance between pretraining and environment-driven learning, engineering trustworthy and efficient agent-agent/human-agent interactions, safely governing autonomous subgoal creation, and designing robust escalation paths for ambiguous or risky situations. The authors advocate for purposeful, systems-level designgrounded in principles from biology, organizational theory, and cognitive scienceto deliberately shape the capabilities and safe operation of increasingly autonomous and multimodal agentic AI."
16,"K. Sowa and A. Przegalinska, ""From Expert Systems to Generative Artificial Experts: A New Concept for Human-AI Collaboration in Knowledge Work,"" Journal of Artificial Intelligence Research, vol. 82, 2025. [Online]. Available: https://doi.org/10.1613/jair.1.17175","This paper introduces the concept of Generative Artificial Experts (GAEs), a new class of generative AI agents explicitly designed to enhance human-AI collaboration in knowledge work by embodying specialized domain expertise, bounded autonomy, synthetic personas, and multimodal generative capabilities. The paper offers a structured definition of GAEs by outlining seven core traits and developing a taxonomy that distinguishes them from traditional generative AI systems. Using a literature-based conceptual analysis and abductive reasoning, the authors propose GAEs as an evolution from classic expert systems, facilitated by advancements in both human-AI collaboration research and generative AI technology. While technical implementation details are not discussed due to the papers conceptual focus, illustrative scenarios are provided to showcase the future potential and applications of GAEs in knowledge-intensive environments, emphasizing their emerging role despite the current lack of fully realized examples."
17,"D. Dell'Anna, N. Alechina, F. Dalpiaz, M. Dastani, and B. Logan, ""Data-Driven Revision of Conditional Norms in Multi-Agent Systems,"" Journal of Artificial Intelligence Research, vol. 75, pp. 1377–1418, 2022. [Online]. Available: https://doi.org/10.1613/jair.1.13683","In multi-agent systems, norm enforcement is a mechanism for steering the behavior of individual agents in order to achieve desired system-level objectives. Due to the dynamics of multi-agent systems, however, it is hard to design norms that guarantee the achievement of the objectives in every operating context. Also, these objectives may change over time, thereby making previously defined norms ineffective. In this paper, we investigate the use of system execution data to automatically synthesise and revise conditional prohibitions with deadlines, a type of norms aimed at prohibiting agents from exhibiting certain patterns of behaviors. We propose DDNR (Data-Driven Norm Revision), a data-driven approach to norm revision that synthesises revised norms with respect to a data set of traces describing the behavior of the agents in the system. We evaluate DDNR using a state-of-the-art, off-the-shelf urban traffic simulator. The results show that DDNR synthesises revised norms that are significantly more accurate than the original norms in distinguishing adequate and inadequate behaviors for the achievement of the system-level objectives."
18,"J. Fu, A. Tacchetti, J. Perolat, and Y. Bachrach, ""Evaluating Strategic Structures in Multi-Agent Inverse Reinforcement Learning,"" Journal of Artificial Intelligence Research, vol. 71, pp. 953–993, 2021. [Online]. Available: https://doi.org/10.1613/jair.1.12594","A core issue in multi-agent systems is inferring agents' motivations from their observed behaviors; inverse reinforcement learning (IRL) serves as a tool for extracting utility functions that rationalize these actions. This work demonstrates an efficient approach to scaling IRL for multi-agent scenarios by decomposing the multi-agent learning challenge into $N$ single-agent problems while preserving essential rationality properties, including strong rationality. The authors identify that naively learned rewards may lack meaningful structure, leading to poor generalization in games featuring new players. The paper further analyzes conditions for identifiability of rewards or utility functions across settings like normal-form games, Markov games, and auctions, illustrating the capability to learn generalizable reward functions for novel environments."
19,"Archana Rani, Naresh Grover, N. Deepa, and C. Prajitha, ""A smart agent-based approach for privacy preservation and threat mitigation to enhance security in the Internet of Medical Things,"" Journal of Autonomous Intelligence, vol. 7, no. 5, Article ID 1629, pp. 1-17, 2024. [Online]. Available: https://jai.front-sci.com/index.php/jai/article/view/1629","Integrating medical sensors and IoT in smart healthcare has enabled the Internet of Medical Things (IoMT), empowering detection and severity assessment of participants conditions, but exposing sensitive health data to privacy risks due to the limited storage and computational abilities of local devices. To address these challenges, the paper proposes a Smart Agent-based Privacy Preservation and Threat Mitigation Framework (SAPPTMF), which employs intelligent agents within a comprehensive IoMT system model to bolster security, includes an attacker model for simulating threats, and utilizes an analytic hierarchy process (AHP) to weight security needs. The framework achieves strong performance, reporting accuracy of $94.5\%$, precision $91.0\%$, recall $93.4\%$, F-score $92.4\%$, and mean squared error (MSE) of $0.09$."
20,"Ahmed Srhir, Tomader Mazri, and Manale Boughanja, ""Smart parking: Multi-agent approach, architecture, and workflow,"" Journal of Autonomous Intelligence, vol. 7, no. 4, Article ID 1376, pp. 1-16, 2024. [Online]. Available: https://jai.front-sci.com/index.php/jai/article/view/1376","Inefficient parking management in urban areas leads to congestion, wasted time and fuel, increased emissions, and economic losses. Smart parking systems leverage IoT sensors to provide real-time data on parking availability, optimizing space usage, improving traffic flow, and enhancing the driver experience. Integrated into urban infrastructure, these systems reduce the search time for parking, mitigate congestion, lower greenhouse gas emissions, and present new revenue opportunities for operators. This paper proposes a multi-agent IoT-based smart parking approach, offering an architectural framework and workflow suitable for diverse scenarios. The methodology combines real-time monitoring, traffic management, and digital user services to increase efficiency and sustainability, minimizing reliance on manual operations and promoting eco-friendly urban environments."
21,"K. Lannelongue, M. de Milly, R. Marcucci, S. Selevarangame, A. Supizet, and A. Grincourt, ""Compositional grounded language for agent communication in reinforcement learning environment,"" Journal of Autonomous Intelligence, vol. 2, no. 1, pp. 22-44, 2019. [Online]. Available: https://jai.front-sci.com/index.php/jai/article/view/56","In the evolving landscape of technology for scientific, economic, and social purposes, significant advances in Artificial Intelligence (AI) and the Internet of Things (IoT) have occurred, emphasizing the importance of human-machine interactions and automation. While machines excel at inter-device communication, enabling them to interact meaningfully with humans remains a challenge, particularly with conversational technologies like chatbots that often produce irrelevant responses, especially when the topic shifts. This paper addresses the challenge by investigating the creation of a communication language from scratch, allowing machines to construct a new exchange channel optimized for their interactions. By equipping each machine with sound-emitting capabilities linked to goal completion and analyzing the convergence toward a shared 'language', the study draws parallels to the development of human languages. Constraints were imposed to ensure the emergent language is both grounded in meaning and compositionally structured, closely mirroring key properties of human languages. This approach led to a rapidly converging evolution of syntactic communication among machines, paving the way for meaningful machine-to-machine language development."
22,"Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang, ""A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges,"" Vicinagearth, vol. 1, Article no. 9, pp. 1–57, 2024. [Online]. Available: https://doi.org/10.1007/s44336-024-00009-2","This paper presents a comprehensive survey of large language model (LLM)-based multi-agent systems (MAS), highlighting their potential for creating intelligent and credible autonomous systems akin to human society. The authors propose a unified framework with five core componentsprofile, perception, self-action, mutual interaction, and evolutionthat encapsulates the workflow in LLM-based MAS. The survey systematically organizes existing research around system construction, detailing how agents are instantiated with unique profiles, perceive environments (potentially via multi-modal input), deploy reasoning and planning, communicate using structures such as hierarchical or decentralized messaging, and evolve through methods like fine-tuning, reinforcement learning, and self-reflection. Applications span problem-solving (software engineering, robotics, scientific debates, industry) and world simulation (virtual societies, economics, gaming, recommendation, epidemiology), with each module exemplified through case studies and reviews. Key challenges include LLMs intrinsic limitations (black box effect, bias, hallucination), risks of misuse, difficulties in scaling (resource constraints, agent coordination), and adaptation to dynamic, multi-modal, and real-time environments. The discussion addresses mitigation strategiessuch as integrating external knowledge, fact-checking, adversarial training, and improved interpretability. The conclusion underscores that, although achieving fully reliable, autonomous MAS remains distant, advances in LLM-based agents, especially in coordinated collective intelligence and scalable, transparent deployment (Agents-as-a-Service), represent significant progress. Future directions advocate expanding into multidisciplinary domains, improving coordination, and tackling security, privacy, and adaptability."
23,"Anetta Jedličková, ""Ethical approaches in designing autonomous and intelligent systems: a comprehensive survey towards responsible development,"" AI & Society, 2024, Open Forum. [Online]. Available: https://doi.org/10.1007/s00146-024-02040-9","Over the past decade, rapid advances in artificial intelligence (AI) have led to widespread adoption and increasingly complex, opaque systems, raising ethical concernsparticularly around transparency, accountability, responsibility gaps, and societal harms. This survey paper synthesizes literature, ethical frameworks (e.g., EU, IEEE, OECD), and practical case studies to underscore major ethical challenges: bias, privacy risks, accountability for autonomous actions, and the gap between ethical ideals and technical realities. It details the principal ethical approaches (e.g., beneficence, autonomy, justice, explicability), explores leading guidelines for trustworthy AI, and highlights practical mechanisms such as transparency, risk assessments, stakeholder engagement, and independent audits. The discussion emphasizes ambiguities in principles, the pace of technological change, and difficulties in operationalizing ethics in complex contexts, advocating for interdisciplinary collaboration, regulatory clarity, technical advancements in explainability and fairness, organizational culture change, and robust impact assessments. Concluding, the paper stresses that AI must prioritize fundamental rights, well-being metrics, and robust ethical oversight throughout its lifecyclerequiring continuous refinement of methodologies and tools. Trustworthy AI systems must exemplify reliability, safety, security, resilience, accountability, transparency, explainability, and respect for privacy, with ethical requirements carefully balanced to serve both human rights and societal well-being."
24,"Herman Veluwenkamp and Stefan Buijsman, ""Design for operator contestability: control over autonomous systems by introducing defeaters,"" AI and Ethics, Published 07 Feb. 2025. [Online]. Available: https://doi.org/10.1007/s43681-025-00657-0","This paper introduces ""Operator Contestability"" as a crucial design principle for AI systems, emphasizing that operatorsthose tasked with overseeing and interacting with AImust possess both sufficient epistemic access and the authority to challenge AI-driven decisions. Leveraging the ""Design for Defeaters"" (DfD) framework, the authors differentiate between undercutting defeaters (which contest the justification for AI's data inputs) and rebutting defeaters (which contest the validity of AI's outputs), advocating for their systematic integration into AI systems. Through conceptual analysis and detailed case studies (such as AI recruitment platforms and autonomous vehicles), they illustrate that current designs often limit operator contestability due to system opacity, lack of real-time awareness, authority constraints, and cognitive limitations (such as automation bias). The results show that embedding tools like explainability methods, confidence scoring, and driver monitoring can enhance operators' capacity to intervene effectively, especially when paired with institutional reforms. The authors argue that failing to ensure operator contestability results not in responsibility gaps, but in unfulfilled indirect responsibilitiesweakening accountability and public trust. They conclude that integrating technical safeguards with organizational and legal changes is essential for socio-technical systems, ensuring operators are empowered to detect, interrogate, and rectify errors, especially in high-stakes, real-time contexts."
25,"N. V. Blamah, A. A. Oluyinka, G. Wajiga, and Y. B. Baha, ""MAPSOFT: A Multi-Agent based Particle Swarm Optimization Framework for Travelling Salesman Problem,"" Journal of Intelligent Systems, vol. 30, no. 1, pp. 413-428, Dec. 2020. Available: https://www.degruyterbrill.com/document/doi/10.1515/jisys-2020-0080/html","This paper introduces MAPSOFT, a Multi-Agent based Particle Swarm Optimization (PSO) framework tailored for the Traveling Salesman Problem (TSP). The system models PSO within a multi-agent system (MAS) context, granting each agent cognitive autonomy and enabling interactions via belief-desire-intention (BDI) reasoning. Key innovations include discrete PSO adaptation for TSP, dynamic k-dimensional neighborhoods, and a strategy where agents retrospectively select neighborhoods based on both present and historical fitness values. The algorithm initializes a swarm, where agents iteratively plan, choose neighborhoods, evaluate personal and local bests, and update positions using classic PSO dynamics: $v_{i}(t+1) = w \cdot v_{i}(t) + c_1 \cdot rand1 \cdot (pbest_i - x_i(t)) + c_2 \cdot rand2 \cdot (lbest_i - x_i(t))$, with $C_1=1.7$, $C_2=1.7$, and $w=0.715$. Simulations on TSP instances (10 agents over 10 cities, 50 iterations) show that agents boost global search effectiveness by switching neighborhoods based on sustained superior fitness, stored in personal belief databases. The approach enhances intelligence, autonomy, and solution quality compared to conventional PSO, although it incurs extra overhead for managing distributed knowledge. Future work will seek to reduce this overhead, improve robustness in neighborhood exploration, and benchmark MAPSOFT in real-world scenarios against other algorithms."
26,"S. Gong, ""Transition from machine intelligence to knowledge intelligence: A multi-agent simulation approach to technology transfer,"" Journal of Intelligent Systems, vol. 34, no. 1, Article 20230320, Jul. 2024. Available: https://www.degruyterbrill.com/document/doi/10.1515/jisys-2023-0320/html","This study addresses the limitations of traditional machine intelligence in autonomous driving by integrating knowledge intelligence through multi-agent simulation. Using real-world data from OpenStreetMap, researchers built a virtual environment where an inception model extracted features from camera images, and knowledge from traffic regulations, road signs, and accident data was represented through knowledge graphs and first-order logic. The autonomous agents employed a Deep Q-Network (DQN) for decision-making, informed by a reward mechanism that balanced safety, efficiency, and compliance. Experiments across 13 simulated environments demonstrated that knowledge-fusion agents achieved lower accident rates (1.4 vs. 3 per 100,000 km), fewer violations (1.8 vs. 4.3 per 100,000 km), and superior perception metrics (accuracy: 96.8% vs. 87.2%, F1: 97.9% vs. 86.9%) compared to traditional approaches. Driving efficiency and GPU utilization were also improved, with average GPU usage at 75.9% and peak at 96.1%. The findings underscore that transitioning from machine intelligence to knowledge intelligence empowers autonomous driving systems with enhanced reasoning, perception, and efficient resource use. The study notes its focus is limited to autonomous vehicles and advocates future research on expanding knowledge fusion to broader multi-agent domains for improved systemic decision-making."
27,"S. Ariffin Kashinath, S. A. Mostafa, D. Lim, A. Mustapha, H. Hafit, and R. Darman, ""A general framework of multiple coordinative data fusion modules for real-time and heterogeneous data sources,"" Journal of Intelligent Systems, vol. 30, no. 1, pp. 947-965, Aug. 2021. Available: https://www.degruyterbrill.com/document/doi/10.1515/jisys-2021-0120/html","This paper presents the Multiple Coordinative Data Fusion Modules (MCDFM) framework designed for distributed, real-time, and heterogeneous data environments where efficient, coordinative decision-making is crucial. The MCDFM framework consists of preprocessing, filtering, and decision phases at each nodeemploying methods like data cleaning, windowing, the extended Kalman filter (EKF) for nonlinear estimation, fuzzy logic for local decisions, and software agents for coordinative (network-wide) decisions. Through a traffic light control (TLC) system case study involving three intersections in Kuala Lumpur, the framework processes traffic data (e.g., speed, density, count) and identifies three congestion periods: nonpeak with congestion degree $0.178$ (variance $0.061$), medium peak at $0.588$ (variance $0.0593$), and peak at $0.796$ (variance $0.0296$). The MCDFM facilitates both micro (local/intersection) and macro (network/collective) perspectives, enabling adaptive strategies like dynamic signal timing and incident detection. While limitations include a lack of real-world deployment and scope for broader domain testing, the MCDFM demonstrates robust resource sharing, information exchange, and real-time responsiveness, promising applicability across diverse distributed systems. Future directions involve practical implementation and integration of advanced methodologies in each processing phase."
28,"B. Tóth, L. Berek, L. Gulácsi, M. Péntek, and Z. Zrubka, “Automation of systematic reviews of biomedical literature: a scoping review of studies indexed in PubMed,” Systematic Reviews, vol. 13, no. 1, Article no. 174, 2024. [Online]. Available: https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-024-02592-3","This paper comprehensively reviews the landscape of systematic review (SR) automation studies indexed in PubMed as of November 2022, analyzing 123 included articlescomprising 108 studies on SR automation methods (SSAMs) and 15 automated SRs (ASRs). Automation was most commonly applied to record screening (72.4%), followed by search (15.4%), data extraction (10.6%), risk of bias assessment (7.3%), and less frequently to full-text selection, evidence synthesis, quality assessment, and reporting. Multiple SR stages were automated in 8.9% of studies, with mean time and workload savings reported but with large variability across tasks and topics. The performance of automation tools, such as support vector machines, naive Bayes, logistic regression, and neural networks, varied considerably, with savings in screening and extraction time documented (e.g., 454 to 1440 seconds saved per study for data extraction). Despite notable technological progress, real-world adoption remains limited, as most tools focus on single SR stages and offer only modest overall time savings. Challenges include inconsistent terminology, reporting standards, and heterogeneity in metrics, complicating direct assessment of utility. The authors emphasize the need for standardized reporting and evaluation metrics and suggest that, while automation can complement manual review processes and sometimes even increase SR sensitivity, full substitution is not yet practical. They advocate for parallel use of automation and human reviewers, accumulation of real-world evidence, and clearer reporting practices to foster broader and more effective adoption of SR automation techniques."
29,"K. E. K. Chai, R. L. J. Lines, D. F. Gucciardi, and L. Ng, “Research Screener: a machine learning tool to semi-automate abstract screening for systematic reviews,” Systematic Reviews, vol. 10, Article no. 93, 2021. [Online]. Available: https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-021-01635-3","Systematic reviews and meta-analyses require rigorous, time-consuming screening of thousands of abstracts, typically taking analysts an average of 33 days and incurring significant financial costs (about USD 141,194 per review). Research Screener, a semi-automated, cloud-hosted tool leveraging deep learning and natural language processing for text embeddings, facilitates this process by iteratively ranking and presenting articles for review, thereby reducing manual workload. Across nine systematic and two scoping reviews, this tool saved 6096% of screening work, with a mean of 79.3% for systematic and 6062% for scoping reviews; in one real-world use case, it reduced screening from 19 to 4 days ($-12.53$ FTE days, USD 2444 saved). Sensitivity analyses showed that screening just 12.8% of articles (or, conservatively, 50%) suffices to recover all eligible studies, and Work Saved over Random Sampling (WSS) at 95% and 100% recall was above 88%. These results confirm that Research Screener can reduce the systematic review burden without diminishing methodological rigour, although wider validation and improved thresholding strategies are suggested for future work, with current limitations including generalizability and potential reviewer complacency due to early identification of relevant articles. Overall, the tool represents a promising advance toward integrating trustworthy machine learning automation in systematic review workflows."
30,"Y. Zhang, S. Liang, Y. Feng, Q. Wang, F. Sun, S. Chen, Y. Yang, X. He, H. Zhu, and H. Pan, “Automation of literature screening using machine learning in medical evidence synthesis: a diagnostic test accuracy systematic review protocol,” Systematic Reviews, vol. 11, Article no. 11, 2022. [Online]. Available: https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-021-01881-5","This protocol outlines a systematic review aiming to evaluate and summarize existing artificial intelligence (AI) algorithms for automatic literature screening in the context of medical systematic reviews. The researchers will use comprehensive searches across major biomedical and computer science databases (PubMed, Embase, ACM Digital Library, IEEE Xplore, and supplementary sources) to identify studies from 2000 onwards that implement AI for literature screening, with human investigators as the reference standard. Two reviewers will independently conduct literature screening and data extraction, resolving disagreements through discussion. Extracted data will include study characteristics, details of training and validation sets, AI algorithm functions, and performance metrics such as sensitivity, specificity, precision, NPV, PPV, NLR, PLR, diagnostic odds ratio (DOR), F-measure, accuracy, and area under the curve (AUC). Risk of bias and evidence level will be assessed using a revised QUADAS-2 framework and GRADE, respectively. The anticipated outcome is a comprehensive summary of the development, effectiveness, and limitations of current AI-based screening systems, with subgroup analyses planned to explore factors affecting diagnostic accuracy and efficiency. The protocol addresses existing challenges, including high variability of algorithm accuracy, low specificity, and the need for comprehensive literature identification, with the ultimate goal of guiding future improvement and adoption of AI tools for evidence synthesis in medicine. No results are yet available, as this is a protocol paper; systematic review registration is PROSPERO CRD42020170815."
31,"K. Cowie, A. Rahmatullah, N. Hardy, K. Holub, K. Kallmes, ""Web-Based Software Tools for Systematic Literature Review in Medicine: Systematic Search and Feature Analysis,"" JMIR Medical Informatics, vol. 10, no. 5, e33219, 2022. DOI: 10.2196/33219. Available: https://medinform.jmir.org/2022/5/e33219","This paper presents a comprehensive, binary feature-by-feature analysis of 24 web-based systematic review (SR) tools, selected via systematic searches of SR Toolbox, prior reviews, and Google, focused on tools functional as of August 2021 and requiring no user coding. From an original pool of 53, tools were excluded if nonfunctional, desktop-only, specialized for non-health evidence, or insufficiently documented. Thirty features spanning retrieval, appraisal, synthesis, output, administration, and support (e.g., direct search, dual screening, extraction, living updates, export options) were assessed as present or absent, with interobserver agreement at 86.5%. The tools Giotto Compliance, DistillerSR, and Nested Knowledge supported the highest proportion of features ($\geq87\%$). Table summarizing top features (selected):

\[
\begin{tabular}{lcccc}
\textbf{Tool} & \textbf{Features (\%)} & \textbf{Search} & \textbf{Dual Extraction} & \textbf{Living Review} \\
\hline
Giotto Compliance & 90 & Yes & Yes & Yes \\
DistillerSR & 87 & Yes & Yes & Yes \\
Nested Knowledge & 87 & Yes & Yes & Yes \\
EPPI-Reviewer Web & 83 & Yes & Yes & Yes \\
\end{tabular}
\]

Only 42% of assessed tools support direct search, 29% dual extraction, and 54% support living reviews; 63% address the full SR process, with strengths in collaboration and screening, but gaps in automation (outside of screening), extraction, and updatability. The authors encourage ongoing independent updates to this landscape, more nuanced feature/quality assessments, and note their potential conflict of interest as developers of one reviewed tool. The study aims to help researchers select suitable SR software and highlight areas for further development, especially regarding support for living reviews and automation beyond screening."
32,"E. Orel, I. Ciglenecki, A. Thiabaud, A. Temerev, A. Calmy, O. Keiser, A. Merzouki, ""An Automated Literature Review Tool (LiteRev) for Streamlining and Accelerating Research Using Natural Language Processing and Machine Learning: Descriptive Performance Evaluation Study,"" Journal of Medical Internet Research, vol. 25, e39736, 2023. DOI: 10.2196/39736. Available: https://www.jmir.org/2023/1/e39736/","LiteRev is an automation tool designed to streamline and accelerate literature reviews (LRs) by leveraging recent advancements in natural language processing (NLP) and machine learning (ML). Unlike traditional LRs, which are laborious and prone to becoming rapidly outdated, LiteRev employs automated queries across eight open-access databases, retrieves comprehensive metadata, and processes text using advanced NLP libraries. Papers are converted into a TF-IDF matrix and subjected to dimensionality reduction via PaCMAP and clustering using HDBSCAN. Hyperparameters are optimized with the Tree-structured Parzen Estimator to maximize the DBCV score, repeating until a threshold (0.5) is achieved; cluster interpretability is bolstered by extracting key topic-defining words. LiteRev enables users to focus searches on abstracts or full texts, refine topics, and iteratively identify relevant papers using a k-nearest neighbors (k-NN) module, with user feedback informing further cycles. In a case study on acute and early HIV infection in sub-Saharan Africa, LiteRev processed 631 unique PubMed papers, identified 16 topical clusters, and, using 18 key papers, suggested 193 papers for screening, correctly detecting 73.6% of relevant abstracts and 87.5% of relevant full texts compared to manual results, saving 56% of screening workload. Processing is rapid (5 minutes for text preparation plus ~50 hours for optimization, which can be parallelized), and LiteRev's outputs (lists of papers, CSV/HTML topic overviews) facilitate rapid evidence mapping. LiteRev remains limited to open-access content and digital PDFs, lacks a web-based interface and collaborative features, and is complementary to but not a replacement for human-led systematic LRs. Planned improvements include a web application, enhanced multi-user functions, and continuous ""living reviews,"" aiming to further automate and expedite evidence synthesis as the volume of academic literature grows. Relevant data and code are openly available for further research and validation."
33,"E. Guo, M. Gupta, J. Deng, Y.-J. Park, M. Paget, C. Naugler, ""Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study,"" Journal of Medical Internet Research, vol. 26, e48996, 2024. DOI: 10.2196/48996. Available: https://www.jmir.org/2024/1/e48996","This study evaluates the use of OpenAI's GPT and GPT-4 APIs for automating the labor-intensive process of screening titles and abstracts in clinical review papers. Using a novel workflow, a Python script prompts GPT models with inclusion and exclusion criteria to classify over 24,000 abstracts from six real-world clinical review datasets, previously labeled by two independent human reviewers. Key results include an overall accuracy of $0.91$, a macro F1-score of $0.60$, sensitivity for excluded papers at $0.91$, and included papers at $0.76$. The interrater agreement between human screeners was $\kappa=0.46$, while agreement between the GPT models and consensus human decisions reached a prevalence and bias-adjusted $\kappa=0.96$. GPT models could also provide reasoning for and reflect upon incorrect classifications, demonstrating potential for enhanced transparency. Although the models exhibit high specificity in excluding irrelevant papers, their lower sensitivity for inclusion suggests they should augment rather than replace human decision-making. The findings underscore large language models' ability to streamline the review process, save time and resources, and improve consistency, though further refinement is needed to enhance inclusivity and generalizability to diverse clinical topics. Future directions include incorporating additional metadata, fine-tuning with generator-discriminator models, and expanding LLM applications in medical literature analysis."
34,"K. Kolaski, L. Romeiser Logan, and J.P.A. Ioannidis, ""Guidance to best tools and practices for systematic reviews,"" JBI Evidence Synthesis, vol. 21, no. 9, pp. 1699–1731, Sep. 2023. DOI: 10.11124/JBIES-23-00139. Available: https://journals.lww.com/jbisrir/fulltext/2023/09000/guidance_to_best_tools_and_practices_for.2.aspx","This paper critically examines persistent methodological deficiencies in systematic reviews and evidence syntheses, noting that despite progress in appraisal tools and reporting standards such as PRISMA, AMSTAR-2, and GRADE, many reviews remain flawed, biased, or uninformative due to inconsistent and superficial use of these methodologies. It highlights that evidence syntheses are foundational to evidence-based medicine but are vulnerable to errors from poor data, choice of inappropriate tools, and lack of critical appraisal skills. The authors detail guidance from leading organizations (e.g., Cochrane, JBI, NICE, WHO), reporting that Cochrane reviews show higher methodological quality but represent only a fraction of published syntheses. They discuss classification challenges for study designs, emphasize the distinction between reporting and methodological tools, and provide a Concise Guide (Table 6.3) outlining best practices and tool selection for various review types, summarized via a LaTeX table structure for clarity. The article underscores widespread misuse of tools (e.g., mistaking PRISMA compliance for methodological rigor), highlights the necessity of training, and calls attention to challenges including language bias, insufficient critical appraisal, and the effort required to remain updated. The authors advocate for ongoing innovation, better education, integration of technologies like machine learning, and collaborative evolution of methods to ensure evidence syntheses are trustworthy supports for policy and clinical practice, warning that mere checklist completion does not substitute for methodological competence."
35,"Z. Munn, ""Software to support the systematic review process: the Joanna Briggs Institute System for the Unified Management, Assessment and Review of Information (JBI-SUMARI),"" JBI Evidence Synthesis, vol. 14, no. 10, p. 1, Oct. 2016. DOI: 10.11124/JBISRIR-2016-002421. Available: https://journals.lww.com/jbisrir/fulltext/2016/10000/software_to_support_the_systematic_review_process_.1.aspx","Systematic reviews are fundamental to evidence-based healthcare, and in the past two decades their prevalence has grown due to advocacy from organizations like the Joanna Briggs Institute (JBI) and Cochrane. Recognizing the complexity and lengthy timeline of conducting systematic reviews, JBI pioneered software support, initially with the Qualitative Assessment and Review Instrument in 2001, and later with JBI-SUMARI, which facilitated various review types including effectiveness, qualitative, and economic evaluations. The challenge during redevelopment of SUMARI was to further expand its capabilities to cover an even broader range of evidence sources, resulting in a global first for systematic review software. The newest version, marking JBI's 20th anniversary, is being continuously updated to support up to ten different review types, encompassing effectiveness, qualitative, economics, epidemiology, diagnostic accuracy, etiology, text and opinion, mixed methods, umbrella, and scoping reviews. The authors encourage researchers interested in systematic reviews to utilize this new SUMARI software to enhance their process."
36,"N. F. Ali, M. M. Mohtasim, S. Mosharrof, and T. G. Krishna, ""Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation,"" arXiv preprint arXiv:2411.18583, 2024. [Online]. Available: https://arxiv.org/abs/2411.18583","This research presents and compares several methods for automating the generation of literature reviews using Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). Aiming to address the overwhelming volume of research articles, the study develops systems that generate literature reviews from PDF files by evaluating three approaches: a frequency-based method using spaCy, a transformer model (Simple T5), and RAG with GPT-3.5-turbo, utilizing the SciTLDR dataset. The effectiveness of each method is measured using ROUGE scores, where the GPT-3.5-turbo LLM achieves the highest ROUGE-1 score of 0.364, outperforming both the transformer and spaCy-based models. Subsequently, a graphical user interface is created for the LLM-based system, which proved most effective."
37,"G. Sundaram and D. Berleant, ""Automating Systematic Literature Reviews with Natural Language Processing and Text Mining: a Systematic Literature Review,"" arXiv preprint arXiv:2211.15397, 2023. [Online]. Available: https://arxiv.org/abs/2211.15397","Objectives: An SLR is presented focusing on text mining based automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained. Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed. Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research. Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews."
38,"J. de la Torre-López, A. Ramírez, and J. R. Romero, ""Artificial intelligence to automate the systematic review of scientific literature,"" Computing, vol. 105, pp. 2171–2194, 2023. [Online]. Available: https://arxiv.org/abs/2401.10917; https://doi.org/10.1007/s00607-023-01181-x","This paper surveys artificial intelligence (AI) techniques proposed in the last 15 years to automate systematic literature review (SLR) processes, which are traditionally laborious and error-prone due to the high volume of scientific publications. The authors outline a detailed methodology involving both automated and manual searches across multiple databases, resulting in 34 primary studies analyzed for AI support in SLR tasks. The survey finds that machine learning (particularly SVMs with bag-of-words and TF-IDF) dominates the automation of the paper selection phase, with active learning approaches enabling efficient human-AI collaboration and major reductions in manual screening timesometimes saving upwards of 80 hours or reducing screening effort by 60%. However, AI applications for planning (e.g., search strategy clustering, process mining) and reporting phases (e.g., summarization via BioBERT+LSTM) are less developed. Few tools are designed for non-AI experts or provide transparent, interpretable outputs, creating a usability gap. Challenges include over-reliance on general-purpose classifiers, underutilization of metadata/citation/context, limited reproducibility due to scarce open tools or datasets, and insufficient studies in domains beyond medicine or computing. The authors recommend future research to expand AI support for planning and reporting, enhance feature engineering and interpretability (explainable AI), create user-friendly interfaces for general researchers, and foster open science with shared datasets and benchmarks. Overall, while AI has demonstrated substantial time- and cost-savings in automating repetitive SLR tasks, especially paper selection, full automation of the SLR workflow and more robust, interpretable, and domain-generalizable systems remain as key directions for future work."
39,"Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko, ""Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods,"" Journal of Artificial Intelligence Research, vol. 79, pp. 1-52, 2023. [Online]. Available: https://jair.org/index.php/jair/article/view/14438","Large language models (LLMs) have advanced such that distinguishing between AI-generated and human-written text has become extremely challenging, yet remains crucial for trust, especially in contexts like fraud detection, academic honesty, and combating misinformation. This comprehensive survey reviews the state-of-the-art in AI-generated text (AIGT) detection, covering methods such as watermarking (white/black-box, cryptographic, neural), statistical analysis (perplexity, entropy, DetectGPT, ensembles), stylistic analysis (lexical, syntactic, semantic), language model-based classification (fine-tuned LMs, adversarial, ensemble), off-the-shelf tools, and human detectors. The survey synthesizes the factors impacting detectionmodel size (larger models produce harder-to-detect text), decoding strategy, text length, language/domain generalizability, human influence, mixed authorship, and adversarial evasion. Notably, even the best detection techniques degrade when facing large LLMs, adversarial attacks, or mixed human/AI content. Practical recommendations emphasize ensemble and mixed-method approaches, high-quality and realistic training data, and ongoing adaptation to evolving LLMs and evasion tactics. The authors provide tabular summaries of datasets and commercial/evasion tools. Key research challenges involve combining NLP with behavioral and social cues, ensuring fairness, robustness to fine-tuned and unknown LLMs, explainability, and addressing legal and ethical concerns. Ultimately, while reliable AIGT detection is essential for a trustworthy information ecosystem, no perfect solution exists, and continued interdisciplinary innovation and vigilance is required as generative AI capabilities and countermeasures rapidly evolve."
40,"Tammy Zhong, Yang Song, Raynaldio Limarga, Maurice Pagnucco, ""Computational Machine Ethics: A Survey,"" Journal of Artificial Intelligence Research, vol. 77, pp. 795–841, 2023. [Online]. Available: https://jair.org/index.php/jair/article/view/14302","Computational Machine Ethics (CME) is an interdisciplinary field that integrates moral philosophy into an agent's decision-making process, contributing to the development of artificial agents capable of making ethical decisions. The survey addresses the landscape, frameworks, and challenges associated with CME."
41,"Giorgio Franceschelli, Mirco Musolesi, ""Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges,"" Journal of Artificial Intelligence Research, vol. 78, pp. 859–899, 2023. [Online]. Available: https://jair.org/index.php/jair/article/view/14369","This survey discusses the intersection of reinforcement learning (RL) and generative artificial intelligence (AI), presenting a comprehensive analysis of state-of-the-art developments, opportunities, and open research challenges. Generative models aim to learn a probability distribution $P_{model}$ that mimics the true data distribution $P_{data}$, with deep learning architectures such as VAEs, GANs, transformers, and diffusion models commonly employed. RL offers advantages for training generative models, particularly in sequential tasks and when objectives are non-differentiable or not easily specified as loss functions. The paper categorizes RL applications for generative AI into: (1) mere generation, where RL replicates self-supervised approaches but can leverage non-differentiable signals; (2) objective maximization, enabling optimization for explicit, possibly non-differentiable metrics (e.g., BLEU, ROUGE) across domains like text, music, and molecules; and (3) alignment with non-easily quantifiable characteristics, where RL from human feedback (RLHF) aligns models with human preferences despite challenges in reward modeling. Key challenges include balancing exploration and exploitationespecially with large action spacesdefining suitable reward functions, high computational costs, and vulnerability to issues like reward hacking and Goodhart's Law. The authors identify future directions such as better integration of inverse RL, multi-agent RL, improved robustness (preventing ""jailbreaks""), and advances in reward modeling and human preference alignment. RL's flexibility to use arbitrary reward signals positions it as a promising but challenging approach for enhancing generative AI systems."
42,"J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, ""Graph neural networks: A review of methods and applications,"" AI Open, vol. 1, pp. 57–81, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651020300016","Lots of learning tasks require dealing with graph data containing rich relational information among elements, such as modeling physical systems, learning molecular fingerprints, predicting protein interfaces, and classifying diseases, all of which demand models capable of learning from graph inputs. In areas with non-structural data like text and images, reasoning over extracted structures (e.g., sentence dependency trees, image scene graphs) is an active research area that also necessitates graph reasoning models. Graph neural networks (GNNs) address these needs by capturing dependencies through message passing between graph nodes, with recent variants including graph convolutional networks (GCN), graph attention networks (GAT), and graph recurrent networks (GRN) achieving state-of-the-art results in various deep learning tasks. This survey proposes a general design pipeline for GNNs, reviews the variants of each pipeline component, systematically categorizes applications, and identifies four open research problems."
43,"T. Lin, Y. Wang, X. Liu, and X. Qiu, ""A survey of transformers,"" AI Open, vol. 3, pp. 111–132, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651022000163","Transformers have achieved major breakthroughs across domains such as natural language processing, computer vision, and audio processing, prompting significant academic and industrial interest. Despite the proliferation of numerous Transformer variants (collectively called X-formers), there has not been a systematic and comprehensive literature review of these models. This survey addresses that gap by thoroughly reviewing the landscape of X-formers: it starts with a concise overview of the vanilla Transformer, then introduces a new taxonomy for organizing X-formers. The survey categorizes and describes the variants from three main perspectivesarchitectural modifications, pre-training strategies, and application areasand concludes by highlighting promising directions for future research."
44,"B. Li, Y. Hou, and W. Che, ""Data augmentation approaches in natural language processing: A survey,"" AI Open, vol. 3, pp. 106–110, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651022000102","As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges. Some helpful resources are provided in the appendix."
45,"C. Dalvi, M. Rathod, S. Patil, S. Gite, and K. Kotecha, ""A Survey of AI-Based Facial Emotion Recognition: Features, ML & DL Techniques, Age-Wise Datasets and Future Directions,"" IEEE Access, vol. 9, pp. 165806–165840, 2021. DOI: 10.1109/ACCESS.2021.3137226. [Online]. Available: https://ieeexplore.ieee.org/document/9631205","Facial Expression Recognition (FER) serves as a crucial AI research area due to its role in interpreting nonverbal human communication, with applications spanning dynamic analysis, pattern recognition, mental health monitoring, and more. The surge in online interactions during the Covid-19 pandemic has highlighted the need for advanced FER frameworks capable of processing visual data from diverse demographicsincluding children, adults, and seniorssince emotional expressions differ by age. Despite extensive research, there remains a gap in comprehensive literature reviews detailing existing FER methodologies, datasets, feature extraction methods, algorithms, and breakthrough applications. This paper addresses that need by providing a thorough evaluation of AI-based FER techniques, uniquely focusing on age-related expression differences and outlining future research directions with the goal of significantly impacting the research community."
46,"E. Mendez Guzman, V. Schlegel, and R. Batista-Navarro, ""From outputs to insights: a survey of rationalization approaches for explainable text classification,"" Frontiers in Artificial Intelligence, vol. 7, Article 1363531, 2024. Available: https://www.frontiersin.org/articles/10.3389/frai.2024.1363531/full","Deep learning has driven significant advances in text classification but at the cost of model interpretability, especially problematic in high-stakes domains. This survey reviews rationalization approaches, which use human-understandable language rationales to explain model predictions. It contrasts extractive methodsselecting evidence from input text for transparency and reliabilitywith abstractive methods, which generate more flexible, human-like explanations but risk unfaithful outputs. Recent work favors multi-task learning models for extractive rationalization, though their need for extensive annotated data and careful tuning is acknowledged. The field has seen rapid growth since 2016, the development of numerous rationale-annotated datasets (though with limited standardization and diversity), and proxy-based evaluation metrics focusing on faithfulness, plausibility, comprehensiveness, sufficiency, and robustness. Key challenges include dataset scarcity in critical areas, scalability of human annotation, and the need for evaluation frameworks that better serve a wider array of stakeholders. Promising directions for future research include diversifying datasets, refining annotation standards, improving metrics to reflect end-user perspectives, mitigating abstraction model hallucinations, and exploring multimodal and dynamic rationalization strategies. Ultimately, tailoring the choice between extractive and abstractive methods to application requirements can improve trust and usability of NLP systems, yet more work is needed to ground, generate, and evaluate rationales effectively in real-world contexts."
47,"S. Gurrapu, A. Kulkarni, L. Huang, I. Lourentzou, and F. A. Batarseh, ""Rationalization for explainable NLP: a survey,"" Frontiers in Artificial Intelligence, vol. 6, Article 1225093, 2023. Available: https://www.frontiersin.org/articles/10.3389/frai.2023.1225093/full","Recent advances in deep learning have greatly improved NLP task performance but at the expense of model explainability, posing significant challenges in domains like healthcare and finance where transparency is vital. Traditional explainability techniques such as LIME and SHAP, while effective, often require specialized expertise, making them less accessible to non-experts. Rationalizationa technique that generates natural language rationales to justify model outputshas emerged as a more intuitive and human-comprehensible solution to this problem. This survey provides the first comprehensive overview of rationalization in NLP from 20072022, detailing its conceptual foundations, the methodology for literature curation, and an exhaustive analysis of extractive and abstractive rationalization methods across varied NLP tasks (e.g., machine reading comprehension, sentiment analysis, fact-checking, and neural machine translation). It highlights challenges such as the lack of standardized evaluation protocols, limited dataset diversity, and underdeveloped human-centered assessment practices, and proposes priorities for future research including standardized metrics (e.g., BLEU, ROUGE), increased dataset diversity, expanded interdisciplinary application, and rigorous human assurance frameworks. The survey introduces Rational AI as a new subfield within XAI, aiming to formalize and expand rationalization research to make AI decision-making both interpretable and complete, ultimately fostering greater trust and accessibility for both experts and non-experts."
48,"M. Afzaal, J. Nouri, A. Zia, P. Papapetrou, U. Fors, Y. Wu, X. Li, and R. Weegar, ""Explainable AI for data-driven feedback and intelligent action recommendations to support students self-regulation,"" Frontiers in Artificial Intelligence, vol. 4, Article 723447, 2021. Available: https://www.frontiersin.org/articles/10.3389/frai.2021.723447/full","This paper introduces a novel approach that leverages learning analytics and explainable machine learning to deliver automated, intelligent, and actionable formative feedback to students, supporting their self-regulation and boosting academic performance. The method utilizes LMS data to predict assignment-level outcomes using various machine learning models (with random forest achieving the highest accuracy, $0.75$$0.91$), explains predictions with LIME, and identifies actionable factors such as engagement, grades, and concept expertise. A dashboard was developed to present personalized feedback and action recommendations; evaluations in a programming course revealed that following these recommendations correlated strongly with exam performance ($r = 0.86$, $p < 0.0001$) and dashboard users showed statistically significant improvement in outcomes ($t = 6.688$, $p = 0.0001$). Student and expert feedback confirmed the dashboard enhanced motivation and self-regulation. Limitations include restriction to a programming course, small sample size, and the challenges of local explainability; future directions include expanding to larger datasets, enhancing dashboard features, and introducing global explainability. The approach is adaptable beyond the tested domain and shows promise for scaling within educational settings."
49,"A. Yehudai, L. Eden, A. Li, G. Uziel, Y. Zhao, R. Bar-Haim, A. Cohan, and M. Shmueli-Scheuer, ""Survey on Evaluation of LLM-based Agents,"" arXiv preprint arXiv:2503.16416, Mar. 2025. [Online]. Available: https://arxiv.org/abs/2503.16416","This paper surveys evaluation methodologies for large language model (LLM)-based agents, which are autonomous systems capable of planning, reasoning, tool use, and memory in dynamic environments. The authors systematically analyze benchmarks and frameworks along four axes: (1) core agent abilities such as planning, tool use, self-reflection, and memory; (2) application-specific benchmarks spanning web, software engineering, scientific, and conversational domains; (3) generalist agent evaluations; and (4) evaluation frameworks. The analysis uncovers trends toward more realistic and continuously updated evaluations and highlights critical gapsincluding the need for assessment of cost-efficiency, safety, robustness, and for fine-grained, scalable evaluation methods. The survey maps the state of agent evaluation, outlines field trends, current limitations, and suggests future research directions."
50,"J. Prather, J. Leinonen, N. Kiesler, J. G. Benario, S. Lau, S. MacNeil, N. Norouzi, S. Opel, V. Pettit, L. Porter, B. N. Reeves, J. Savelka, D. H. Smith IV, S. Strickroth, and D. Zingaro, ""Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools,"" arXiv preprint arXiv:2412.14732, Dec. 2024, to appear in Proceedings of the 2024 Working Group Reports on Innovation and Technology in Computer Science Education (ITiCSE-WGR 2024). [Online]. Available: https://arxiv.org/abs/2412.14732","Generative AI (GenAI) is rapidly transforming computing education, eliciting a wide range of responses from initial anxiety to high optimism about its potential. The reviewed literature indicates GenAI tools can now effectively solve most introductory programming tasks, disrupting established curricula and enabling new instructional methods, such as personalized feedback and explanation generation akin to a teaching assistant. Since 2024, research has focused on GenAIs classroom integration, including large-scale instructional support, direct programming assistance, and the emergence of tools for teaching both programming and prompting skills. This report synthesizes findings from a systematic literature review, educator and industry surveys, and interviews with tool creators, researchers, and educators involved in GenAI-augmented instruction. Key research questions addressed include how instructors use GenAI for teaching and support (e.g., grading, chatbots like CS50, feedback), the variety and application of new GenAI tools, evolving policies and motivations, impacts on desired competencies and student outcomes, industry adoption, and the future trajectory of computing education with GenAI. Triangulation of qualitative and quantitative evidence advances understanding of GenAI usage, perceptions, and its implications during this pivotal period for the computing education community."
51,"L. Buess, M. Keicher, N. Navab, A. Maier, and S. Tayebi Arasteh, ""From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine,"" arXiv preprint arXiv:2502.09242, Feb. 2025. [Online]. Available: https://arxiv.org/abs/2502.09242","Generative artificial intelligence (AI) models, including diffusion models and systems like OpenAI's ChatGPT, are rapidly advancing the field of medicine by bolstering diagnostic precision and automating a variety of clinical tasks. The technology has evolved from unimodal, text-only large language models supporting clinical documentation and decision-making to sophisticated multimodal AI models that can process and integrate data from sources such as imaging, text, and structured records. This scoping review systematically examines the evolution, methodologies, applications, dataset resources, and real-world evaluations of multimodal AI in clinical settings, guided by PRISMA-ScR standards and prioritizing recent literature through 2024 from major databases. From 144 rigorously reviewed studies, the paper illuminates a major paradigm shift toward multimodal AI that significantly advances medical applications, including diagnostic support, automated report generation, drug discovery, and conversational agents. Nevertheless, substantial challenges remain regarding the seamless integration of varied data types, model interpretability, ethical considerations, and clinical validation. The review identifies major opportunities and persistent gaps, offering crucial insights for building trustworthy, scalable, and clinically relevant multimodal AI tools for healthcare."
52,"S. L. Rhodes, S. A. Crabtree, and J. Freeman, ""An Agent-Based Model of Hierarchical Information-Sharing Organizations in Asynchronous Environments,"" Journal of Artificial Societies and Social Simulation, vol. 27, no. 2, p. 2, 2024. [Online]. Available: https://www.jasss.org/27/2/2.html","This paper investigates why command hierarchies are prevalent in organizations and how environmental variation influences their effectiveness using an agent-based model. The model assumes agents (workers) with perfect but limited memory face synchronized or asynchronous binary decision problems in changing environments, with managers having broader but less local information. Results show that command hierarchies outperform non-hierarchical groups in most conditions, especially where local changes are somewhat synchronized, and that the benefit persistsalbeit less stronglyeven when local environments differ. Crucially, performance is highest when workers can autonomously judge and weigh managerial input, allowing organizational learning, whereas strictly enforced hierarchy can result in worse outcomes. Memory length further improves group performance across all environments. The study reveals an inherent, unresolvable computational tension between the local and managerial perspectives, suggesting that this tension, managed by allowing both manager input and worker autonomy, is not dysfunctional but fundamental to organizational adaptability. Thus, the best organizational strategy couples the informative input of managers with space for worker discretion, enabling hierarchical organizations to be both structured and adaptable in dynamic contexts."
53,"M. Belfrage, E. Johansson, F. Lorig, and P. Davidsson, ""[In]Credible Models – Verification, Validation & Accreditation of Agent-Based Models to Support Policy-Making,"" Journal of Artificial Societies and Social Simulation, vol. 27, no. 4, p. 4, 2024. [Online]. Available: https://www.jasss.org/27/4/4.html","This paper examines the crucial concept of model credibility for Agent-based Models (ABMs) in policy-making, defining credibility as a measure of confidence in a models inferential capability. It critically surveys the literature on Verification, Validation, and Accreditation (VV&A), identifying persistent challenges such as insufficient V&V documentation and a lack of standardized accreditation routines in public administration. The authors propose a structured V&V plan and an accreditation frameworkdrawing practical inspiration from the US Coast Guard processwhich involves collaborative engagement between modellers, policy actors, and an accreditation unit across the conceptualization, implementation, and evaluation phases. They underline that high system complexity and the pressure to act, especially in crises, can induce cognitive shortcuts in credibility assessment, amplifying the need for rigorous procedures to avoid both type-I and type-II errors. The discussion emphasizes that a models credibility depends on appropriate validation relative to real-world referents and careful design choices, particularly as ABMs gain prominence in complex social systems. The work concludes that establishing clear accreditation protocols not only bolsters trust and transparency between ABM developers and policy-makers but also enhances preparedness for societal challenges, recommending further research to test these frameworks in real-world settings."
54,"K. Padur, H. Borrion, and S. Hailes, ""Using Agent-Based Modelling and Reinforcement Learning to Study Hybrid Threats,"" Journal of Artificial Societies and Social Simulation, vol. 28, no. 1, p. 1, 2025. [Online]. Available: https://www.jasss.org/28/1/1.html","This paper addresses the growing need to understand and respond to hybrid attacks, which strategically coordinate threats across cyber, physical, and social domains to undermine trust and public stability, as evidenced in cases like the 2014 Ukraine conflict. Traditional game-theoretic approaches are critiqued for their reliance on strong rationality and simplifications. The authors present a novel agent-based model simulating a Cyber-Physical-Social System (CPSS), where agentsincluding malicious onesuse reinforcement learning (RL) to make adaptive decisions amid denial-of-service and misinformation campaigns. The model integrates opinion dynamics, linking social feedback and personal experiences to behavior. Experiments demonstrate that, absent attacks, agents choose providers equitably and are satisfied, but during RL-driven hybrid attacks, satisfaction and positive opinion metrics fall, regret rises, and preferences shiftimpacts that intensify with longer and coordinated attacks. Linear regression quantifies relationships between attack properties and societal effects. Discussion highlights the value of RL-based approaches over classical solutions in simulating adaptive, intelligent threats, acknowledging the limitations of lacking real-world data, model complexity, and currently omitting defender learning. Future directions include simulating optimal attack orderings, evaluating proactive defences (potentially also RL-based), and mitigating coordinated campaigns. Overall, the approach underscores the importance of data-driven, adaptive simulation to inform holistic, defence-in-depth strategies against future AI-augmented hybrid threats. The model and additional materials are available online for replication and extension."
55,"D. J. Rosenkrantz, M. V. Marathe, Z. Qiu, S. S. Ravi, and R. E. Stearns, ""On Some Fundamental Problems for Multi-Agent Systems Over Multilayer Networks,"" Proc. of the 24th International Conference on Autonomous Agents and Multi-agent Systems (AAMAS 2025), Detroit, MI, May 2025. [Online]. Available: https://arxiv.org/abs/2503.12684","This paper studies multilayer synchronous dynamical systems (MSyDSs), a class of multi-agent models for processes such as contagion and information spread over multilayer networks, wherein inter-agent connections possess distinct semantics (e.g., friendship, family, colleagues). The authors rigorously analyze both the structural and computational aspects of MSyDSs, contrasting them with single-layer networked systems. Key results establish that MSyDSs exhibit strikingly more complex phase spaces: for instance, even with only two layers and symmetric threshold functions, phase space cycles can attain exponential length ($2^n$), and certain configurations display exactly $n+1$ cycles with minimal transients. The central computational discovery is that the equivalence (or inequivalence) problem for MSyDSs is NP-complete, even if the only difference is the local function in one node of one layer. Despite this, the authors design efficient (polynomial-time) equivalence algorithms for restricted MSyDSs, such as those with fixed layers or all symmetric/bounded-threshold functions with OR/AND master functions. The work further explores the expressive power granted by additional layers, identifying conditions under which a $k$-layer system can or cannot be represented by a system with fewer layers. Challenges discussed include the NP-completeness boundary and phase space explosion, and the paper closes with open questions about generalizing these results to broader classes of local/master functions, stochastic update rules, and additional fundamental problems such as fixed-point detection."
56,"F. Adobbati and Ł. Mikulski, ""Asynchronous Multi-Agent Systems with Petri nets,"" arXiv preprint arXiv:2504.00602, 2025. [Online]. Available: https://arxiv.org/abs/2504.00602","Modeling the interaction between components is crucial for many applications and forms a fundamental step in analyzing and verifying properties in multi-agent systems. This paper proposes a method based on 1-safe Petri nets to model Asynchronous Multi-Agent Systems (AMAS), drawing from two semantics defined on AMAS as transition systems. The focus is on two synchronization types: synchronization on transitions and synchronization on data, each equipped with an operator to compose 1-safe Petri nets. The authors demonstrate relationships between the resulting composed Petri net and global transition systems as defined in existing literature. Furthermore, they analyze the relationships between these two semantics on Petri nets and present two constructions that enable switching between semantics, thereby facilitating system analysis by allowing the selection of the most suitable model for the property under verification."
57,"D. Deplano, N. Bastianello, M. Franceschelli, and K. H. Johansson, ""Optimization and Learning in Open Multi-Agent Systems,"" arXiv preprint arXiv:2501.16847, 2025. [Online]. Available: https://arxiv.org/abs/2501.16847","Modern artificial intelligence leverages networks of agents that collaboratively collect and process data to tackle optimization and learning problems. This work introduces a novel distributed algorithm designed for ""open networks,"" where the number of participating agents may change over time due to factors like agent autonomy, resource variability, or denial-of-service attacks. The algorithm's convergence is grounded in the new ""Theory of Open Operators,"" which studies operators whose updated components and dimensions vary over time. Unlike traditional regret-based analyses, the framework evaluates algorithm performance via punctual distance from the optimal solution. Demonstrative applications encompass dynamic consensus and tracking (for metrics such as average, median, min/max), as well as classification problems using logistic loss, underscoring the algorithm's flexibility and robustness in evolving network environments."
58,"A. Domenteanu, C. Delcea, N. Chiriță, and C. Ioanăș, ""From Data to Insights: A Bibliometric Assessment of Agent-Based Modeling Applications in Transportation,"" Applied Sciences, vol. 13, no. 23, p. 12693, Dec. 2023. [Online]. Available: https://doi.org/10.3390/app132312693","This paper presents a bibliometric analysis of the application of agent-based modeling (ABM) within transportation research, utilizing 1016 papers from the ISI Web of Science database (20022023) identified via targeted ABM and transportation keywords. The systematic bibliometric approach encompassed evaluation of leading contributors, institutional affiliations, influential works, and prominent journals. The results show robust growth in ABM-related transportation research, with a significant increase from 2008 and a peak in 2021, yielding an overall annual publication growth rate of 21.67%. N-gram analysis identified air and road transport as major domains of ABM advancement, with climate change considered a persistent thematic concern across the entire period."
59,"C. Delcea, R. J. Milne, and L.-A. Cotfas, ""Evaluating Classical Airplane Boarding Methods for Passenger Health during Normal Times,"" Applied Sciences, vol. 12, no. 7, p. 3235, 2022. [Online]. Available: https://doi.org/10.3390/app12073235","The paper examines how classical airplane boarding methods perform with respect to passenger health metrics that were developed during the COVID-19 pandemic, even when social distancing measures such as keeping middle seats empty are no longer in place. While pre-pandemic research mostly focused on minimizing boarding time, this study incorporates additional health-related evaluation criteria. The analysis is relevant for both normal times and scenarios where an epidemic has begun but is not yet recognized, aiming to maintain lower health risks for passengers. The findings indicate that the reverse pyramid boarding method achieves favorable results across most health metrics and continues to minimize overall boarding time, suggesting a beneficial approach even beyond the immediate pandemic context."
60,"J. Xu, J. Huang, J. Yang, and N. Zhong, ""M2GCF: A multi-mixing strategy for graph neural network based collaborative filtering,"" Web Intelligence and Agent Systems: An International Journal, vol. 21, no. 2, pp. 149-166, 2023. [Online]. Available: https://dblp.org/rec/journals/wias/XuHYZ23","Graph Neural Networks (GNNs) have been effectively employed for Collaborative Filtering (CF) recommendations, learning user and item representations (GNN-CF). While contrastive learning is usually added as an auxiliary task, both the main recommendation task and this auxiliary task face challenges such as noise and hard negative sample distillation; however, existing models typically address only one of these issues. To address both problems simultaneously, this work proposes a Multi-Mixing strategy for GNN-based CF (M2GCF). In the main task, M2GCF introduces a mixing strategy to perturb user, item, and negative item embeddings with sample noise, while in the auxiliary task, it applies a two-step mixing strategy within a contrastive learning framework to generate hard negatives. Experiments across three benchmark datasets show that M2GCF outperforms baselines, demonstrating robustness to interaction noise and improved recommendation accuracy, particularly for long-tail items."
61,"S. Demir, U. Mutlu, and Ö. Özdemir, ""Neural Academic Paper Generation,"" arXiv preprint arXiv:1912.01982, 2019. [Online]. Available: https://arxiv.org/abs/1912.01982","In this work, the authors address the problem of structured text generation by focusing on academic paper generation in LaTeX, motivated by the promising results of basic character-level language models. They seek to apply advanced language modeling techniques to the complex domain of LaTeX source files, with the goal of producing realistic academic papers. Their main contributions include the preparation of a dataset composed of recent open-source computer vision papers in LaTeX format, and the application of cutting-edge models such as Transformer and Transformer-XL for generating consistent LaTeX code. The study reports quantitative results using cross-entropy and bits-per-character (BPC), alongside qualitative analysis of generated LaTeX examples."
62,"S. Liu, J. Cao, R. Yang, and Z. Wen, ""Generating a Structured Summary of Numerous Academic Papers: Dataset and Method,"" arXiv preprint arXiv:2302.04580, 2023. [Online]. Available: https://arxiv.org/abs/2302.04580","Writing a survey paper on a research topic often requires synthesizing content from many related papers, framing this process as a multi-document summarization (MDS) challenge. While existing MDS datasets focus on unstructured summaries from a few documents and previous methods for structured summaries address only single documents, there is a lack of resources and techniques suitable for generating structured summaries from numerous academic papers. To address this, the authors introduce BigSurvey, the first large-scale dataset designed for comprehensive summaries across many papers per topic, sourcing summaries from over seven thousand survey papers and using abstracts from 430,000 reference papers as input. To effectively organize and condense information from such extensive sources, they propose the category-based alignment and sparse transformer (CAST) method, which improves the efficiency of long-sequence processing. Experimental results demonstrate that CAST outperforms several advanced summarization techniques."
63,"T. Rehman, D. K. Sanyal, S. Chattopadhyay, P. K. Bhowmick, and P. P. Das, ""Generation of Highlights From Research Papers Using Pointer-Generator Networks and SciBERT Embeddings,"" IEEE Access, vol. 11, pp. 91358-91374, 2023. doi:10.1109/ACCESS.2023.3292300. Available: https://ieeexplore.ieee.org/document/10172215","Nowadays many research articles include research highlights to concisely summarize their key findings, facilitating both researcher comprehension and article discoverability. This paper proposes an automatic research highlight generation system using a pointer-generator network with a coverage mechanism and a contextual embedding layer that encodes input tokens into SciBERT embeddings. Evaluated on the CSPubSum benchmark and the newly presented MixSub multidisciplinary corpus, the model achieves superior results compared to existing models. On CSPubSum, using only the abstract as input, the model yields ROUGE-1, ROUGE-2, and ROUGE-L F1-scores of 38.26, 14.26, and 35.51, respectively, along with a METEOR score of 32.62 and a BERTScore F1 of 86.65, outperforming all baselines. On MixSub, trained across all subject categories, the scores are ROUGE-1: 31.78, ROUGE-2: 9.76, ROUGE-L: 29.3, METEOR: 24.00, and BERTScore F1: 85.25, demonstrating the models robust generalizability."
64,"M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, “A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges,” IEEE Access, vol. 12, pp. 26839-26874, 2024. doi:10.1109/ACCESS.2024.3365742. Available: https://ieeexplore.ieee.org/document/10433480","Large Language Models (LLMs) have achieved remarkable advancements across tasks such as natural language processing, translation, text generation, and question answering, underpinned by their capacity to model and generate complex language patterns. This comprehensive review surveys recent LLM research, encompassing their history, transformer architectures, resources, training methodologies, and key datasets. It details the extensive applications of LLMs in fields like biomedical healthcare, education, business, and agriculture, and evaluates their societal impacts, highlighting both transformative potential and challenges including ethics, model bias, computational resource requirements, interoperability, privacy, and security. The review further addresses strategies to enhance the robustness and controllability of LLMs and explores open research directions for improving their impact and reliability. Serving as a concise reference, the paper aims to assist researchers, practitioners, and experts in navigating the rapid evolution of LLM technologies and their practical deployment across diverse domains."
65,"H. Hassani and E. S. Silva, ""The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"" Big Data and Cognitive Computing, vol. 7, no. 2, Article number 45, 2023. [Online]. Available: https://www.mdpi.com/2504-2289/7/2/45","ChatGPT, a conversational AI leveraging natural language processing and machine learning, presents significant opportunities and challenges for data science. This perspective article outlines ChatGPTs potential to automate tasks such as data cleaning, preprocessing, model training, and results interpretation, as well as its capacity to generate new insights from unstructured data and improve decision-making. The architectures flexibility enables fine-tuning across diverse language-related tasksincluding language translation, sentiment analysis, and text classificationand supports synthetic data generation. However, challenges persist regarding bias, plagiarism, and interpretability of output. Despite these issues, the authors argue that the advantages of integrating ChatGPT into data science, such as enhanced productivity and accuracy, make it poised to become a pivotal tool for intelligence augmentation in this domain, although careful consideration must be given to its limitations, especially in cases outside the scope of its prior training."
66,"V. Taecharungroj, ""‘What Can ChatGPT Do?’: Analyzing Early Reactions to the Innovative AI Chatbot on Twitter,"" Big Data and Cognitive Computing, vol. 7, no. 1, Article number 20, 2023. [Online]. Available: https://www.mdpi.com/2504-2289/7/1/20","This study analyzed 233,914 English tweets about ChatGPT collected in the first month after its launch, applying latent Dirichlet allocation (LDA) topic modeling to determine public perceptions of ""what can ChatGPT do?"". The analysis identified three overarching topicsnews, technology, and reactionsand five main functional domains linked to ChatGPT: creative writing, essay writing, prompt writing, code writing, and answering questions. Findings highlight ChatGPT's potential to impact both technology and society, bringing both benefits and drawbacks. The author concludes by outlining four major issues arising from AI advancements such as ChatGPT: the evolution of jobs, the emergence of a new technological landscape, the quest for artificial general intelligence, and ethical dilemmas related to technological progress."
67,"G. Bonifazi, E. Corradini, D. Ursino, and L. Virgili, ""Modeling, Evaluating, and Applying the eWoM Power of Reddit Posts,"" Big Data and Cognitive Computing, vol. 7, no. 2, Article number 26, 2023. [Online]. Available: https://www.mdpi.com/2504-2289/7/2/26","This paper explores Electronic Word of Mouth (eWoM) specifically on Reddit, a platform less studied in this context compared to Facebook or Twitter. The authors propose a model to represent and evaluate the eWoM Power of Reddit posts, and introduce two applications for this model: lifespan templates, which capture how a post's engagement evolves over time, and post profiles that summarize Reddit post characteristics. These two tools are orthogonal and can be used together for various applications in social media analysis."
68,"K. Pillutla, L. Liu, J. Thickstun, S. Welleck, S. Swayamdipta, R. Zellers, S. Oh, Y. Choi, and Z. Harchaoui, “MAUVE Scores for Generative Models: Theory and Practice,” Journal of Machine Learning Research, vol. 24, no. 356, pp. 1–92, 2023. Available: https://www.jmlr.org/papers/volume24/23-0023/23-0023.pdf","Generative artificial intelligence has advanced to produce text and images that closely resemble human outputs, creating a need for robust metrics to compare generated and true data distributions. This paper introduces Mauve, a family of divergence frontier-based measures designed to capture two key error types in generative modeling for text and images. The authors propose three statistical estimation approachesvector quantization, non-parametric, and classifier-basedand provide theoretical error bounds for the vector quantization method. Experiments demonstrate that Mauve scores correlate with human evaluations and effectively distinguish properties of both text and image generative models, outperforming existing metrics in some cases. Practical guidelines are recommended for deploying Mauve across language and vision tasks."
69,"W. Fedus, B. Zoph, and N. Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,” Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. Available: https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf","In deep learning, standard models use the same parameters for all inputs, but Mixture of Experts (MoE) architectures challenge this norm by selecting different parameters for each example, resulting in sparsely-activated models with immense parameter counts but constant computational costs. The Switch Transformer addresses the main limitations of MoEscomplexity, communication overhead, and training instabilityby simplifying the routing mechanism and improving model design to reduce both communication and computational expenses. The paper introduces new training techniques that enable, for the first time, the stable training of large sparse models with lower precision (bfloat16) numerics. Incorporating these advances into T5-Base and T5-Large models achieves up to 7x faster pre-training at equal computational cost. These benefits generalize to multilingual settings, outpacing mT5-Base performance in all 101 languages assessed. The work culminates in the pre-training of trillion-parameter models using the ""Colossal Clean Crawled Corpus,"" demonstrating a 4x speedup compared to T5-XXL, establishing a new frontier in language model scale and efficiency."
70,"A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, “PaLM: Scaling Language Modeling with Pathways,” Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113, 2023. Available: https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf","Large language models have demonstrated remarkable performance across diverse natural language tasks via few-shot learning, significantly reducing the need for task-specific training data. To investigate the influence of model scale on few-shot learning, the authors trained PaLM, a 540-billion parameter Transformer language model, efficiently utilizing 6144 TPU v4 chips through the Pathways system. PaLM achieved state-of-the-art results on numerous language understanding and generation benchmarks, exhibiting breakthrough performance in multi-step reasoning tasks and outpacing average human performance on the BIG-bench benchmark. Notably, several BIG-bench tasks displayed discontinuous performance gains as model size increased. The model also exhibited robust multilingual and source code generation abilities. Comprehensive analyses were conducted on bias, toxicity, data memorization, and ethical considerations, with discussion of relevant mitigation strategies."
71,"B. Sun and K. Li, ""Neural Dialogue Generation Methods in Open Domain: A Survey,"" Natural Language Processing Research, vol. 1, no. 3-4, pp. 56–70, Mar. 2021. [Online]. Available: https://www.atlantis-press.com/journals/nlpr","This survey systematically reviews advances in neural open-domain dialogue generation, tracing the history from rule- and retrieval-based systems to the current dominance of deep learning methods. The authors categorize generative approaches into six main types: Encoder-Decoder (Seq2Seq), Hierarchical Recurrent Encoder-Decoder (HRED), Variational Autoencoder (VAE), Reinforcement Learning (RL), Generative Adversarial Network (GAN), and pretraining-based methods. Each method is discussed with respect to its motivation, architecture, and evolution. Comparative analysis finds GAN-based approaches tend to achieve the best overall performance, with RL and VAE methods offering advantages in certain dimensions, though HRED and traditional Seq2Seq lag behind. For evaluation, datasets such as OpenSubtitles, Cornell Movie, and DailyDialog are widely used, and metrics like Perplexity, BLEU, and embedding-based measures are common, though automatic evaluations remain only loosely correlated with human judgments. The authors highlight challenges like incorporating knowledge, leveraging multidisciplinary insights, improving evaluation alignment with human perception, and creating practical high-quality datasets. The conclusion emphasizes the promise but immaturity of current neural generative dialogue systems and enumerates key future research directions required for substantial progress in the field."
72,"E. Yilmaz Ince and A. Kutlu, ""Web-Based Turkish Automatic Short-Answer Grading System,"" Natural Language Processing Research, vol. 1, no. 3-4, pp. 46–55, Mar. 2021. [Online]. Available: https://www.atlantis-press.com/journals/nlpr","This paper introduces TASAG, the first web-based Turkish Automatic Short-Answer Grading (ASAG) system, addressing the lack of automated grading tools tailored for Turkish, an agglutinative language with significant morphological complexity. TASAG employs a hybrid algorithm: Cosine similarity for responses with fewer than 10 words and Integrated Latent Semantic Analysis (ILSA), enhanced by synonym and hypernym handling via Turkish WordNet and Zemberek, for longer answers. In a university case study (41 students, 10 questions, 2 independent instructor answer keys), TASAGs scores closely matched manual grading, achieving 92% accuracy and a strong Kendalls agreement ($W = 0.9254$), with no statistically significant difference to human raters ($p > 0.05$). The system is scalable, supports instant feedback, reduces instructor bias and effort, and is extensible to other domains by incorporating additional domain-specific vocabulary. Future work includes expanding Turkish WordNet and applying more advanced NLP techniques, such as ontological or explicit semantic analysis approaches. TASAG thus represents a robust, objective, and adaptable solution for Turkish-language automated grading in educational contexts."
73,"Q. Su, M. Wan, X. Liu, and C.-R. Huang, ""Motivations, Methods and Metrics of Misinformation Detection: An NLP Perspective,"" Natural Language Processing Research, vol. 1, no. 1, pp. 1–13, 2021. [Online]. Available: https://www.atlantis-press.com/journals/nlpr","The paper provides a comprehensive review of misinformation detection in the digital age, outlining its emergence as a global challenge due to the democratization of information dissemination via the internet and social media. Misinformationencompassing rumors, deception, hoaxes, fake news, and spamposes severe risks to democracy and public trust. Early detection efforts have transitioned from human-crafted rules to leveraging traditional machine learning (e.g., SVM, Naive Bayes, Logistic Regression, Random Forests) and neural network models (LSTM, RNN, CNN, GAN), cast as classification, regression, or clustering problems. Key performance results include statistical models achieving up to 91.3% accuracy with SVMs on syntactic and object-descriptor features, and neural networks yielding up to 93% F1 on multi-modal datasets (text+image). The field's development is constrained by domain-limited and small datasets (such as LIAR, FEVER, BuzzFeedNews) and high annotation costs, while multi-modal and multi-source inference is promising but hampered by data scarcity. Major challenges include the inadequacy of content-only cues, difficulty in early detection, reliance on metadata, paucity of annotated multi-modal data, and lack of interpretability in deep models. The authors argue for future advances in richer, multi-modal datasets, better integration of heterogeneous information, unsupervised/semi-supervised learning to address label scarcity, and improved interpretability. They conclude that although efforts have persisted for decades, the dynamic, evolving, and multifaceted nature of misinformation ensures ongoing research challenges, and suggest that more diversified, multi-source, and multi-modal detection systems are essential for future progress."
74,"E. Orel, I. Ciglenecki, A. Thiabaud, A. Temerev, A. Calmy, O. Keiser, and A. Merzouki, “An Automated Literature Review Tool (LiteRev) for Streamlining and Accelerating Research Using Natural Language Processing and Machine Learning: Descriptive Performance Evaluation Study,” Journal of Medical Internet Research, vol. 25, p. e39736, Sep. 2023. [Online]. Available: https://www.jmir.org/2023/1/e39736/","LiteRev is an advanced automation tool developed to enhance and expedite literature reviews (LRs) using natural language processing (NLP) and machine learning (ML) techniques. It operates by querying eight open-access databases, extracting metadata and text (abstract or full text), and representing papers as a term frequency-inverse document frequency (TF-IDF) matrix. Texts undergo preprocessing, including lemmatization and n-gram modeling, before dimensionality reduction (PaCMAP) and clustering (HDBSCAN) identify topic groupings. Users can refine searches via keyword or topic selection and provide key papers, after which LiteRev uses a k-nearest neighbor (k-NN) module to suggest potentially relevant articles. In a parallel evaluation with a manual LR on acute and early HIV infection in sub-Saharan Africa, LiteRev processed 631 unique PubMed papers, identified 16 topics, and suggested 193 papers for screening. It achieved a recall of 73.6% at the abstract screening stage (64/87 relevant papers) and 87.5% at full text (42/48), enabling a 56% reduction in workload compared to manual screening. Limitations include dependence on open-access sources and the lack of a web-based interface as of January 2023, but ongoing development aims to address these. LiteRev demonstrates significant time savings and facilitates a rapid, in-depth overview of research landscapes, with datasets and code available for reference and benchmarking."
75,"K. Cowie, A. Rahmatullah, N. Hardy, K. Holub, and K. Kallmes, “Web-Based Software Tools for Systematic Literature Review in Medicine: Systematic Search and Feature Analysis,” JMIR Medical Informatics, vol. 10, no. 5, pp. e33219, May 2022. [Online]. Available: https://medinform.jmir.org/2022/5/e33219/","This study provides a comprehensive, feature-by-feature comparison of web-based software tools supporting systematic reviews (SRs), which are essential yet costly and time-intensive for evaluating medical therapies. The authors identified 53 SR tools by reviewing repositories, literature, and web search but excluded reference managers, desktop apps, non-functional software, and tools requiring coding, resulting in 24 tools for detailed assessment. Thirty features across six classes were assessed using binary scoring and adjudicated review, yielding an 86.46% interobserver agreement. Of the tools evaluated, Giotto Compliance (27/30 features), DistillerSR (26/30), and Nested Knowledge (26/30) had the broadest support, though crucial features like direct database search, dual data extraction, and support for living (updatable) reviews were uncommon (supported by only 10, 7, and 13 tools, respectively). Screening and collaboration functionalities were robust, but automation, dual extraction, and living review support were notable gaps. Despite the evolving landscape, the review highlights DistillerSR, EPPI-Reviewer Web, and Nested Knowledge as the most feature-complete, but underscores the need for enhanced automation and regular updating mechanisms. The comparison aims to guide evidence-based selection of SR tools and inform future development priorities for greater transparency, replicability, and efficiency in SR workflows."
76,"E. Guo, M. Gupta, J. Deng, Y.-J. Park, M. Paget, and C. Naugler, “Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study,” Journal of Medical Internet Research, vol. 26, p. e48996, Jan. 2024. [Online]. Available: https://www.jmir.org/2024/1/e48996/","This study evaluates the efficacy of OpenAIs GPT and GPT-4 APIs in automating the labor-intensive process of screening titles and abstracts for clinical reviews, comparing their performance against human reviewers across six diverse medical data sets comprising over 24,000 records. The workflow involves providing plain-language screening criteria to the GPT API, which then classifies each entry as included or excluded. The model achieved an overall accuracy of 0.91, a macro F1-score of 0.60, sensitivity of 0.76 for included papers and 0.91 for excluded papers, and a prevalence- and bias-adjusted  (PABAK) of 0.96, while interrater agreement between two human reviewers was =0.46. A summary table of dataset-specific performances is:

\[
\begin{tabular}{lccccccc}
\hline
Data set & Accuracy & \text{F1} & \text{Sens. (incl.)} & \text{Sens. (excl.)} & \kappa_{\text{human}} & \kappa_{\text{screen}} & \text{PABAK} \\
\hline
IVM & 0.748 & 0.610 & 0.686 & 0.756 & 0.72 & 0.26 & 0.78 \\
SSRI & 0.846 & 0.595 & 0.966 & 0.949 & 0.58 & 0.21 & 0.99 \\
LPVR & 0.949 & 0.613 & 0.593 & 0.862 & 0.51 & 0.25 & 0.88 \\
RAYNAUDS & 0.965 & 0.607 & 0.833 & 0.966 & 0.91 & 0.22 & 0.99 \\
NOA & 0.895 & 0.601 & 0.782 & 0.898 & 0.35 & 0.23 & 0.95 \\
LLM & 0.943 & 0.594 & 1.000 & 0.942 & 0.69 & 0.21 & 0.98 \\
\hline
\end{tabular}
\]

The API demonstrated the ability to provide and revise its reasoning upon request, signaling potential for transparent AI-aided review pipelines. However, while the models specificity in excluding irrelevant articles is strong, lower sensitivity for inclusion raises concern over potentially missing relevant studies, and thus it should be seen as a decision-support tool rather than a replacement for human review. The discussion notes that leveraging GPT as a first-pass screening tool can help streamline the process, reduce the human workload, and promote consistency, especially when resources are constrained. Challenges include the potential for relevant article omission and the need for broader data set validation. Future directions include enhancing model performance, incorporating richer metadata, experimenting with few-shot learning and fine-tuning, and expanding to other domains of medical research. Overall, large language models such as GPT-4 offer substantial promise in expediting clinical review processes, with the caveat that human oversight remains essential to ensure comprehensiveness and accuracy."
77,"C. F. Atkinson, ""Cheap, Quick, and Rigorous: Artificial Intelligence and the Systematic Literature Review,"" Social Science Computer Review, vol. 42, no. 2, pp. 376–393, 2024. [Online]. Available: https://journals.sagepub.com/doi/full/10.1177/08944393231196281","The paper discusses how systematic literature reviews (SLRs) can benefit from advances in Artificial Intelligence (AI) and Machine Learning Techniques (MLTs) to enhance speed, rigour, transparency, and replicability. Targeted at researchers with coding experience, it outlines how programming languages can facilitate unsupervised machine learning for data synthesis and abstraction in SLRs. By leveraging Deductive Qualitative Analysis, the article demonstrates the utility of AI and MLTs in coding and categorising extracted SLR data. As a proof of concept, the paper presents code for implementing Topic Modelling using Latent Dirichlet Allocation (LDA), showcasing a practical approach to automating data synthesis and abstraction. This integration is argued to increase the efficiency and rigour of research projects."
78,"D. Antons, C. F. Breidbach, A. M. Joshi, and T. O. Salge, ""Computational Literature Reviews: Method, Algorithms, and Roadmap,"" Organizational Research Methods, vol. 26, no. 1, pp. 107–138, 2023. [Online]. Available: https://journals.sagepub.com/doi/full/10.1177/1094428121991230",The paper introduces and defines computational literature reviews (CLRs) as a novel review methodology and proposes a six-step roadmap for their execution.
79,"G. Wagner, R. Lukyanenko, and G. Paré, ""Artificial intelligence and the conduct of literature reviews,"" Journal of Information Technology, vol. 37, no. 2, pp. 209–226, 2022. [Online]. Available: https://journals.sagepub.com/doi/full/10.1177/02683962211048201","Artificial intelligence (AI) is increasingly transforming conventional research methodologies, particularly in the realm of literature reviews. The paper discusses how AI can enhance and expedite various steps of the literature review process, introducing recent trends and outlining strategies that support researchers in leveraging AI for more efficient scholarly work. By highlighting these developments, the authors aim to familiarize the research community with emerging AI-enabled opportunities in literature analysis and synthesis."
80,"F. Bolaños, A. Salatino, F. Osborne, and E. Motta, ""Artificial intelligence for literature reviews: opportunities and challenges,"" Artificial Intelligence Review, vol. 57, no. 259, pp. 1–59, 2024. Available: https://link.springer.com/article/10.1007/s10462-024-10902-3","This paper provides a comprehensive review of Artificial Intelligence (AI) applications in Systematic Literature Reviews (SLRs), focusing on the screening and extraction phases that are typically time-consuming and resource-intensive. Using the PRISMA framework, the authors evaluated 21 SLR tools against a set of 34 features23 traditional and 11 AI-centricrevealing that while most tools employ AI, especially machine learning classifiers like SVMs for screening, only a subset supports automated extraction through techniques such as Named Entity Recognition. The study shows partial adoption of advanced methods such as embeddings and Large Language Models (LLMs), with 11 nascent LLM-based tools also analysed for literature search and writing assistance. Usability, transparency, and comprehensive evaluation remain significant challenges, as many tools vary widely in accessibility and tend to lack standardised assessments beyond performance metrics. Three primary research challenges are highlighted: the integration of advanced AI elements (including LLMs, knowledge graphs, and interpretability), improvement of usability to drive adoption, and the development of standard evaluation frameworks to build trust and comparability. The authors propose best practices including documentation, open evaluation, use of standardised datasets, and public code release. They conclude that while existing AI-enhanced SLR tools are promising, the next generationdriven by LLMs and explainable AIrequires further advancement in usability, transparency, and ethical standards, emphasising the enduring importance of human oversight and critical judgement in literature reviews. Appendix tables (available online) detail the evaluation mappings, and directions for future research advocate living review systems, open science measures, and accelerating LLM-assisted academic workflows."
81,"M. N. Quang, T. Rogers, J. Hofman, and A. B. Lanham, ""New framework for automated article selection applied to a literature review of Enhanced Biological Phosphorus Removal,"" PLOS ONE, vol. 14, no. 5, e0216126, 2019. [Online]. Available: https://doi.org/10.1371/journal.pone.0216126","This paper presents a novel bibliometric framework designed to automate article selection for literature reviews, demonstrated in the context of Enhanced Biological Phosphorus Removal (EBPR)a core technology in wastewater treatment to prevent eutrophication. By aggregating and analyzing thousands of EBPR-related publications from 19752017, the authors employ citation network analysis, keyword clustering, citation profiles, and benchmarks to identify twelve key research topics within EBPR. These topicsranging from the identification of relevant microorganisms and characterization of their metabolism to emerging areas like volatile fatty acid production, P-recovery, and the use of aerobic granuleswere distilled using only about 5% of the original publication set. Detailed cluster-by-cluster reviews trace the evolution and interconnections of EBPR research, while the automated approach offers objectivity and scalability, reducing both manual effort and bias inherent in traditional reviews. The study discusses methodological strengths and limitations, particularly concerning metadata source and language bias, and suggests refinements for iterative research niche identification. Ultimately, the framework enables systematic, computer-aided knowledge curation in rapidly growing scientific fields, improving accessibility for newcomers and supporting comprehensive reviews with high efficiency and reproducibility."
82,"B. Ozek, Z. Lu, F. Pouromran, S. Radhakrishnan, and S. Kamarthi, ""Analysis of pain research literature through keyword Co-occurrence networks,"" PLOS Digital Health, vol. 2, no. 9, e0000331, 2023. [Online]. Available: https://doi.org/10.1371/journal.pdig.0000331","Pain is a major public health problem, and the exponential growth in pain-related research over the past two decades has made manual literature reviews infeasible. This study analyzes 264,560 articles on pain, published from 2002 to 2021 and indexed in IEEE, PubMed, Engineering Village, and Web of Science, by constructing keyword co-occurrence networks (KCNs) that treat keywords as nodes and their co-occurrences as edges. Using natural language processing and bibliometric network analysis, the authors built adjacency and weighted adjacency matrices for four-year publication windows, then analyzed centrality, affinity, and cohesiveness measures to map knowledge components and research trends. Key findings include a threefold growth in articles, sevenfold growth in unique keywords, and a 17-fold increase in keyword co-occurrences; dominant research areas involve pain management, chronic pain, opioids, and mental health comorbidities, while emergent trends include machine learning, biomarkers, self-management, and sensor-based methods. The analysis revealed shifts in research focus, such as increased interest in individualized care and quantification strategies, and used association rule mining to uncover strong thematic clusters (e.g., IF bladder pain syndrome, THEN interstitial cystitis). Limitations stem from subjective, single-category keyword classification and omission of multi-category or overlapping terms, which future work aims to address by automating categorization and expanding keyword sources. Overall, this KCN-based automated review maps the evolving structure of pain research, highlights synergies among its diverse topics, and paves the way for more refined, scalable literature analysis and collaboration mapping in the future."
83,"S. Blaschke, ""Publication authorship: A new approach to the bibliometric study of scientific work and beyond,"" PLOS ONE, vol. 19, no. 4, e0297005, 2024. [Online]. Available: https://doi.org/10.1371/journal.pone.0297005","This paper introduces ""publication authorship"" as a new bibliometric approach that complements traditional methods such as co-citation, bibliographic coupling, co-authorship, and collaboration networks. Using data sets from accounting, astronomy, and gastroenterology, the study constructs networks where vertices represent articles and edges signify shared authorship, contrasting this with bibliographic coupling, where edges represent shared references. Clustering analysis reveals that publication authorship typically produces significantly more, and finer-grained, clusterse.g., accounting jumps from 7 to 278 clustersdemonstrating higher goodness-of-fit to article contents as measured by cosine similarity. The method also identifies more detailed segmentation within research fields, enhancing the mapping of research frontiers. Compared to alternatives like co-word analysis and Latent Dirichlet Allocation, publication authorship displays greater transparency and interpretability while capturing both social and intellectual dimensions of scientific work. The methodology proves particularly promising for broader applications, including corporate and organizational studies, and offers a fine-grained, explanatory perspective in bibliometric analyses that surpasses simple refinements of prior methods."
84,"T. Kasanishi, M. Isonuma, J. Mori, and I. Sakata, “SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation,” arXiv preprint arXiv:2305.15186, 2023. [Online]. Available: https://arxiv.org/abs/2305.15186","Automatic literature review generation is a complex natural language processing challenge, hindered by the lack of large-scale datasets. This work introduces SciReviewGen, a resource containing over 10,000 literature reviews and 690,000 referenced papers. Using SciReviewGen, the authors assess transformer-based summarization models, such as an adapted Fusion-in-Decoder, for the literature review generation task. Human evaluations indicate that while certain machine-generated reviews approach the quality of human-written ones, significant obstacles remain, notably hallucinations and insufficient detail. The dataset and code are publicly available at https://github.com/tetsu9923/SciReviewGen."
85,"Joeri Minnen, Sven Rymenants, Ignace Glorieux, and Theun Pieter van Tienoven, ""Answering Current Challenges of and Changes in Producing Official Time Use Statistics Using the Data Collection Platform MOTUS,"" Journal of Official Statistics, vol. 39, no. 4, pp. 489–505, Dec. 2023. [Online]. Available: https://sciendo.com/article/10.2478/jos-2023-0023","The modernization of official statistics production faces hurdles such as technological advances, financial constraints, and heightened privacy concerns, driving a demand for scalable online data collection platforms. Time Use Surveys (TUS), which gather detailed household-level data through complex methodologies, are particularly impacted. This article introduces the Modular Online Time Use Survey (MOTUS) platform, designed in alignment with the Harmonized European Time Use Survey guidelines to address these challenges. MOTUS offers a paradigm shift by enhancing efficiency, reducing costs and respondent burden, and improving data reliability. Its modular architecture allows flexible deployment for various TUS configurations and can also support other diary-based surveys like household budget surveys using the same platform and applications."
86,"Andrea Roberson, ""Applying Machine Learning for Automatic Product Categorization,"" Journal of Official Statistics, vol. 37, no. 2, pp. 395–410, Jun. 2021. [Online]. Available: https://sciendo.com/article/10.2478/jos-2021-0017","The U.S. Census Bureau's Economic Census, an essential source for official business statistics and key economic measures like GDP and the Producer Price Index, faces challenges due to respondent burden and low survey response rates because of its extensive questionnaires. To mitigate these issues, the authors developed a natural language processing (NLP) approach that classifies goods and services automatically using business-supplied spreadsheets containing Universal Product Codes and product descriptions. By mapping these products to the North American Product Classification System (NAPCS) with NLP algorithms, the strategy achieved over 90% accuracy in text classification, streamlining data collection and potentially improving response rates for the Economic Census."
87,"Asaph Young Chun, Steven G. Heeringa, and Barry Schouten, ""Responsive and Adaptive Design for Survey Optimization,"" Journal of Official Statistics, vol. 34, no. 3, pp. 581–597, Sep. 2018. [Online]. Available: https://sciendo.com/article/10.2478/jos-2018-0028","We discuss an evidence-based approach to guiding real-time design decisions during the course of survey data collection, called responsive and adaptive design (RAD), which is a scientific framework driven by cost-quality tradeoff analysis and optimization to efficiently produce high-quality data. RAD is not a new concept or a one-size-fits-all solution, but incorporates existing practices and variants in survey research. The four foundational components of RAD are: (1) survey process data and auxiliary information, (2) design features and interventions, (3) explicit quality and cost metrics, and (4) quality-cost optimization tailored to survey strata. The paper reviews how these components are addressed in relevant literature, interrelating different perspectives and highlighting ongoing challenges and future directions, such as survey-assisted population modeling, rigorous optimization, and total survey cost modeling."
88,"M. Schonlau and M. P. Couper, ""Semi-automated categorization of open-ended questions,"" Survey Research Methods, vol. 10, no. 2, pp. 143–152, 2016. [Online]. Available: https://doi.org/10.18148/srm/2016.v10i2.6213","Text data from open-ended survey questions are challenging to analyze and are often overlooked, despite their value in capturing unconstrained responses. Traditional approaches use multiple human coders to categorize such answers, but recent advances in text mining present opportunities for automation. However, automated text classification does not reach sufficient accuracy to fully replace human coders. This work proposes a hybrid method: using text mining algorithms for easily categorized responses and reserving human coding for harder cases, with the division based on expected accuracy thresholds. The method employs multinomial boosting rather than relying on machine learning ""confidence scores"" transformed into pseudo-probabilities. The paper demonstrates this approach on examples including respondents' advice in hypothetical dilemmas, perceptions of disclosure risk, and reasons for quitting smoking, targeting an 80% combined accuracy. Results showed that using this thresholding strategy, 54%-80% of responses could be categorized automatically, illustrating potential efficiency gains in coding open-ended survey data."
89,"Z. He and M. Schonlau, ""Automatic Coding of Open-ended Questions into Multiple Classes: Whether and How to Use Double Coded Data,"" Survey Research Methods, vol. 14, no. 3, pp. 267–287, 2020. [Online]. Available: https://doi.org/10.18148/srm/2020.v14i3.7639","Responses to open-ended survey questions are typically categorized into predefined classes either manually or via statistical learning algorithms trained on such manual codings. This paper explores the impact of double codingwhere each response is coded by two annotatorson improving the automatic classification of open-ended responses. Four different training strategies using double-coded data are examined through experiments on both simulated and real datasets. The findings indicate that if double coding is already performed (with no extra cost), having an expert resolve disagreements between coders yields the highest classification accuracy. However, if there is a fixed manual coding budget, single coding is preferably more efficient when the expected coding error rate is below approximately 35% to 45%."
90,"M. Revilla, ""How to enhance web survey data using metered, geolocation, visual and voice data?,"" Survey Research Methods, vol. 16, no. 1, pp. 1–12, 2022. [Online]. Available: https://doi.org/10.18148/srm/2022.v16i1.8013","After briefly summarizing why there is a need to enhance web survey data, this paper explains how metered, geolocation, visual and voice data could help to supplement conventional web survey data, particularly when mobile participation is high. It presents expected benefits of these four data types in terms of respondents burden, data quality and possible new insights, as well as a number of expected disadvantages, both on the respondents and researchers sides. Finally, the paper discusses what is still missing and the next steps to turn these new opportunities into realities."
91,"H. Gweon, M. Schonlau, and M. Wenemark, ""Semi-automated classification for multi-label open-ended questions,"" Survey Methodology, vol. 46, no. 2, pp. 265-282, 2020. [Online]. Available: https://www150.statcan.gc.ca/n1/pub/12-001-x/2020002/article/00005-eng.htm","This paper introduces a semi-automated method for classifying multi-label open-ended survey responses, addressing the challenges of manual classification's labor-intensity and error-proneness. The approach begins with an initial set of human-coded answers used to train a machine learning classifier (specifically, a binary relevance strategy with random forests), capable of assigning multiple codes per answer and suggesting additional likely codes. Applied to data from the Swedish National Patient Survey, the method achieved high accuracy, flagged ambiguous responses for human review, and resulted in up to 70% reduction in manual coding actions without significant loss in coding quality. The system allows humans to verify and adjust suggestions interactively, balancing efficiency with reliability, although challenges remain in modeling rare codes and determining the optimal amount of initial manual coding. The method is most effective when many responses can be classified with high confidence, with human oversight reserved for more complex cases. Future work aims to refine thresholding strategies, broaden applicability to different languages and coding frameworks, and further integrate human-machine collaboration into routine survey workflows."
92,"Ting Yan, Hanyu Sun, and Anil Battalahalli, ""Applying Machine Learning to Survey Question Assessment,"" Survey Practice, vol. 17, May 2024. [Online]. Available: https://doi.org/10.29115/SP-2024-0006","Sun and Yan (2023) present an enhanced Computer-Assisted Recorded Interviewing (CARI) Machine Learning (ML) pipeline designed to rapidly and cost-effectively process 100% of recorded survey interviews, identifying recordings and specific survey questions at higher risk of falsification, undesirable interviewer behaviors, or poor performance. The pipelines extended architecture integrates speaker diarization (using Pyannote.Audio), speech-to-text (Whisper), and acoustic feature extraction (openSMILE, eGeMAPS, and random forest classifiers) to generate seven quantitative measures: number of respondent/interviewer turns, duration of respondents first turn, overall sequence duration, frequency of long pauses, overlapping speech, and respondent positive emotion. In a proof-of-concept study using 479 establishment survey recordings covering 20 questions, 53 respondents, and 4 interviewers, the pipelines flagged high-risk questions closely matched those identified by experts (inter-rater $\kappa=0.87$), although some discrepancies related to question type (e.g., debriefing prompts or points of respondent fatigue) persisted. The system excels at screening large datasets for problematic items, greatly improving review efficiency for behavior coding and question evaluation, though it may produce some false positives/negatives and benefits from complementary expert assessment. The authors recommend future validation against behavior coding, development of composite metrics, and broader application to interviewer-administered surveys, concluding that the approach is generalizable and valuable for scalable, ongoing survey quality assurance."
93,"Ting Yan, Hanyu Sun, and Anil Battalahalli, ""Using Machine Learning to Evaluate Questions in a Multilingual Survey,"" Survey Practice, vol. 19, Special Issue, March 2025. [Online]. Available: https://doi.org/10.29115/SP-2024-0021","This study examines the application of a machine learning (ML) pipeline to automatically evaluate survey question performance in both English and Spanish interviews from a nationally representative U.S. household survey. The pipeline computes seven outcome measuresincluding number of respondent/interviewer turns, respondents first turn duration, total duration, presence of pauses, overlapping speech, and expressions of positive emotionto flag problematic items. Analysis of 2,926 question-answering recordings, comprising 17 survey questions (1,425 in Spanish, 1,501 in English), revealed substantial agreement between the outcome measures and across languages in identifying the most challenging items. Eleven items were identified as poor-performing in Spanish by at least two measures and were also flagged in English. For instance, Q13 (open-ended) and Q16 (single-choice with showcard) induced more interactional difficulties in both languages. The study highlights that ML-based triaging allows efficient, automated, and cost-effective screening of poorly performing survey items across multiple languages, suitable for pre- and post-collection diagnostics. The authors recommend integrating this ML approach with traditional evaluation methods, leveraging its automation and flexibility to enhance multilingual survey quality. Future directions include using the variation (spread) in outcome measures for finer-grained detection of item/interviewer issues and extending the approach to assess interviewer behaviors in non-English interviews."
94,"Christine P. Chai, ""Text Mining in Survey Data,"" Survey Practice, vol. 12, no. 1, 2019. [Online]. Available: https://doi.org/10.29115/SP-2018-0035","This paper demonstrates how supervised latent Dirichlet allocation (sLDA) can be used to automatically and jointly analyze free-text and numerical data from an employee satisfaction survey, overcoming the high cost and subjectivity of manual coding. The authors preprocess 530 survey responses, applying stemming, tokenization, stopword removal, and careful handling of negations. With sLDA, each response (document) generates topic assignment vectors, and topics are modeled as distributions over words, with numerical ratings (110) treated as response variables drawn from a normal distribution using topic proportions; the core model is described as follows: for each document $D_d$, draw topic proportions $\theta_d \sim \mathrm{Dirichlet}(\alpha)$, for each word $W_{d,n}$ draw topic $Z_{d,n} \sim \mathrm{Multinomial}(\theta_d)$ and word $W_{d,n} \sim \mathrm{Multinomial}(\beta_{Z_{d,n}})$, then draw response $Y_d \sim N\left(\eta\,\overline{Z}_d, \sigma^2\right)$ where $\overline{Z}_d$ is the mean of topic assignments. Results show clear associations between selected words and each rating, e.g., high scores include words like opportunity and challenge, and low scores include lack and interest. The correlation between the appearance of challenge and higher ratings is 0.920. sLDA scales well, enabling analysis of large text datasets, and outputs topic-word distributions and credible intervals for topics and ratings. While initial setup can be significant and ample improvement in predictive intervals is necessary, this approach offers scalable, consistent, and less subjective survey text analysis, with future work needed to refine methods, quantify value-add, and explore the effectiveness of text-only surveys."
95,"N. Pinzón, M. M. Mathur, A. H. Liu, D. L. Redmiles, W. D. Bowen, M. A. Rodriguez, J. S. Jones, J. W. Deem, and P. Grabowicz, ""AI-powered fraud and the erosion of online survey integrity: An analysis of 31 fraud detection strategies,"" OSF Preprints (SocArXiv), 2024. [Online]. Available: https://osf.io/95tka/","This study investigates the escalating threat of both human and AI-driven fraud in online survey research, highlighting a two-year, two-survey project that faced two major waves of fraudulent responsesinitially dominated by human survey farmers and later by AI-generated entries. Across more than 3,000 responses and multiple fraud detection protocols, the researchers evaluated 31 existing fraud indicators, finding only three (hidden variables with manual review, open-ended AI-detection, and post-survey verification) to exhibit high or very high diagnostic accuracy, while 15 widely used indicators (such as attention checks and IP address filters) proved ineffective or poor as standalone measures. The results underscore that the rapidly evolving fraud landscape has rendered many standard detection methods obsolete, necessitating more robust, layered approaches that combine automation and labor-intensive human review. The authors call for community-wide development of adaptive, forward-looking anti-fraud protocols, enhanced publisher and IRB standards, and collaborative intelligence sharing, warning that without urgent action, the integrity of online survey-based researchand thus scientific validity and public trustface a growing risk of severe data contamination."
96,"H. Aljuaid, ""The Impact of Artificial Intelligence Tools on Academic Writing Instruction in Higher Education: A Systematic Review,"" Arab World English Journal (AWEJ) Special Issue on ChatGPT, pp. 1-30, Apr. 2024. [Online]. Available: https://ssrn.com/abstract=4814342","With the rapid development of Artificial Intelligence (AI) technologies, their integration into university academic writing courses has prompted examination of the extent to which AI tools might replace traditional instruction. This study synthesizes existing literature on the use of AI in academic writing pedagogy, evaluating both the potential enhancements in grammar and style and the ongoing concerns regarding creativity, critical thinking, and academic integrity. While some universities have updated policies to accommodate or restrict AI tool usagesuch as Stanfords revised integrity guidelines and Middlebury Colleges classroom bansfindings indicate that AI cannot fully replicate the comprehensive educational value of academic writing courses, which encompass critical thinking, research, citation, argumentation, creativity, originality, and ethics. The study concludes that, while AI tools may support academic writing, they are not poised to replace foundational writing courses; rather, a balanced pedagogical approach incorporating AI while preserving essential elements of writing education is recommended to best prepare students for a broad spectrum of writing demands."
97,"H. Guo and S. H. Zaini, ""Artificial Intelligence in Academic Writing: A Literature Review,"" Asian Pendidikan, vol. 4, no. 2, pp. 46-55, 2024. [Online]. Available: https://doi.org/10.53797/aspen.v4i2.6.2024","Artificial intelligence (AI) is revolutionizing academic writing by offering solutions to challenges such as plagiarism, language barriers, and feedback efficiency. This review critically examined eleven highly cited, open-access articles published since 2020, identified via Google and Scopus searches, that explore AI's role in academic writing. ChatGPT emerged as the most commonly used tool, applied in academic text generation, plagiarism detection, and supporting language learners. The review underscores both the opportunities for improved learning and the ethical concerns like plagiarism, content accuracy, and potential over-reliance on AI-generated content. It concluded that while AI can personalize feedback and improve writing outcomes, responsible implementation is crucial to prevent dependency and uphold academic integrity. Theoretical implications include AI's role in scaffolding learning for non-native speakers, while practical implications highlight the balance between leveraging AIs benefits and sustaining critical thinking skills; future research should further address ethics, accuracy, and best practices for integrating AI in academic contexts."
98,"M. M. Boillos and N. Idoiaga, ""Student perspectives on the use of AI-based language tools in academic writing,"" Journal of Writing Research, Early view, published Jan. 24, 2025. [Online]. Available: https://www.jowr.org/jowr/article/view/1518","Artificial intelligence-based Language Tools (AILTs) are increasingly prominent in higher education essay writing, supporting broader educational and multicultural goals and fostering research dissemination. This study surveyed 314 undergraduate and graduate education students to assess their perspectives, analyzing responses with the Reinert method. Students identified positive impacts of AILTs across text construction stagesplanning, textualization, and revisionwhile voicing concerns regarding academic integrity and potential negative effects on student competencies. These insights aim to inform educators in encouraging responsible and advantageous AILT use in academic settings."
99,"I. van Heerden and A. Bas, “Viewpoint: AI as Author – Bridging the Gap Between Machine Learning and Literary Theory,” Journal of Artificial Intelligence Research, vol. 71, pp. 1269–1277, 2021. doi:10.1613/jair.1.12593. Available: https://jair.org/index.php/jair/article/view/12593","Anticipating advances in Artificial Intelligence that enable the generation of original literary works, this study identifies a gap in understanding literarinessthe qualities that make a text literarywithin AI text generation research. The authors note the computational challenge of literature due to its figurative and ambiguous language, and they argue that insights from literary experts are often neglected. By fostering collaboration between machine learning practitioners and literary scholars, particularly during the evaluation phase of text generation, the study contends that literary theory can enhance both algorithm design and the understanding of how AI acquires and produces meaning and emotion in literary texts."
100,"K. C. Fraser, H. Dawkins, and S. Kiritchenko, “Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods,” Journal of Artificial Intelligence Research, vol. 82, pp. 1145–1187, Apr. 2025. doi:10.1613/jair.1.16665. Available: https://jair.org/index.php/jair/article/view/16665","This survey examines the critical and evolving task of AI-generated text (AIGT) detection as large language models (LLMs) produce increasingly indistinguishable text from human writing, with implications for fraud, academic integrity, and combating misinformation. It systematically reviews current detection strategies, including watermarking, statistical and stylistic analyses, and machine learningbased classification, presenting their respective strengths and challenges. The survey catalogs existing datasets (e.g., GPABench2, OUTFOX, LLMFake), highlighting that most are English-centric and generated via prompting rather than real-world samples. Detection effectiveness is influenced by factors such as LLM size, decoding strategies, language, document length, domain, human-mixed or paraphrased text, and adversarial attacks, with empirical results demonstrating detector performance variability. The authors argue for ensemble approaches and the need for continuous, in-domain dataset collection, emphasizing fairness, explainability, and the importance of addressing cross-lingual and out-of-domain generalization; they warn about detector bias (e.g., against non-native English users) and advocate for regulatory attention to watermarking and transparency. As LLMs advance, the report concludes that detection must become more adaptive, robust, and equitable to safeguard information integrity, and recommends research directions in uncertainty calibration, multimodal fusion, human-in-the-loop systems, and policy to manage this ongoing technological and societal shift."
101,"A. Dafoe, R. Sandbrink, C. O'Brien, S. Cotton-Barratt, J. Drexler, F. Flynn, J. Hannon, P. Kwon, L. Maynard, K. Redei, R. Salvatier, and M. Scharre, “When Will AI Exceed Human Performance? Evidence from AI Experts,” Journal of Artificial Intelligence Research, vol. 62, pp. 729–754, 2018. doi:10.1613/jair.1.11222. Available: https://jair.org/index.php/jair/article/view/11222","Advances in artificial intelligence (AI) are expected to transform various sectors including transportation, health, science, finance, and the military. This paper presents the results from a large survey of machine learning researchers regarding their expectations for progress in AI. The researchers predict that AI will outperform humans in tasks such as language translation by 2024, writing high-school essays by 2026, driving a truck by 2027, working in retail by 2031, writing a bestselling book by 2049, and performing surgery by 2053. The surveyed experts estimate a 50% probability that AI will surpass humans in all activities within 45 years and will automate all human jobs within 120 years, with Asian respondents anticipating these milestones significantly sooner than their North American counterparts. These findings are intended to guide policymakers and the research community in preparing for and managing the societal impact of advancing AI."
102,"Z. Shao, J. Zhang, H. Li, X. Huang, C. Zhou, Y. Wang, J. Gong, C. Li, and H. Chen, “Authorship style transfer with inverse transfer data augmentation,” AI Open, vol. 5, pp. 94–103, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651024000135","This paper addresses the problem of authorship style transfermodifying neutral text to reflect the stylistic signature of a specific individualby proposing a novel method called inverse transfer data augmentation (ITDA). Traditional approaches and even sophisticated Large Language Models (LLMs) like GPT-3.5 often struggle to accurately perform stylistic transformation for rare or out-of-distribution authors due to a lack of parallel (neutral, stylized) data and limited exposure during pre-training. ITDA overcomes this by leveraging LLMs' proficiency at neutralizing text: stylized samples are run through an LLM to strip away author-specific markers, creating (neutral, stylized) pairs. These pairs, which mimic parallel data, are then used to train a compact neural model that efficiently replicates the target authors style and is suitable for deployment, especially for less common or synthetic styles. Experimental results across four diverse authorship datasets show that ITDA-trained models consistently outperform both traditional style transfer methods and LLM-based baselinesincluding those using forward transfer with GPT-3.5on a variety of quality metrics. The paper highlights that while LLMs excel at frequent author styles, ITDA significantly boosts performance for rare or synthetic authors. The presented approach thus reframes author style transfer as a data augmentation problem, alleviating the need for costly parallel corpora and enabling robust, practical deployment. Datasets and code for ITDA are open-sourced to encourage further research (https://github.com/Vicky-Shao/ITDA)."
103,"X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, “GPT understands, too,” AI Open, vol. 4, pp. 30–36, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651023000141","Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performancee.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings."
104,"Y. Yang, H. Sun, J. Li, R. Liu, Y. Li, Y. Liu, Y. Gao, and H. Huang, “MindLLM: Lightweight large language model pre-training, evaluation and domain application,” AI Open, vol. 5, pp. 1–26, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666651024000111","Large Language Models (LLMs) have achieved outstanding results in various natural language tasks, advancing towards general AI, but their scale often entails high resource costs. This paper introduces MindLLM, a new series of bilingual and lightweight LLMs (with 1.3B and 3B parameters), trained from scratch to mitigate these costs while maintaining strong performance, sometimes matching or surpassing that of larger open-source models on public benchmarks. The authors detail the full development lifecycle, including data construction, architecture, evaluation, and real-world applications, aiming to provide a resource for the community. Notably, they present an instruction tuning framework optimized for smaller models, and demonstrate MindLLMs adaptability in specialized areas such as law and finance, highlighting its efficiency and versatility."
105,"M. Z. Degu and M. Meshesha, ""Fine‐Tuned Pretrained Transformer for Amharic News Headline Generation,"" Applied AI Letters, vol. 5, no. 4, e98, pp. 1-10, Jul. 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1002/ail2.98","Amharic, as an under-resourced language, faces challenges in news headline generation due to limited high-quality datasets. This study fine-tuned the t5-small checkpoint of the T5v1.1 model for Amharic news headline generation using a dataset of over 70,000 news articles paired with headlines. The process involved collecting data from Amharic news sites, cleaning text, optimizing article size with the TF-IDF algorithm, and developing a tokenizer with byte pair encoding (BPE) before model training. Evaluation using Rouge-L, BLEU, and Meteor metrics yielded respective scores of 0.5, 0.24, and 0.71 on a test set of 7,230 instances, markedly higher than the non-fine-tuned model (0.1, 0.03, and 0.14). Implementing rule-based postprocessing further improved the scores to 0.72, 0.52, and 0.81, outperforming previous abstractive Amharic summarization work (Rouge-L: 0.27). This demonstrates the practical value of fine-tuning and postprocessing, suggesting further gains through increased data, longer articles, adaptive postprocessing, and trial of other pretrained models."
106,"J. Conde, P. Reviriego, J. Salvachúa, G. Martínez, J. A. Hernández, and F. Lombardi, ""Understanding the Impact of Artificial Intelligence in Academic Writing: Metadata to the Rescue,"" Computer, vol. 57, no. 1, pp. 85–88, 2024. [Online]. Available: https://arxiv.org/abs/2502.16713","The paper argues that the increasing prevalence of generative AI tools in academic writing necessitates a systematic approach to understanding their impact, and the key step toward this is introducing standardized metadata describing AI usage in scientific publications. Currently, identifying if and how AI tools have assisted in academic writing is challenging, as there is no structured way to report what tools were used, on which parts of a paper, and for what tasks. The authors propose defining metadata fields such as the AI tool name, version, parameters, usage context (summarization, translation, etc.), and specific sections assisted (abstract, introduction, etc.). This structured metadata, which could be implemented in formats like JSON or XML, would enable large-scale, automated analysis of trends in AI adoption, linguistic shifts, geographic or disciplinary usage patterns, and correlations with citation metrics. Furthermore, it would facilitate building corpora for studying the linguistic characteristics of AI-assisted text and developing AI-detection tools. The authors conclude that implementing such metadata requires publishers and academic databases to add new fields, but doing so is vital for transparent analysis and a deeper understanding of AIs evolving influence on academic communication."
107,"D. J. Liebling, M. Kane, M. Grunde-Mclaughlin, I. J. Lang, S. Venugopalan, and M. P. Brenner, ""Towards AI-assisted Academic Writing,"" in Proceedings of the NAACL 2025 Workshop on AI for Scientific Discovery, 2025. [Online]. Available: https://arxiv.org/abs/2503.13771","This paper introduces an AI-assisted academic writing system focusing on two key features: contextual citation recommendation and introduction generation. For citation recommendations, the system leverages both local project citations (e.g., BibTeX) and a scalable, filtered copy of the OpenAlex scholarly corpus, employing SPECTER2 embeddings and a retrieval augmented generation (RAG) architecture to suggest relevant works. Evaluation on S2ORC datasets using Precision at $k$ and Mean Reciprocal Rank (MRR) demonstrated that ground-truth citations are ranked higher than distractors, with less difference in precision as distractor choice approaches the target context. Introduction generation is assessed on National Bureau of Economic Research papers, combining ROUGE metrics (average 29.9) and LLM-based entailment checks, with 47 out of 52 claims from generated texts entailed by original introductions. Qualitative interviews with academic authors highlight the value and challenges of citation validation, the importance of contextualization, and community norms. The system addresses citation bias and reference curation challenges but is limited to English-language sources; future work aims to broaden linguistic and methodological scope for more interpretable, diverse, and robust writing assistance. Overall, the results suggest that precise, context-aware AI tools can meaningfully augment academic authors workflows without replacing human expertise."
108,"R. Zheldibayeva, ""The impact of AI and peer feedback on research writing skills: a study using the CGScholar platform among Kazakhstani scholars,"" arXiv preprint arXiv:2503.05820, 2025. [Online]. Available: https://arxiv.org/abs/2503.05820","This study investigates how Kazakhstani scholars familiarity with AI tools and peer feedback processes influences their integration of feedback into academic writing, using the CGScholar platform. Thirty-six Bolashak scholars at the University of Illinois Urbana-Champaign participated in a survey featuring multiple-choice, Likert scale, and open-ended questions available in English and Russian, capturing demographic and experiential data. The findings reveal a moderate positive correlation between familiarity with AI tools and openness to adopting feedback-driven changes, as well as a strong positive correlation between research writing experience and expectations for peer feedback, particularly concerning research methodology. Although participants are receptive to AI-assisted feedback, they still highly value peer insights for methodological support. Consequently, integrating AI tools with traditional peer feedback shows promise for enhancing research writing quality. The study recommends further longitudinal and cross-cultural studies to assess the long-term and broader impacts of these feedback mechanisms in academia."
109,"H. Aljuaid, ""The Impact of Artificial Intelligence Tools on Academic Writing Instruction in Higher Education: A Systematic Review,"" OSF Preprints, 2024. [Online]. Available: https://osf.io/ph24v/download/","With the growth of Artificial Intelligence technologies, there is interest in studying their potential impact on university academic writing courses. This study examined whether AI tools are replacing these courses by exploring how they effectively replace traditional academic writing instruction and this shifts potential benefits and drawbacks. The researcher reviewed existing literature on integrating AI tools into academic writing instruction. The findings provide insights to educators navigating the integration of Artificial Intelligence tools into writing curricula while maintaining instructional quality and academic integrity standards. By synthesizing the latest research, this study can inform decisions about the appropriate use of Artificial Intelligence in teaching essential writing skills. Increased use of Artificial Intelligence writing tools has sparked debate about their role in academic writing instruction. Universities like Stanford have updated policies around Artificial Intelligence tool usage and academic integrity. The University of California issued guidance acknowledging the prevalence of generative Artificial Intelligence on campuses. Middlebury College banned classroom use of ChatGPT over concerns it could impede critical thinking and writing skill development. Results show that while Artificial Intelligence helps with grammar and style, questions remain about its impact on creativity and critical thinking. However, Artificial Intelligence is not replacing university writing courses. These courses teach critical thinking, research, citation, argumentation, creativity, originality, and ethics, which Artificial Intelligence lacks. Academic writing courses offer a complete learning experience. Artificial Intelligence may improve academic writing but is unlikely to replace traditional courses soon. A balanced approach integrating Artificial Intelligence support while preserving core elements of academic writing education appears most effective for preparing students for diverse writing challenges."
110,"S. L. Jen and A. R. Salam, ""Using Artificial Intelligence for Essay Writing,"" OSF Preprints, 2024. [Online]. Available: https://osf.io/vtcz9/download/?format=pdf","This study provides a systematic review focusing on the integration of artificial intelligence (AI) in essay writing over the past decade, aiming to offer insights for researchers and educators. While AI has had established roles in fields like medicine and engineering, its significant impact on education has become evident with the advent of generative tools such as ChatGPT. The paper identifies AI's roles in generating ideas, evaluating essays, storytelling, feedback provision, and even co-authoring student manuscripts, while noting a paucity of research on its direct effect on student essay writing performance. By summarizing available AI tools for teaching essay writing and identifying research gapsespecially the limited studies conducted in Malaysiathe study aspires to guide future research and assist language teachers in effectively leveraging AI for student-centred learning activities."
111,"M. H. Nguyen, ""Academic writing and AI: Day-1 experiment,"" OSF Preprints, 2023. [Online]. Available: https://osf.io/xgqu5/download","To assess AI's capability to accurately identify detailed scientific information, this study investigates how well AI can recognize scientific terms, their origins, meanings, and applications, as well as whether its accuracy improves over time."
112,"S. Yanes Luis, H. Li, S. Pan, X. Sun, X. Wei, and J. Yu, “Deep Reinforcement Multiagent Learning Framework for Information Contamination Event Detection,” Advanced Intelligent Systems, vol. 6, no. 2, article 2300059, Feb. 2024. [Online]. Available: https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202300850","The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristic. Simulation results indicate an improvement of up to 24% in terms of the mean absolute error with the proposed models. Also, training results with 1-3 agents indicate that our proposed approach returns 20% and 24% smaller average estimation errors for, respectively, monitoring water quality variables and monitoring algae blooms, as compared to state-of-the-art approaches."
113,"O. Erin, X. Liu, J. Ge, J. Opfermann, Y. Barnoy, L. O. Mair, J. U. Kang, and Y. Diaz-Mercado, “Comparative Analysis of Sensors in Rigid and Deformable Modular Robots for Shape Estimation,” Advanced Intelligent Systems, vol. 4, no. 6, article 2200072, Jun. 2022. [Online]. Available: https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202200072","Modular self-reconfigurable robots can alter their connection topology to create extensive networks of distributed sensors and actuators. This study evaluates the relative merits of commonly available connectivity and IR sensors on these modules, investigating the advantages deformable modular robots with embedded sensors bring to system shape estimation. The findings indicate that different sensing strategies involve trade-offs between system complexity and estimation accuracy."
114,"C. Delcea and N. Chirita, ""Exploring the Applications of Agent-Based Modeling in Transportation,"" Applied Sciences, vol. 13, no. 17, p. 9815, Aug. 2023. [Online]. Available: https://doi.org/10.3390/app13179815","Agent-based modeling (ABM) represents an innovative paradigm that offers an alternative to traditional equation-based modeling approaches, enabling the depiction of specific events or phenomena in a more flexible way. This method allows researchers to simulate systems composed of discrete, autonomous agents interacting within complex environments, making ABM particularly suited for applications where individual heterogeneity and interactions are central, such as in transportation systems. The cited work highlights the distinct advantages of ABM over conventional methods, emphasizing its capacity to capture emergent behaviors arising from local interactions that are often not adequately described by aggregate, equation-based models."
115,"A. Malas, S. El Falou, M. El Falou, and M. Hussein, ""Decentralized multi-agent approach based on A* algorithm for on-demand transport problem,"" Web Intelligence and Agent Systems: An International Journal, vol. 21, no. 1, pp. 1-17, 2023. [Online]. Available: https://dblp.org/rec/journals/wias/MalasFFH23","The paper addresses the need for popular acceptance of on-demand transport (ODT) systems, which offer significant societal, environmental, and economic advantages. It highlights reactivitydelivering fast, reliable service while respecting vehicle and user constraintsas a key to such acceptance. The authors propose a decentralized multi-agent method for modeling and solving the ODT problem on a static road network, with each agent employing the A* algorithm to identify optimal routes for each transport request. Optimality is defined as the fastest route serviced by the cheapest available vehicle. Using real-world data from a Lebanese city, the approach is experimentally evaluated, demonstrating its application and potential effectiveness."
116,"S. Kherchaoui and A. Houacine, ""Identification of micro expressions in a video sequence by Euclidean distance of the facial contours,"" Web Intelligence and Agent Systems: An International Journal, vol. 21, no. 3, pp. 211-221, 2023. [Online]. Available: https://dblp.org/rec/journals/wias/KherchaouiH23",This paper presents an automatic facial micro-expression recognition system (FMER) that processes video sequences. The system identifies and classifies micro-expressions based on facial contours using the Euclidean distance as the primary metric.
117,"J. Fei, L. Spadaro, D. M. T. Farias, S. Komoroske, and R. Stehle, ""Automated chat application surveys using WhatsApp,"" SocArXiv Preprints, 2022. [Online]. Available: https://osf.io/j9a2y/","This paper introduces an automated method for conducting surveys over WhatsApp, leveraging WhatsApp Business API, Twilio, and Google Sheets to streamline survey administration, message flow, and data management. Designed especially for mobile and hard-to-reach populationsdemonstrated through case studies with refugees in Colombia and the USthe approach enables surveys to be completed in a familiar chat interface, supports branching logic, reminders, and error handling, and yields lower respondent costs and higher completion rates compared to traditional SMS or IVR methods. Results from the case studies show the system's viability for collecting longitudinal data with sustained engagement, although challenges include technical setup, WhatsApp API constraints, message delivery issues, and sample attrition. The authors provide actionable documentation and code to support adoption and discuss further work to compare data quality, optimize workflows, and expand functionalities (like automated translation and randomization). Overall, automated WhatsApp surveys offer a scalable, cost-effective alternative for research with mobile or transient groups."
