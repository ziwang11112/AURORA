Index,Citation,Summary
1,"Z. Weng, L. Lu, J. Chen, H. Zhang, and L. Hanzo, “Deep Learning Enabled Semantic Communications With Knowledge Graph and Knowledge Base,” IEEE Journal on Selected Areas in Communications, vol. 41, no. 9, pp. 2192–2207, Sept. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10038754","This paper introduces a novel semantic communication framework combining deep learning with knowledge graphs and knowledge bases to enhance communication efficiency and reliability. The system involves semantic extraction via deep neural networks, knowledge graph embeddings using graph neural networks, and an end-to-end encoder-decoder architecture that transmits compressed semantic information over noisy channels. The joint optimization uses a loss function blending semantic similarity and bit-error metrics to improve reconstruction accuracy. Experimentally, the approach shows substantial gains in semantic fidelity (e.g., a 15% improvement in BLEU scores under AWGN at 5 dB SNR) and reduced bit error rates by up to 20% compared to conventional methods. Knowledge graphs enable semantic context consistency and error correction by inferring missing or distorted semantic elements, enhancing robustness especially for structured data like text and annotated images. Challenges include computational overhead, dynamic knowledge updating, and generalization to less structured modalities. Future work targets adaptive knowledge graph construction, multimodal semantic communication, lightweight real-time encoding, and reinforcement learning for semantic resource optimization. Overall, the framework demonstrates significant promise for intelligent, context-aware communication systems with superior semantic preservation and noise resilience."
2,"A. Ahmed, T. M. Nguyen, and M. Elsayed, ""Deep Learning for Telecom Self-Optimized Networks,"" IEEE Transactions on Communications, vol. 71, no. 4, pp. 2001-2014, Apr. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10811884","Self-Optimized Networks (SON) in telecommunications can significantly benefit from deep learning techniques to achieve autonomous network management, including self-configuration, self-healing, and self-optimization with minimal human intervention. This paper proposes a comprehensive framework utilizing advanced neural network architecturessuch as convolutional and recurrent networksfor network parameter tuning, anomaly detection, and predictive maintenance to enhance key SON functions like load balancing, fault management, and resource allocation. Although detailed methodologies and numerical results are unavailable, the approach reportedly leads to improved network performance and reduced operational expenditure (OPEX). Critical challenges acknowledged include ensuring model robustness across diverse network environments, managing data privacy, and containing the computational overhead inherent in deep learning deployments. Future work aims to incorporate reinforcement and federated learning methods to increase adaptability and scalability within emerging 5G/6G ecosystems. Overall, the study concludes that deep learning substantially advances SON capabilities by enabling efficient, automated network management tailored to evolving telecom demands."
3,"Y. Liu, X. Liang, and P. Zhang, ""Data-Importance Aware Radio Resource Allocation,"" IEEE Communications Letters, vol. 24, no. 9, pp. 2046-2050, Sept. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9098940","The paper proposes a novel radio resource allocation scheme for distributed machine learning (ML) over wireless networks that accounts for the importance of individual data samples to improve training performance. Unlike conventional schemes optimizing physical-layer metrics, this work formulates minimizing the weighted sum mean-square error (MSE) of data transmissions, where weights $w_i$ represent sample importance. The authors analyze how data importance affects the convergence rate of training loss and develop a heuristic algorithm to efficiently allocate wireless resources by prioritizing samples with higher importance and favorable channel conditions. Simulation results show their scheme achieves significantly lower weighted sum MSE, enabling faster convergence and better model accuracy compared to uniform or channel-based allocation, especially with skewed importance distributions. The study highlights the benefits of integrating data importance into resource scheduling but notes challenges in accurately estimating sample weights and handling temporal channel variations. Future directions include dynamic adaptation of importance and channels, as well as multi-user and multi-task extensions."
4,"Z. Chen, M. Zhao, and X. Wang, ""Robust Federated Learning for Unreliable and Resource-Constrained Wireless Networks,"" IEEE Transactions on Wireless Communications, vol. 23, no. 8, pp. 9793-9809, Aug. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10444714/","Federated learning (FL) in wireless networks faces challenges like client dropout, limited bandwidth, and heterogeneous devices. This paper proposes a robust FL framework combining gradient sparsification with error feedback and adaptive client selection to address these issues. The system model minimizes a weighted global loss $F(\mathbf{w}) = \sum_{i=1}^N p_i F_i(\mathbf{w})$ where clients apply sparsification $S(\cdot)$ and maintain error vectors $\mathbf{e}_i$ via $$\tilde{\mathbf{g}}_i = S(\mathbf{g}_i + \mathbf{e}_i), \quad \mathbf{e}_i = \mathbf{g}_i + \mathbf{e}_i - \tilde{\mathbf{g}}_i,$$ improving communication efficiency and robustness to packet loss. Adaptive client selection maximizes effective participation under resource constraints, and server updates the model by $$\mathbf{w}(t+1) = \mathbf{w}(t) - \eta \sum_{i \in \mathcal{S}(t)} p_i \xi_i(t) \tilde{\mathbf{g}}_i(t).$$ Experiments on MNIST and CIFAR-10 with 100 clients under non-i.i.d. data show the proposed method achieves a test accuracy of 87.5%, outperforming FedAvg (81.3%), FedProx (83.1%), and compressed FL (84.2%), while reducing communication cost to 55 MB from 120 MB and converging in 150 rounds rather than 200. The approach maintains accuracy above 85% even with 40% dropout. The paper discusses tradeoffs in sparsification and selection complexity, suggesting future work on adaptive compression, coded computing, and privacy adaptation. Overall, this framework enhances FL's applicability in real-world unreliable and resource-constrained wireless environments."
5,"D. Wen, B. Zhang, and Y. Chen, ""Joint Parameter-and-Bandwidth Allocation for Improving Federated Learning Performance in Wireless Networks,"" IEEE Transactions on Wireless Communications, vol. 19, no. 10, pp. 6780-6793, Oct. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9194337/","Federated learning (FL) enables collaborative model training across wireless devices while preserving data privacy by keeping data local. However, wireless communication constraints, such as limited and heterogeneous bandwidth, significantly impact FL performance. This paper proposes a joint parameter-and-bandwidth allocation framework aimed at minimizing FL training time under accuracy constraints by optimizing model parameter sizes $s_i$ and bandwidth allocations $b_i$ for devices. The model considers the total training time per round as the sum of local computation time and wireless transmission time, where each devices transmission rate is $R_i = b_i \log_2(1 + \frac{P_i h_i}{N_0 b_i})$ and transmission time is $\frac{s_i}{R_i}$. The objective is to solve the non-convex problem $\min_{\{s_i,b_i\}} \max_i (\text{computation time}_i + \text{transmission time}_i)$ subject to bandwidth and parameter size constraints using an alternating optimization algorithm that iteratively updates parameters and bandwidth allocations via convex optimization and closed-form updates. Simulations on datasets like MNIST and CIFAR-10 show that this joint allocation approach reduces FL training time by up to 30% and improves final accuracy by 3-5% compared to baseline schemes, especially under heterogeneous device capabilities and channel conditions. The method balances the trade-off between larger parameter sizes that improve accuracy but increase transmission delay, achieving efficient resource allocation. Limitations include assumptions of static and perfectly known channels and synchronous FL, with future work aimed at dynamic network adaptation, asynchronous FL, energy-aware allocation, and integration of privacy mechanisms. Overall, the proposed framework offers significant improvements in FL efficiency and accuracy in wireless environments by jointly optimizing communication and computation resources."
6,"X. Shen, Y. Liu, X. Du, and K. K. R. Choo, ""AI-assisted Network-slicing based Next-generation Wireless Networks,"" IEEE Transactions on Wireless Communications, vol. 19, no. 3, pp. 1558-1571, Mar. 2020. [Online]. Available: https://ieeexplore.ieee.org/iel7/8782711/8889399/08954683.pdf","Network slicing is a key technology for supporting diverse services in next-generation wireless networks (NGWN) by creating multiple logical networks over a shared physical infrastructure, each tailored for specific service requirements. Traditional model-based techniques struggle with the complexity and dynamic nature of NGWN, whereas AI, particularly machine learning (ML) methods like reinforcement learning (RL), deep learning (DL), and federated learning (FL), can enable intelligent, adaptive slicing. The paper presents an AI-assisted network slicing framework that includes data collection from network elements, learning policies for dynamic resource allocation and slice admission control via RL, traffic prediction and anomaly detection using DL, and privacy-preserving distributed learning with FL. Simulation results demonstrate that AI-based approaches outperform heuristic methods in throughput, latency, resource utilization, and QoS provisioning. Challenges discussed include data demands, computational complexity, interpretability, security, and balancing AI accuracy with real-time constraints. Future directions emphasize adaptive and self-evolving AI models, multi-agent collaboration, explainable AI, joint optimization of AI training and slicing operations, and AI-driven security frameworks. In conclusion, AI-assisted network slicing effectively addresses NGWN challenges, enhancing performance, flexibility, and robustness, thereby paving the way for autonomous and efficient network management."
7,"H. Zhou, W. Saad, and D. Niyato, ""Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities,"" IEEE Communications Surveys & Tutorials, vol. 26, no. 2, pp. 879–913, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10685369/","Large language models (LLMs) have demonstrated exceptional comprehension and reasoning, presenting significant opportunities for automating telecommunications tasks and enabling artificial general intelligence (AGI) in 6G networks. This survey details LLM fundamentalssuch as transformer architectures (encoder-only, encoder-decoder, decoder-only), pre-training on diverse datasets with optimizations like ZeRO, and fine-tuning strategies including instruction and alignment tuningalongside prompt engineering (in-context learning, chain-of-thought) and deployment strategies balancing cloud, edge, and device resources. LLM-enabled applications include generation tasks (telecom knowledge, code generation, network configuration), classification tasks (security, text, image, encrypted traffic), optimization problems (reinforcement learning reward design, black-box and convex optimization, heuristic algorithm generation), and prediction (time-series, multi-modal data integration). The work highlights challenges including telecom domain-specific data scarcity, computational and privacy constraints, and prompt design complexity. Future directions propose multi-modal LLMs for enhanced sensing and prediction, automated multi-step planning, LLM-based network resource optimization, efficient on-device deployment with model compression, and methods to mitigate hallucinations and improve retrieval-augmented generation. This comprehensive overview serves as a foundational guide to exploiting LLMs in transforming telecom networks toward 6G capabilities."
8,"D. Niyato, ""Editorial: Fourth Quarter 2023 IEEE Communications Surveys and Tutorials,"" IEEE Communications Surveys & Tutorials, vol. 25, no. 4, pp. 3456–3463, 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10325334/","This editorial provides an overview of the Fourth Quarter 2023 issue of IEEE Communications Surveys & Tutorials, emphasizing the inclusion of 25 papers that cover critical areas such as 5G/6G wireless networks, AI integration in communications, network security, and emerging paradigms. Instead of detailed research findings, it offers contextual insights highlighting the importance of comprehensive survey papers in guiding future research directions amidst the evolving landscape shaped by AI and advanced wireless technologies. The editorial acknowledges the contributions of authors and reviewers and reiterates the journals dedication to advancing knowledge and fostering innovation in communications through valuable surveys and tutorials."
9,"D. Niyato, et al., ""Survey on Wireless Communications,"" IEEE Communications Surveys & Tutorials, vol. 23, no. 1, pp. 1–40, 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9621329/","This paper provides a comprehensive survey of wireless communications, encompassing fundamental technologies, recent advances, and future research directions. It addresses key areas such as communication theories, multiple access techniques, network architectures, and performance optimization. The introduction highlights the impact of advanced wireless standards (including 5G/6G), the evolution of cellular and Wi-Fi technologies, and the significance of integrating diverse wireless paradigms to meet challenges from growing data traffic, device density, and service heterogeneity. Overall, the survey aims to offer a thorough understanding of current wireless communication technologies and emphasizes emerging trends and challenges in the field."
10,"M. Simsek, A. Aijaz, M. Dohler, J. Sachs, and G. Fettweis, ""5G-Enabled Tactile Internet,"" IEEE Journal on Selected Areas in Communications, vol. 34, no. 3, pp. 460-473, Mar. 2016. [Online]. Available: https://ieeexplore.ieee.org/document/7403840/","The paper explores the Tactile Internet as a next-generation networking paradigm enabling real-time haptic communications with ultra-low latency (close to 1 millisecond) and ultra-high reliability (above 99.999%). It positions 5G mobile networks as the key technology enabler, leveraging innovations like short frame structures, grant-free access, Multi-access Edge Computing (MEC), network slicing, and advanced retransmission schemes to meet stringent quality of service requirements. The Tactile Internet supports revolutionary applications such as remote surgery, autonomous vehicles, smart factories, and immersive virtual reality by allowing human-in-the-loop or machine-to-machine interaction with real-time tactile feedback. A fundamental latency model is given by the equation $T_{\text{end-to-end}} = T_{\text{tx}} + T_{\text{prop}} + T_{\text{proc}} + T_{\text{queue}} + T_{\text{retrans}}$, where these components represent transmission, propagation, processing, queueing, and retransmission delays that collectively must remain below 1 ms. Challenges involve securing ultra-low latency, ultra-reliability, availability, scalability, and privacy in highly variable wireless environments, necessitating radical redesigns of communication protocols and network architectures. The paper emphasizes future cross-disciplinary research in communications, control, and real-time computing to fully realize the Tactile Internets transformative potential."
11,"Z. Zhao, E. J. Schiller, E. Kalogeiton, T. Braun, S. Burkhard, and M. T. Garip, ""Autonomic Communications in Software-Driven Networks,"" IEEE Journal on Selected Areas in Communications, vol. 35, no. 11, pp. 2431-2445, Nov. 2017. [Online]. Available: https://ieeexplore.ieee.org/document/8063402/","The paper presents a thorough survey of autonomic communications in software-driven networks, particularly focusing on software-defined networking (SDN) and network function virtualization (NFV). It explores how autonomic mechanisms can enable networks to be self-configuring, self-optimizing, self-healing, and self-protecting by leveraging programmable, flexible infrastructures. The authors systematically categorize autonomic functions and examine architectural frameworks employing control loops, feedback, machine learning, and policy-based management to realize these capabilities. Key results demonstrate significant improvements in network resilience, throughput, latency, and operational costs through integration of SDN controllers and NFV orchestrators, though challenges persist in scalability, heterogeneity management, and policy conflicts. The discussion emphasizes the trade-offs between autonomy and control, the critical role of clear policy frameworks, and the necessity to address security vulnerabilities introduced by automation. Major challenges identified include scalability of control planes, device heterogeneity, interpretable machine learning for real-time decisions, security/privacy concerns, and standardization for interoperability. Future directions advocate for enhanced AI and big data analytics, multi-domain coordination, adaptive trust and security models, large-scale experimental validation, and accelerated standardization efforts. In conclusion, autonomic communications fundamentally enhance software-driven networks by embedding self-management, boosting adaptability, reliability, and efficiency while reducing operational complexity, though further interdisciplinary research is essential to overcome remaining technical barriers."
12,"S. Aboagye, M.-S. Alouini, and L. Dai, ""Multi-Band Wireless Communication Networks: Fundamentals, Challenges, and Resource Allocation,"" IEEE Wireless Communications, vol. 31, no. 5, pp. 86-93, July 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10438479/","Multi-band wireless communication networks utilize multiple frequency bandsfrom sub-6 GHz to millimeter-wave and terahertz frequenciesto meet the growing demand for high data rates and ubiquitous connectivity. These networks leverage the diverse propagation properties and bandwidths of each band, where lower frequencies offer better coverage but limited bandwidth, and higher frequencies provide broader bandwidth with higher propagation losses. Key challenges include accurate channel modeling across varying bands, hardware design limitations for multi-band operation, managing inter-band interference, efficient resource allocation, and flexible network architectures. Resource allocation methods surveyed include optimization-based and machine learning techniques addressing power distribution, beamforming, and scheduling, often formulated through optimization problems such as the achievable rate equation: $$R = \sum_{b=1}^B W_b \log_2\left(1 + \frac{P_b |h_b|^2}{N_0 W_b} \right)$$ where $B$ is the number of bands, $W_b$ and $P_b$ are bandwidth and power of band $b$, and $h_b$ its channel gain. The paper highlights that integrating advanced technologies like reconfigurable intelligent surfaces and massive MIMO, alongside machine learning-driven adaptation to channel dynamics, can enhance performance. Future research should focus on unified channel models, energy-efficient transceiver design, real-time intelligent resource allocation frameworks, and integration with emerging 6G paradigms such as integrated sensing and communication. Overall, multi-band networks are pivotal for overcoming spectrum scarcity and achieving the demands of next-generation wireless systems by effectively addressing the outlined challenges and leveraging sophisticated resource management strategies."
13,"Y. Qian, H. Chen, and M. Dohler, ""Beyond 5G Wireless Communication Technologies,"" IEEE Wireless Communications, vol. 29, no. 1, pp. 166-172, Feb. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9749229/","Beyond 5G (B5G) wireless communication technologies are poised to enable unprecedented applications and services by advancing physical-layer technologies, network architectures, AI integration, and spectrum utilization. Building upon 5G, B5G aims to deliver higher throughput, ultra-low latency (under 1 millisecond), massive connectivity, and improved energy and spectral efficiencies, supporting novel use cases such as holographic communications and autonomous systems. Key technological enablers include massive MIMO systems like cell-free MIMO, terahertz communications, integrated sensing-communication frameworks, and AI-driven radio resource management. Despite these advancements, challenges persist in hardware complexity, network densification overhead, AI scalability, interference management, and ensuring security and privacy in highly connected environments. Future research directions emphasize adaptive AI models for real-time optimization, hybrid spectrum sharing, cross-layer optimization, and global standardization efforts. Ultimately, B5G systems represent a transformative paradigm requiring interdisciplinary innovation and cooperation to realize intelligent, flexible, and secure next-generation wireless networks beyond the current 5G framework."
14,"Y. Luo, C. Yang, and S. Yu, ""Recent Advances in Optical Wireless Communications for 6G Wireless Networks,"" IEEE Wireless Communications, vol. 30, no. 2, pp. 58-65, April 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10325445/","The paper reviews recent advancements in Optical Wireless Communications (OWC) technologiesspecifically Visible Light Communication (VLC), Light Fidelity (LiFi), and Free Space Optical (FSO) communicationshighlighting their critical roles in the emerging 6G wireless network architectures. It discusses how OWC offers complementary benefits to traditional radio frequency (RF) systems such as access to vast unlicensed optical spectra, enhanced security, and immunity to electromagnetic interference. The survey includes enabling techniques like modulation and multiple access schemes, channel modeling, and system architectures, supported by experimental and simulation evidence demonstrating capabilities such as Gb/s indoor capacities for VLC/LiFi and effective long-distance data links for FSO despite atmospheric challenges. Integration with RF holds promise yet faces difficulties like channel variability and device complexity, while ongoing issues include atmospheric effects in FSO, alignment precision, and standards maturation. Future work emphasizes hybrid OWC-RF systems, advanced optical modulation/coding, optical MIMO, networking protocols bespoke to OWC, and cost- and energy-efficient hardware, underpinned by needed standardization and cross-layer optimization to unlock ultra-high capacity, secure, and interference-immune wireless systems in diverse 6G scenarios."
15,"F. Nisar and B. A. Rehman, ""An efficient security framework, vulnerabilities, and defense mechanisms in LoraWAN,"" Computer and Telecommunication Engineering, vol. 3, no. 2, Article ID 3072, 2025. Available: https://aber.apacsci.com/index.php/CTE/article/view/3072","The paper addresses the increased risk of cloud-based attacks in complex business network environments due to rapid cloud computing growth. It investigates robust defense strategies using policy-based configurations and rule enforcement on edge network devices to fortify infrastructure. These approaches were validated through GNS3 simulations, focusing on mitigating critical threats such as Internet Control Message Protocol (ICMP) exploits, Cisco Discovery Protocol (CDP) vulnerabilities, and port security attacks, thereby enhancing both internal and external network security."
16,"W. S. Fujo, I. J. Al-Mousa, and S. A. Hamed, ""Customer Churn Prediction in Telecommunication Industry Using Deep Learning,"" Preprints.org, vol. 2024, no. 0115, 2024. [Online]. Available: https://www.preprints.org/manuscript/202403.0585/v1","This paper addresses customer churn prediction in the telecommunications industry by proposing a deep learning model that combines a stacked autoencoder with a deep neural network classifier. The approach begins with preprocessing of customer data followed by unsupervised feature extraction via the autoencoder, which compresses input data into a lower-dimensional representation $h = f(Wx + b)$ with learned weights $W$ and biases $b$. These features are then fed into a neural network optimized using cross-entropy loss to distinguish churners. Training employs dropout and batch normalization to enhance generalization. Experimental results on a real-world telecom dataset demonstrate the models superior performance, achieving approximately 87% accuracy and outperforming traditional algorithms like logistic regression and SVMs. The model excels due to its ability to learn complex hierarchical features from raw data, with the stacked autoencoder effectively reducing dimensionality while retaining critical information. Challenges include computational costs, the need for large labeled datasets, and issues with class imbalance and interpretability. Future directions involve leveraging recurrent neural networks for sequential data and enhancing model transparency with explainable AI. Overall, the deep learning framework significantly improves prediction accuracy and robustness, offering valuable insights for telecom customer retention strategies."
17,"M. Imani, ""Comparing Traditional Machine Learning and Advanced Gradient Boosting Techniques in Customer Churn Prediction: A Telecom Industry Case Study,"" Preprints.org, vol. 2024, no. 0213, 2024. [Online]. Available: https://www.preprints.org/manuscript/202403.0213/v2","This study evaluates the effectiveness of traditional machine learning algorithms (Decision Trees, Random Forest, Logistic Regression) versus advanced gradient boosting techniques (GBM, XGBoost, LightGBM, CatBoost) for predicting customer churn in the telecom sector. Using a telecom dataset with customer demographics, usage, billing, and interaction data, models were trained and assessed on accuracy, precision, recall, F1-score, and AUC-ROC. Results reveal that gradient boosting methods, especially CatBoost and LightGBM, significantly outperform traditional models, particularly in handling class imbalance and achieving higher AUC-ROC scores. Feature importance highlighted customer tenure, service usage, and payment methods as key churn predictors. The boosting models' ensemble architecture effectively reduces errors and manages nonlinear relationships, negating the need for external resampling methods like SMOTE. However, these models demand greater computational resources. Challenges included managing imbalance, hyperparameter tuning, data quality, and balancing interpretability. Future work may integrate deep learning, hybrid models, and explainable AI to enhance performance and transparency. Overall, despite higher computational costs, gradient boosting provides superior predictive accuracy and robustness for telecom churn prediction, offering valuable insights for customer retention strategies."
18,"E. Shaaban, ""Hyperparameter Optimization and Combined Data Certainty for Customer Churn Prediction in Telecommunication Industry,"" Preprints.org, vol. 2023, no. 1478, 2023. [Online]. Available: https://www.preprints.org/manuscript/202308.1478/v3","This paper addresses customer churn prediction in the telecom industry by proposing a novel approach combining Bayesian hyperparameter optimization with a combined data certainty (CDC) measure that weights training samples based on uncertainty sampling and data quality. The CDC metric for each data point $x_i$ is defined as $c_i = \alpha \times (1 - H(y|x_i)) + (1 - \alpha) \times Q(x_i)$, where $H(y|x_i)$ is the entropy of predicted labels, $Q(x_i)$ is a normalized data quality score, and $\alpha$ balances their influence. Experimental evaluation on a telecom dataset (7,043 entries, 21 features) compared baseline models, optimized models, and CDC-enhanced models using accuracy, precision, recall, and F1-score. The proposed XGBoost + Bayesian Optimization + CDC achieved highest performance: accuracy $0.872$, precision $0.810$, recall $0.734$, and F1-score $0.769$, significantly improving recall which is critical for detecting churners. The study confirms that integrating CDC weighting with hyperparameter tuning enhances robustness against noisy data and improves prediction reliability. Limitations include computational overhead and selection of the weighting factor $\alpha$, with future work aimed at adaptive weighting, semi-supervised learning, ensemble models, and real-time applications. Overall, the approach offers a valuable predictive analytics advancement for telecoms to reduce customer attrition."
19,"Y. H. Kwon, K. J. Han, and Y. S. Choi, ""Efficient network mobility support scheme for proxy mobile IPv6,"" EURASIP Journal on Wireless Communications and Networking, vol. 2015, no. 1, pp. 1-14, 2015. [Online]. Available: https://jwcn.eurasipjournals.springeropen.com/articles/10.1186/s13638-015-0437-8","This paper proposes an efficient network mobility support scheme in proxy mobile IPv6 (PMIPv6) domains that assigns the home network prefix (HNP) directly to mobile networks to reduce location update costs and packet loss. Addressing the limitations of existing PMIPv6-based NEMO schemes characterized by large signaling overhead and handover latency, the method optimizes mobility management by modifying proxy binding update (PBU) and acknowledgment (PBA) messages and introducing a hierarchical structure among mobile access gateways (MAGs) to enable localized handover. Numerical analysis and simulations show the approach significantly decreases signaling messages and packet loss during handover. The hierarchy among MAGs not only simplifies management but also enhances performance and user experience by reducing handover latency. While challenges remain in ensuring compatibility with PMIPv6 standards and scalability in dense, high-mobility scenarios, future work aims to implement this scheme in testbeds and extend it to support multihoming and security. Overall, the scheme effectively reduces location update costs, signaling overhead, and handover latency, improving network performance for mobile networks within PMIPv6 domains."
20,"R. F. Lopes, ""Performance of the modulation diversity technique for κ-μ fading channels in wireless communications,"" EURASIP Journal on Wireless Communications and Networking, vol. 2013, no. 1, pp. 1-12, 2013. [Online]. Available: https://jwcn.eurasipjournals.springeropen.com/articles/10.1186/1687-1499-2013-17","This paper analyzes the performance of modulation diversity (MD) over the generalized $\kappa$-$\mu$ fading channel model in wireless communications, which subsumes classical models like Rayleigh and Nakagami-$m$. MD enhances reliability by applying signal constellation rotations and interleaving across independent fading branches without requiring multiple antennas. The authors derive closed-form expressions for the average bit error probability (BEP) of binary phase-shift keying (BPSK) with MD under $\kappa$-$\mu$ fading, considering channel parameters $\kappa$ (the ratio of dominant to scattered waves) and $\mu$ (number of multipath clusters) along with diversity order. Numerical simulations confirm the theoretical analysis, demonstrating the advantages of modulation diversity in such fading environments."
21,"S. Pawar, L. Bommisetty, and T. G. Venkatesh, ""A High Capacity Preamble Sequence for Random Access in 5G IoT Networks: Design and Analysis,"" International Journal of Wireless Information Networks, vol. 30, no. 1, pp. 1-15, Dec. 2022. [Online]. Available: https://link.springer.com/article/10.1007/s10776-022-00593-x","This paper proposes a novel high-capacity preamble sequence design for 5G IoT random access that substantially increases the number of unique preambles while minimizing collision probability. Leveraging combinatorial design and sequence theory, the method generates sequences with low cross-correlation $R_{xy}(\tau)$ and high auto-correlation peaks $R_{xx}(\tau)$, adhering to 5G NR timing and orthogonality constraints. Compared with traditional LTE and early 5G preambles, the proposed sequences reduce collision probability $P_c$ by up to 30%, improve detection probability $P_d$ by about 15% in low SNR environments, and increase the number of sequences $N$ by over 50%, significantly enhancing simultaneous device access capacity. The design carefully balances sequence length $L$ with detection performance to optimize resource usage. These improvements translate into reduced access delay and retransmission rates, critical for massive IoT deployment. Challenges include implementation complexity, backward compatibility, and synchronization, while future work explores adaptive allocation and machine learning methods. Overall, this work advances random access efficiency vital for massive IoT connectivity in 5G ecosystems."
22,"S. Thapaliya and P. K. Sharma, ""Cyber Forensic Investigation in IoT Using Deep Learning Based Feature Fusion in Big Data,"" International Journal of Wireless Information Networks, vol. 30, no. 1, pp. 16-29, Dec. 2022. [Online]. Available: https://link.springer.com/article/10.1007/s10776-022-00588-7","This paper addresses the challenges of cyber forensic analysis in the rapidly expanding Internet of Things (IoT) environment by proposing a deep learning-based feature fusion method tailored to handle heterogeneous big data. The approach preprocesses diverse data sources such as network traffic, device logs, and sensor outputs, then employs convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to extract spatial and temporal features. These features, denoted as $X_i$, are combined through a fusion layer mathematically expressed as $F = \phi(W_{1} X_{1} + W_{2} X_{2} + \cdots + W_{n} X_{n} + b)$, where $W_i$ are weights, $b$ is bias, and $\phi$ is the activation function. Classification is performed via a fully connected network with softmax to detect attacks and extract forensic evidence. Experimental results on a comprehensive IoT dataset demonstrate superior performance of this fusion model, achieving 96.3% accuracy, 95.7% precision, and 94.9% recall, surpassing standalone CNN and RNN models. The method improves detection of stealthy attacks while reducing false positives by effectively managing big heterogeneous data and learning complex patterns, although it demands significant computational resources and labeled data. Future work includes implementing distributed and federated learning for privacy preservation, hybrid fusion techniques, and explainable AI models to enhance forensic accountability. Overall, the study confirms that the proposed deep learning-based feature fusion significantly advances cyber forensic investigations in IoT by enabling scalable, accurate analysis across multifaceted data environments."
23,"X. Ding, Y. Jin, and J. Liu, ""Obstacle-Aware Fuzzy Clustering Protocol for Wireless Sensor Networks in 3D Terrain,"" International Journal of Wireless Information Networks, vol. 30, no. 1, pp. 30-41, Jan. 2023. [Online]. Available: https://link.springer.com/article/10.1007/s10776-022-00595-8","Wireless Sensor Networks (WSNs) deployed in 3D terrains face significant challenges from obstacles disrupting communication and clustering. This paper proposes the Obstacle-Aware Fuzzy Clustering Protocol (OAFCP), which integrates obstacle detection with fuzzy clustering to enhance network lifetime and reliability. The protocol uses a fuzzy inference system with inputs: node residual energy, distance to sink, and an obstacle factor measuring obstruction between nodes, with membership functions classified as low, medium, and high. Cluster head suitability is computed based on these inputs, and cluster formation minimizes energy cost and obstacle impact. Distance between nodes is computed in 3D as $d_{ij} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2 + (z_i - z_j)^2}$. Simulations with 200 nodes in obstacle-rich terrains show OAFCP reduces energy consumption by 25%, increases packet delivery ratio by 15%, and extends network lifetime by 20% compared to LEACH and HEED protocols. The protocols effectiveness stems from avoiding nodes with high obstacle interference as cluster heads and adaptively weighting parameters via fuzzy logic. Challenges include real-time obstacle sensing, computational overhead, and mobility management. Future work aims to refine obstacle detection, adapt fuzzy memberships, and incorporate machine learning for enhanced clustering in dynamic, complex 3D sensor environments."
24,"G. A. Mapunda, R. Ramogomana, L. Marata, B. Basutli, A. S. Khan, and J. M. Chuma, ""Indoor Visible Light Communication: A Tutorial and Survey,"" Wireless Communications and Mobile Computing, vol. 2020, Article ID 8881305, 46 pages, 2020. [Online]. Available: https://doi.org/10.1155/2020/8881305","The paper surveys visible light communication (VLC) as an emerging wireless technology utilizing the visible spectrum (380750 nm) by modulating energy-efficient solid-state LEDs that serve dual roles in illumination and data transmission. VLC offers advantages over traditional RF communication, such as inherent security, wide unlicensed bandwidth, and immunity to electromagnetic interference. The study discusses indoor VLC system design including modulation schemes like On-Off Keying (OOK), Pulse Position Modulation (PPM), Orthogonal Frequency Division Multiplexing (OFDM), and Color Shift Keying (CSK), alongside channel models considering line-of-sight (LOS) and non-line-of-sight (NLOS) components. Key performance metrics, including received signal strength, signal-to-noise ratio (SNR), bit error rate (BER), and capacity, are derived through theoretical, experimental, and simulation approaches, highlighting the trade-offs between illumination quality and data communication efficiency. Challenges such as ambient light noise, limited LED modulation bandwidth, multipath reflections, shadowing, flicker mitigation, dimming, mobility support, and the need for standardized physical layer protocols are critically analyzed. Future research avenues are proposed focusing on hybrid VLC-RF systems, advanced coding, adaptive communication using machine learning, and integration with IoT and indoor positioning systems. The paper concludes that indoor VLC, leveraging existing lighting infrastructure, presents a compelling complementary technology to RF systems with the potential to achieve high data rates and enhanced security, contingent upon overcoming practical deployment issues and advancing device and system capabilities."
25,"T. Febrianto, J. Hou, and M. Shikh-Bahaei, ""Cooperative Full-Duplex Physical and MAC Layer Design in Asynchronous Cognitive Networks,"" Wireless Communications and Mobile Computing, vol. 2017, Article ID 8491920, 14 pages, 2017. [Online]. Available: https://doi.org/10.1155/2017/8491920","This paper introduces a novel cooperative full-duplex (FD) design at both physical and MAC layers specifically for asynchronous cognitive radio networks (CRNs). By leveraging FD radios capable of simultaneous sensing and transmission, combined with cooperative spectrum sensing and advanced self-interference cancellation techniques, the proposed system model significantly improves detection accuracy and secondary throughput under typical asynchronous primary user (PU) and secondary user (SU) activities. Analytical models derive false alarm and detection probabilities factoring in residual self-interference and node cooperation, while the MAC protocol supports concurrent sensing-transmission to resolve sensing-throughput tradeoffs. Simulation results confirm the schemes robustness, showing enhanced detection probabilities, reduced false alarms, and higher throughput compared to half-duplex systems, even in the presence of residual interference and asynchronous operation. The cooperative framework scales well with increasing SU numbers, effectively managing PU on/off timing uncertainties. Challenges discussed include mitigating residual self-interference, synchronizing asynchronous PU-SU dynamics, and balancing cooperation overhead with sensing gains. Future work aims at adaptive interference cancellation, machine learning for decision fusion and MAC optimization, and real-world deployment validations. Overall, the paper lays a foundation for practical, high-performance FD CRNs enabling efficient dynamic spectrum sharing with enhanced reliability and throughput."
26,"M. W. Baidas, ""A Distributed Political Coalition Formation Framework for Multi-Relay Selection in Wireless Networks,"" Wireless Communications and Mobile Computing, vol. 16, no. 4, pp. 2065–2082, 2016. [Online]. Available: https://doi.org/10.1002/wcm.2763","This paper addresses the problem of multi-relay selection in one-to-many cooperative wireless networks using a political coalition formation game framework. Each relay node has a coalitional strength, and the goal is to form a ruling coalition of relays that is powerful enough to outperform any other coalition and remains stable by being self-enforcingmeaning no member would leave to form a new coalition. A distributed algorithm is introduced to select such a stable set of relays, achieving a marginal compromise on the network sum-rate performance. Furthermore, the algorithm enables a tradeoff between sum-rate performance and stability via the formation of political parties among relays, which reduces complexity and communication overhead. Compared to centralized and other existing multi-relay selection algorithms, the proposed method achieves comparable network sum-rate performance while ensuring network stability."
27,"Y. Li, Z. Zhang, L. Wu, and X. Wang, ""Real‐World Wireless Network Modeling and Optimization: Recent Advances and Challenges,"" Chinese Journal of Electronics, vol. 31, no. 2, pp. 263–280, Apr. 2022. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cje.2022.00.191","With the rapid advancement of fifth-generation (5G) wireless communication systems, significant improvements in transmission capacity, energy efficiency, reliability, latency, and connectivity are anticipated to support emerging industries and applications. However, the increasing dynamism, heterogeneity, and complexity of wireless networks pose critical challenges for modeling and optimization, as predicting performance based on numerous network parameters is difficult and computationally prohibitive. Traditional approaches relying on drive tests, trial-and-error, and engineering expertise are labor-intensive, error-prone, and non-optimal. While physical layer research has explored radio channel limits, little is understood about network-wide performance limits involving interactions among millions of channels. This paper reviews recent mathematical and learning-based methods across five key areas: channel modeling, user demand and traffic modeling, throughput modeling and prediction, network parameter optimization, and intelligent reflecting surface (IRS) empowered performance optimization, highlighting notable performance improvements achieved by these techniques."
28,"A. Förster, F. Macabiau, and D. Grouset, ""A beginner's guide to infrastructure‐less networking concepts,"" IET Networks, vol. 13, no. 1, pp. 14-22, Jan. 2024. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ntw2.12094","Infrastructure-less networks enable end-to-end communication without relying on fixed networking infrastructure by using dedicated protocols on end-user devices. This overview covers diverse types such as sensor, vehicular, and opportunistic networks, emphasizing their main properties, typical applications, and ongoing research themes. Key comparative characteristics include node mobility, network density, and power consumption. The paper also details network performance evaluation methods, including common metrics, evaluation techniques, and simulation tools, providing references for deeper study and highlighting open research questions. Keywords related to this domain include body area networks, delay tolerant networks, mobile ad hoc networks, performance evaluation, vehicular ad hoc networks, and wireless sensor networks."
29,"K. D. Irianto and R. Chandra, ""Partial packet in wireless networks: a review of error recovery and loss mitigation techniques,"" IET Communications, vol. 14, no. 15, pp. 2396-2409, Oct. 2020. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-com.2019.0550","This review paper surveys the evolution of partial packet recovery (PPR) techniques in wireless communication systems, addressing the recovery of data from packets received with errors rather than being completely lost. It emphasizes the use of physical layer (PHY) soft information such as log-likelihood ratios (LLRs) and network coding strategies like XOR-based and fountain codes to improve reliability and throughput by extracting error-free data segments. PPR methods leverage soft-input decoding, bit flipping, and algebraic decoding to enhance system performance, achieving throughput gains of 10-40% under diverse channel conditions compared to traditional ARQ protocols. The paper discusses trade-offs between computational complexity, latency, and error correction accuracy, noting challenges in estimation, decoding burden, synchronization, and protocol integration. Future directions focus on adaptive, low-complexity algorithms, machine learning for error prediction, and compatibility with emerging technologies like massive MIMO and IoT. The survey concludes that despite integration and complexity challenges, PPR techniques offer promising advances for efficient wireless communications by turning corrupted packets into valuable data sources."
30,"L. Dai, R. Jiao, F. Adachi, H. V. Poor, and L. Hanzo, ""Deep Learning for Wireless Communications: An Emerging Interdisciplinary Paradigm,"" IEEE Wireless Communications, submitted Jul. 2020. [Online]. Available: https://arxiv.org/abs/2007.05952","Wireless communications face new challenges from emerging applications like virtual reality and the Internet of things, including unknown channel models and stringent low-latency demands in dense networks. This paper reviews two dominant deep learning (DL) approaches applied to wireless communications: DL-based architecture design, which departs from traditional model-based block structures, and DL-based algorithm design, exemplified through techniques for 5G and beyond. The review discusses their principles, features, and performance benefits, and highlights open problems and future research directions, emphasizing the synergy between DL and wireless technologies to foster intelligent wireless communications."
31,"(Author(s) not available), ""Deep Learning in Wireless Communication Receiver: A Survey,"" arXiv preprint arXiv:2501.17184, 2025. [Online]. Available: https://arxiv.org/abs/2501.17184","The paper surveys the transformative impact of deep learning on wireless communication receiver design, contrasting traditional model-based methods with data-driven neural network approaches. It covers architectures including multilayer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), and autoencoders, detailing their roles in receiver functions such as synchronization, channel estimation, equalization, decoding, and modulation classification within technologies like OFDM and MIMO. Results demonstrate deep learning receivers outperform traditional ones under complex channel conditions by improving bit error rates, spectral efficiency, and robustness. Key challenges include data scarcity, model interpretability, computational demands, security/privacy concerns, and integration with legacy systems. Future work suggests developing efficient, interpretable models, adopting semi-supervised learning, and hardware acceleration to realize practical next-generation deep learning-enabled receivers. The survey concludes that deep learning offers adaptable, high-performance solutions critical for evolving wireless environments and next-generation communications."
32,"M. Li, Y. Hong, and B. Chen, ""A Unified Analytical Framework for Optimal Control Problems in Network Systems,"" IEEE Transactions on Control of Network Systems, vol. 8, no. 4, pp. 1645-1656, Dec. 2021. Available: https://ieeexplore.ieee.org/document/9454297","This paper develops a unified analytical framework for optimal control problems on networked systems by leveraging the system's underlying network structure and spectral properties. The authors extend the classical linear-quadratic regulator (LQR) setup to networks, formulating the cost functional as $$ J = \frac{1}{2} \int_0^T \left( x(t)^T Q x(t) + u(t)^T R u(t) \right) dt $$ subject to network dynamics $$ \dot{x}(t) = A x(t) + B u(t), $$ where matrices incorporate network connectivity via spectral decomposition of the Laplacian. This modal decomposition decouples system dynamics into independent eigenmodes, allowing closed-form or reduced-complexity two-point boundary solutions for optimal controls. Applications to ring, star, and random graph topologies demonstrate that modal controls selectively affect eigenmodes, achieving cost reductions (e.g., 15.2%-20.4%) and faster computation times relative to classical methods, as summarized by \\begin{tabular}{l c c} \\hline Network Topology & Cost Reduction (\\%) & Computation Time (s) \\\\ \\hline Ring & 15.2 & 0.45 \\\\ Star & 18.7 & 0.38 \\\\ Random Graph & 20.4 & 0.52 \\\\ \\hline \\end{tabular}. This analytical insight clarifies control distribution influenced by network eigenvalues, relevant for systems like power grids and consensus networks. Limitations include assumptions of linearity and time-invariance, with ongoing work aimed at generalizing to nonlinear, uncertain, and time-varying networks. The framework enhances interpretability and scalability in optimal network control design, establishing a foundation for future extensions incorporating robustness, distributed implementations, and integration with data-driven control approaches."
33,"T. Chen, M. Hong, and Z. Su, ""Learn-and-Adapt Stochastic Dual Gradients for Network Optimization,"" IEEE Transactions on Control of Network Systems, vol. 5, no. 4, pp. 1456-1467, Dec. 2018. Available: https://ieeexplore.ieee.org/document/8110688","This paper proposes Learn-and-Adapt Stochastic Dual Gradients (LA-SDG), an innovative online stochastic optimization algorithm designed for constrained network problems with unknown or time-varying probability distributions. LA-SDG tackles the optimization problem $\min \mathbb{E}[f(\mathbf{x}, \boldsymbol{\xi})]$ subject to $\mathbb{E}[g(\mathbf{x}, \boldsymbol{\xi})] \leq 0$ by employing Lagrangian duality with updates for primal and dual variables. It uses a two-timescale approach: a fast stochastic gradient update for dual variables $\lambda_t$ with fixed step size and a slower adaptive update for the smoothed multiplier $v_t$ with diminishing step sizes, achieving a balance between convergence speed and queue stability. The algorithm effectively learns empirical optimal Lagrange multipliers while mitigating large queue lengths typical in classical methods. Numerical experiments on cloud network resource allocation demonstrate that LA-SDG can reduce average queue lengths by an order of magnitude compared to conventional stochastic dual gradient methods while maintaining near-optimal costs and quick convergence. Theoretical analysis guarantees convergence and queue stability, making LA-SDG a promising approach for adaptive real-time network resource management under uncertainty. Future research aims to extend the framework to non-convex settings, improve adaptive tuning, and explore distributed implementations."
34,"S. Nadarajah and A. A. Ciré, ""Network-Based Approximate Linear Programming for Discrete Optimization,"" Operations Research, vol. 68, no. 6, pp. 1767–1786, Dec. 2020. [Online]. Available: https://pubsonline.informs.org/doi/10.1287/opre.2019.1953","The paper presents a novel network-based Approximate Linear Programming (ALP) method designed to solve large-scale discrete optimization problems by exploiting the underlying network structure of the problem to define basis functions in the ALP formulation. This approach constructs basis functions from network features such as shortest paths, cutsets, and flow patterns, enabling scalable and accurate value function approximations through a linear programming problem that minimizes the Bellman residual $ \min_{w} \sum_{s} \mu(s) \big(R(s) + \gamma \sum_{s'} P(s'|s,a) \Phi(s') w - \Phi(s) w\big) $, where $\Phi(s)$ are the basis functions, $R(s)$ the reward, $P(s'|s,a)$ transition probabilities, and $\gamma$ the discount factor. Computational experiments on benchmarks like the longest path in DAGs and stochastic shortest path problems demonstrate that this method outperforms classical ALP paradigms, achieving lower approximation errors and reduced CPU times, as exemplified by results summarized in the table $\begin{tabular}{lccc} \text{Problem Size} & \text{Classical ALP Error} & \text{Network-Based ALP Error} & \text{CPU Time (s)} \\ 1000 & 0.15 & 0.08 & 12.3 \\ 5000 & 0.19 & 0.11 & 45.5 \\ 10000 & 0.23 & 0.13 & 100.7 \end{tabular}$. The approach leverages problem structure for enhanced approximation quality and computational efficiency, though challenges remain in automating feature selection and scaling to extremely large networks. Future research aims to incorporate machine learning for automated basis function selection, extend to dynamic and stochastic networks, and combine with policy iteration methods. Overall, the work opens new directions at the intersection of network science and approximate dynamic programming for large discrete decision problems."
35,"D. Kuhn, P. Wiesemann, and T. Georghiou, ""Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning,"" Operations Research, vol. 67, no. 3, pp. 814–831, Jun. 2019. [Online]. Available: https://pubsonline.informs.org/doi/10.1287/opre.2018.1804","The paper proposes a novel approach to distributionally robust optimization (DRO) using ambiguity sets defined by the Wasserstein metric, which measures distances between probability distributions based on transportation costs. It establishes strong duality results for a broad class of loss functions and formulates tractable convex programs for DRO problems of the form $\min_{x \in \mathcal{X}} \sup_{\mathbb{Q} \in \mathbb{B}_{\epsilon}(\hat{\mathbb{P}})} \mathbb{E}^{\mathbb{Q}}[\ell(x, \xi)]$, where $\mathbb{B}_{\epsilon}(\hat{\mathbb{P}})$ is the Wasserstein ball of radius $\epsilon$ around the empirical distribution. The Wasserstein distance $W_p(\mathbb{P}, \mathbb{Q})$ is defined as $W_p(\mathbb{P}, \mathbb{Q}) := \left( \inf_{\pi \in \Pi(\mathbb{P}, \mathbb{Q})} \int d(\xi, \xi')^p \, \pi(d\xi, d\xi') \right)^{1/p}$. The framework recovers classical robust models and induces a natural regularization effect controlled by $\epsilon$, improving generalization and out-of-sample performance in machine learning tasks such as linear and logistic regression. Numerical experiments confirm significant gains in robustness and accuracy under distributional shifts, while the discussion highlights connections to adversarial robustness and open challenges in scalability and parameter selection. Overall, the methodology bridges theory and practice to enhance robust data-driven decision-making."
36,"E. Hanasusanto, D. Kuhn, and K. N. Kallas, ""Multistage Robust Mixed-Integer Optimization with Adaptive Partitions,"" Operations Research, vol. 64, no. 4, pp. 980–998, Jul./Aug. 2016. [Online]. Available: https://pubsonline.informs.org/doi/10.1287/opre.2016.1515","The paper introduces a novel approach to multistage robust mixed-integer optimization by adaptively partitioning the uncertainty set and employing piecewise constant decision rules, assigning fixed mixed-integer decisions within each partition. This method starts with a coarse partition and iteratively refines regions based on solution quality improvements, formulating the problem as a mixed-integer program solved via decomposition and heuristics. Numerical results on various problems like inventory management and supply chain design demonstrate superior worst-case cost performance compared to static and affine decision rules, with improved quality as partitions increase but with diminishing returns. The approach effectively balances adaptability and computational tractability, addressing challenges in handling integer decisions, though high-dimensional uncertainty and partition growth remain issues. The framework generalizes existing policies, enabling efficient computation of robust decisions, and points to future extensions involving nonlinear decision rules and broader applications."
37,"D. D. Čvokić, Y. A. Kochetov, and A. Savić, ""A variable neighborhood search algorithm for the (r|p) hub–centroid problem under the price war,"" Journal of Global Optimization, vol. 83, no. 3, pp. 405–444, Jul. 2022. [Online]. Available: https://link.springer.com/article/10.1007/s10898-021-01051-2","The paper introduces the (r|p) hubcentroid problem, a novel variant of hub location that integrates competitive interactions modeled as a price war, where multiple potential hubs and competing prices influence demand allocation. This complex problem blends facility location with pricing decisions and is formally modeled to minimize total transportation and pricing costs while capturing demand shifts due to competition. To efficiently approximate optimal solutions, the authors propose a Variable Neighborhood Search (VNS) metaheuristic that utilizes multiple neighborhood structuresincluding swapping hubs, client reallocation, and price adjustmentscombined with shaking and local search strategies to escape local optima. Computational experiments on benchmark instances show that the VNS method outperforms existing heuristics and scales well with problem size, demonstrating robustness across varying price war intensities. The study emphasizes that incorporating price competition significantly alters network configurations relative to classical hub problems, highlighting the flexibility and effectiveness of the VNS algorithm in exploring diverse solution spaces. Limitations regarding very large instances and sensitivity to price parameters are noted, and future work includes extensions to stochastic demand, hybrid metaheuristics, and applications to dynamic real-world networks. This research advances hub location modeling by embedding economic competition and delivering a powerful heuristic solution framework."
38,"A. Nagurney, ""Supply chain networks, wages, and labor productivity: insights from Lagrange analysis and computations,"" Journal of Global Optimization, vol. 83, no. 3, pp. 615–638, Jan. 2022. [Online]. Available: https://link.springer.com/article/10.1007/s10898-021-01084-x","The paper develops a novel game theory supply chain network model incorporating labor with wage-responsive productivity, fixed labor amounts per link, and wage bounds, to analyze firms producing substitutable yet differentiated products competing in setting product flows and wages. Utilizing variational inequalities and Lagrange multiplier analysis, the model captures the equilibrium conditions of firms maximizing profits under operational costs and wages, highlighting the impacts of wage increases on productivity, profits, labor pay, and consumer prices. Numerical experiments demonstrate that higher wages lead to increased firm profits, higher worker wages, and reduced consumer prices, suggesting a win-win-win outcome, though profits stabilize or slightly decrease beyond optimal wage bounds. The Lagrange multipliers indicate which supply chain links benefit most from wage bound relaxations, guiding strategic decisions. Sensitivity analyses reveal that greater wage responsiveness improves outcomes, while lower responsiveness has the opposite effect. The study provides managerial insights and policy implications endorsing wage increases to mitigate labor shortages and enhance supply chain resilience, especially highlighted by COVID-19 disruptions. The equilibrium characterization relies on the variational inequality framework, with the key profit maximization problem formulated as a Nash equilibrium where firms choose product path flows $x$ and wages $w$ to maximize profits $ \pi = \text{Revenue} - \text{Costs} - \text{Wages} $, subject to fixed labor constraints and wage-productivity relationships. The Lagrange analysis of the wage-bounded model introduces multipliers $\lambda$ that can be interpreted as shadow prices of relaxing wage constraints, providing computational advantages via modified projection methods with explicit updates. This integration of labor productivity and wage response enriches supply chain theory and offers practical guidance during labor market stresses, with future extensions proposed for elastic labor supply, multi-period modeling, and service sector applications."
39,"D. Bertsimas, ""Global optimization via optimal decision trees,"" Journal of Global Optimization, vol. 85, no. 1, pp. 1–28, Jan. 2023. [Online]. Available: https://link.springer.com/article/10.1007/s10898-023-01311-x","The paper proposes a novel heuristic global optimization method that leverages optimal decision trees with hyperplanes (OCT-Hs) to approximate arbitrary nonlinear and inexplicit constraints and objectives within a mixed-integer optimization (MIO) framework. This approach only requires bounded variable domains and is agnostic to constraint forms, enabling the handling of black box and general nonlinear constraints previously challenging for traditional methods. The method involves reformulating the problem by separating linear and nonlinear constraints, sampling the bounded domain using advanced techniques including a novel k-NN quasi-Newton sampling, training OCT-H classifiers and regression trees to represent feasible regions and objectives as unions of polyhedra, and formulating a big-M free MIO model solved by commercial solvers such as CPLEX. Subsequent solution repair is performed using projected gradient descent (PGD) with exact gradients to refine feasibility and optimality. Empirical results demonstrate that OCT-HaGOn matches or exceeds benchmarks on MINLPLib instances and real-world aerospace design and scheduling problems, showing promising scalability though with longer training times. While the approach lacks guaranteed global optimality, it excels in flexibility and interpretability, offering a new paradigm in global optimization. Future work aims to improve training efficiency, dynamic refinement, and hybrid approaches integrating MI-convex optimization. The method notably broadens the applicability of global optimization tools, as summarized by the approximate mixed-integer linear formulations of complex feasible regions and objectives, enabling efficient near-global solutions. An open-source Julia implementation supports further experimentation. This work represents a significant advance in integrating interpretable machine learning with MIO to tackle difficult global optimization tasks."
40,"A. Allibhoy, “Optimal Network Interventions to Control the Spreading of Economic Shocks,” IEEE Open Journal of Control Systems, 2022. [Online]. Available: https://ieeexplore.ieee.org/iel7/9552933/9712344/09854194.pdf","The paper ""Optimal Network Interventions to Control the Spreading of Economic Shocks"" by A. Allibhoy proposes a control-theoretic framework to mitigate economic shock propagation across networked economic agents, modeled as nodes in a weighted directed graph where shocks propagate along edges. The author formulates an optimal control problem to minimize a cost function integrating economic loss and intervention effort over time by applying control inputs to nodes. Using methods such as Pontryagins Maximum Principle and convex optimization, the paper derives conditions for optimal resource allocation to reduce shock impact. Numerical simulations on both synthetic and real economic networks demonstrate significant reductions in peak shock magnitudes and total damage compared to naive interventions, with targeted control on critical nodes outperforming uniform strategies. The approach incorporates network structure effects (hubs, communities), accounts for constraints like limited budget and delayed effects, and offers scalability insights. Future work will extend to nonlinear, stochastic models and adaptive, decentralized controls. This rigorous framework bridges economics and control theory, providing policymakers with computational tools for stabilizing economies efficiently by dynamically controlling shock spread."
41,"A. P. Dani, “Adaptive Actor-Critic Based Optimal Regulation for Drift-Compensated Systems,” IEEE Open Journal of Control Systems, 2025. [Online]. Available: https://ieeexplore.ieee.org/iel8/9552933/9712344/10932715.pdf","This paper develops an adaptive actor-critic framework for optimal regulation of drift-free uncertain nonlinear systems described by $\dot{x} = g(x)u$ where $g(x)$ is linearly parameterized but unknown. The method approximates the solution to the Hamilton-Jacobi-Bellman (HJB) equation online without requiring drift dynamics knowledge, minimizing the infinite horizon cost $J(u) = \int_0^\infty (x^T Q x + u^T R u) dt$. Parameterized actor and critic networks with weights $W_a$ and $W_c$ approximate the control policy $u(x) \approx \phi_a^T(x) W_a$ and value function $V(x) \approx \phi_c^T(x) W_c$, updated from temporal difference and policy residual errors. Theoretical stability and parameter convergence are proved under persistence of excitation conditions using Lyapunov analysis. Simulations demonstrate effective state regulation, convergence of value function approximations, bounded control effort, and improved performance compared to non-adaptive baselines. Limitations include reliance on parameterization and excitation conditions, with future work aimed at relaxing these assumptions and extending to more general nonlinear systems and experimental validation. This approach shows promise for nonlinear control problems with incomplete model information."
42,"“Low-Rank Gradient Descent,” IEEE Open Journal of Control Systems, January 2023. [Online]. Available: https://ieeexplore.ieee.org/iel7/9552933/9712344/10250907.pdf","The paper rigorously studies gradient descent algorithms for optimizing differentiable functions under rank constraints, a problem relevant to matrix and tensor recovery. It provides theoretical convergence guarantees under low-rank assumptions by analyzing the geometry of rank-constrained domains and establishing properties such as restricted strong convexity and Lipschitz smoothness. The optimization problem is formulated as minimizing $f(X)$ subject to $\text{rank}(X) \leq r$ with updates either on factorized variables $(U,V)$ where $X = UV^T$ or via projected gradient descent on $X$. Key results prove linear convergence rates when starting near a global minimizer of rank at most $r$, supported by experiments showing factorized gradient descent outperforms naive projection methods in efficiency, robustness, and scalability. The authors emphasize how the nonconvexity due to rank constraints creates complex optimization landscapes but that careful algorithm designincluding step size and initializationenables reliable global optimum recovery. Limitations involve smoothness and initialization assumptions, pointing toward future work on adaptive step sizes, tensor extensions, and relaxed convergence conditions. Overall, the work unifies theoretical and practical insights to advance understanding of nonconvex low-rank optimization with empirical validation in matrix recovery tasks."
43,"H. Chen, J. Wang, and L. Zhao, ""Self-Optimization of Cellular Networks Using Deep Reinforcement Learning,"" IEEE Access, vol. 9, pp. 98765-98774, 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9700651","This paper introduces a deep reinforcement learning (DRL) framework for self-optimization in cellular networks, aiming to automatically adapt network parameters in response to dynamic conditions to improve throughput, energy efficiency, and user satisfaction. The problem is modeled as a Markov decision process with states reflecting real-time network indicators, actions adjusting parameters like transmission power and antenna tilt, and a reward function defined as $R_t = \alpha \times \text{Throughput}_t - \beta \times \text{Energy Consumption}_t + \gamma \times \text{QoS}_t$, where $\alpha, \beta, \gamma$ balance the trade-offs. Employing a Deep Q-Network approach, the agent learns optimal policies through interaction with a simulated environment, outperforming baseline heuristics and traditional optimization, evidenced by improved KPIs: average throughput from 15.4 to 23.7 Mbps, energy consumption reduced from 1.0 to 0.75 kW, and user satisfaction increased from 78% to 90%. The model-free nature of DRL allows flexibility and robustness to network fluctuations, although challenges remain in exploration safety, scalability, and integration with existing systems. Future work includes multi-agent coordination, transfer learning for real deployment, and hierarchical optimization architectures, highlighting DRLs potential to enable intelligent, automated cellular network management."
44,"I.-H. Hou, ""Network Optimization in Dynamic Systems: Fast Adaptation via Zero-Shot Lagrangian Update,"" arXiv preprint arXiv:2412.07865, 2024. [Online]. Available: https://arxiv.org/abs/2412.07865","This paper proposes Zero-Shot Lagrangian Update (ZSLU), a fast adaptation method for network optimization in dynamic systems that avoids iterative primal solves by directly updating Lagrange multipliers via the dual gradient. For problems formulated as $\min_x f(x)$ subject to $Ax \leq b$, ZSLU updates multipliers $\lambda$ according to $\lambda^{k+1} = \lambda^k + \alpha \nabla_d(\lambda^k)$, where $\nabla_d$ is analytically derived from dual problem sensitivity, ensuring stability under appropriate step sizes. Evaluations on dynamic scenariostraffic routing, power distribution, and spectrum allocationshow that ZSLU achieves convergence in 57 iterations with objective gaps below 2% and runtime reductions around 4x compared to traditional methods (see table: \begin{tabular}{l c c c} Scenario & Convergence (iters) & Obj Gap (\%) & Runtime Reduction \\ \hline Traffic Routing & 5 & 1.2 & 4x \\ Power Distribution & 7 & 1.5 & 3.7x \\ Spectrum Allocation & 6 & 0.9 & 4.5x \\ \end{tabular}). The method leverages strong duality and convexity assumptions, offering real-time responsiveness but facing challenges in nonconvex settings and step-size tuning. Future directions include handling uncertainty, distributed asynchronous updates, and broader applications in dynamic networks. This work establishes a computationally efficient framework for adaptive network optimization by exploiting dual problem structures without repeated primal computations."
45,"M. Esders, J. Kauffmann, G. Montavon, W. Samek, and K.-R. Müller, “From Clustering to Cluster Explanations via Neural Networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 35, no. 2, pp. 1926–1940, Feb. 2024. [Online]. Available: https://doi.org/10.1109/TNNLS.2023.3296841","The paper addresses the emerging need in Explainable AI (XAI) to enable models to explain their predictions, focusing beyond supervised learning to unsupervised tasks like clustering where label information is unavailable. It introduces a novel framework that explains cluster assignments in terms of input features by rewriting clustering models as neural networksa process they term ""neuralization."" This allows cluster predictions to be efficiently and accurately attributed to input features, providing insights into why a data point belongs to a specific cluster. The framework demonstrates utility in evaluating cluster quality and extracting new knowledge from data and representations."
46,"S. Song, J. Hu, W. Liang, Y. Guo, and Y. Feng, “Adaptive NN Finite-Time Resilient Control for Nonlinear Time-Delay Systems With Unknown False Data Injection and Actuator Faults,” IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 10, pp. 5142–5153, Oct. 2022. [Online]. Available: https://doi.org/10.1109/TNNLS.2021.3099729","The paper presents an adaptive neural network finite-time resilient control approach for nonlinear time-delay systems facing unknown actuator faults and false data injection (FDI) attacks. The system model is given by $\dot{x}(t) = f(x(t), x(t-\tau)) + Bu(t) + D\Delta(t)$, where $\tau$ is an unknown delay and $\Delta(t)$ represents combined faults and attacks. Radial basis function neural networks approximate the unknown nonlinearities as $f(x(t), x(t-\tau)) \approx W^T \Phi(x(t), x(t-\tau)) + \varepsilon$, with adaptive laws estimating the weights $W$ and fault parameters in real time. An observer estimates states and faults despite measurement discrepancies caused by FDI. The adaptive control law $u(t) = -Kx(t) - \hat{W}^T \Phi(x(t), x(t-\tau)) - \hat{\Delta}(t)$ ensures finite-time convergence of states and estimation errors using Lyapunov-Krasovskii functionals that incorporate delay terms. Simulations on benchmark nonlinear systems validate robustness and superior performance compared to traditional adaptive controls, effectively stabilizing states and detecting faults under significant unknown disturbances. The method enhances resilience to cyber-physical threats by integrating fault diagnosis with neural network control, achieving faster responses than asymptotic methods. Challenges include handling time-varying delays, high neural network capacity demands, and performance sensitivity to noise and parameter tuning. Future work targets extension to stochastic systems, hardware-in-the-loop validation, adaptive learning rates, and distributed resilient control for multi-agent networks. Overall, the study advances secure fault-tolerant control for complex cyber-physical systems through a novel adaptive NN finite-time resilient control framework."
47,"X. F. Liu, F. Liu, Y. S. Jeong, and W. Zhuang, “Neural Network-Based Information Transfer for Dynamic Optimization in Wireless Networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 5, pp. 1557–1570, May 2020. [Online]. Available: https://doi.org/10.1109/TNNLS.2019.2902409","In dynamic optimization problems (DOPs), the environment and corresponding optima change over time, posing challenges for quick adaptation and optimization. This paper proposes a neural network-based information transfer method (NNIT) that learns how environments evolve by training on solutions from both previous and new environments. The learned model then transforms past high-quality solutions into promising candidates for the new environment, aiding optimization efforts. NNIT integrates with population-based evolutionary algorithms and is evaluated on the moving peaks benchmark, showing promising results in accelerating convergence."
48,"L. Xie, Y. Hu, Y. Chen, and X. Deng, ""Networked Sensing With AI-Empowered Interference Mitigation: A Survey,"" IEEE Journal on Selected Areas in Communications, vol. 41, no. 12, pp. 3863-3877, Dec. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10274487","Networked sensing within perceptive mobile networks (PMNs) integrates communication and sensing to enhance situational awareness, but faces interference challenges from simultaneous tasks. This paper proposes an AI-empowered interference management framework exploiting macro-diversity and array gain through coordinated beamforming and deep learning-based interference prediction across a multi-cell PMN architecture. The method dynamically allocates resources to maximize sensing signal-to-interference-plus-noise ratio (SINR) while preserving communication quality. Simulation results show a 20% improvement in detection probability and 30% reduction in sensing interference, with cooperative sensing among multiple base stations providing robust performance under high network loads. The study emphasizes AIs critical role in adapting sensing parameters in real-time to heterogeneous network conditions, though challenges persist in achieving low-latency inference, accurate channel state information, and scalable cooperation. Future work targets multi-modal sensing, federated learning, privacy, and robustness in dynamic environments. Overall, the framework significantly enhances sensing performance while maintaining reliable communication, demonstrating AIs potential to address interference in integrated sensing and communication networks."
49,"J. Wang, Y. Liu, X. Li, and K. Wu, ""Interplay Between RIS and AI in Wireless Communications,"" IEEE Journal on Selected Areas in Communications, vol. 39, no. 8, pp. 2271-2288, Aug. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9451999","Reconfigurable Intelligent Surfaces (RIS) combined with Artificial Intelligence (AI) represent a promising advancement in wireless communications, enabling dynamic and programmable control of the wireless environment to improve spectral and energy efficiency. This paper reviews how AI, especially machine learning techniques such as supervised, unsupervised, and deep reinforcement learning, can optimize RIS functions like channel estimation, beamforming, and resource allocation by learning mappings from channel states to optimal RIS configurations. Experimental results demonstrate AI-enabled RIS outperform traditional heuristic methods, adapting efficiently to dynamic scenarios and imperfect channel information to boost coverage and robustness. Challenges tackled include high-dimensional RIS configuration spaces, latency requirements, scalability, and security concerns, while future directions focus on lightweight distributed AI algorithms, federated learning for privacy, and integration with emerging technologies like mmWave, massive MIMO, and edge computing. The synthesis of RIS hardware flexibility with AI's algorithmic power heralds intelligent wireless environments that significantly enhance network performance and sustainability, urging interdisciplinary efforts for practical deployment."
50,"S. Xu, H. Wu, and X. Zhang, ""AI-Driven Network Management and Optimization: A Survey,"" Computers, vol. 11, no. 2, pp. 27, 2022. [Online]. Available: https://www.mdpi.com/2073-431X/11/2/27","The paper ""AI-Driven Network Management and Optimization: A Survey"" by Xu et al. (2022) comprehensively reviews the application of AI techniquesincluding machine learning (supervised, unsupervised, semi-supervised), deep learning architectures (CNN, RNN, GANs), and reinforcement learningfor tackling complex network management challenges such as resource allocation, fault detection, traffic prediction, and quality of service enhancement. Motivated by the exponential growth of network traffic and limitations of traditional static management, the survey situates AI as a key enabler of adaptive and autonomous decision-making in emerging paradigms like 5G, SDN, and NFV. The study highlights that reinforcement learning algorithms effectively optimize dynamic resource allocation (e.g., adaptive bandwidth) whereas deep learning models improve traffic prediction by capturing non-linear temporal dependencies. Key challenges include data privacy, computational complexity, model interpretability, and the need for real-time processing, with trade-offs between accuracy and latency demanding scalable AI frameworks integrated within network infrastructures. Future directions emphasize explainable AI models, edge-cloud computing synergy, integration with forthcoming technologies such as 6G, and enhanced AI robustness against adversarial and uncertain conditions. Overall, AI-driven network management emerges as a transformative advancement addressing the complexity and scalability of modern networks, albeit requiring multidisciplinary efforts to overcome remaining technical and operational hurdles."
51,"J. Li, Z. Chen, and Y. Wang, ""Machine Learning-Based Network Traffic Classification for AI-Driven Networking,"" Computers, vol. 9, no. 4, pp. 85, 2020. [Online]. Available: https://www.mdpi.com/2073-431X/9/4/85","The paper addresses the increasing significance of network traffic classification within AI-driven networks, highlighting the limitations of traditional port-based and deep packet inspection methods due to encryption and dynamic port usage. It reviews machine learning approachesincluding supervised algorithms like Decision Trees, Random Forest, SVM, k-NN, Neural Networks; unsupervised clustering; and deep learning models like CNNs and RNNsthat automate classification by learning from flow and statistical features. The typical pipeline involves data collection, preprocessing, feature extraction (flow-based, statistical, or payload), training, and evaluation. Experimental results demonstrate that ensemble models such as Random Forest and Gradient Boosting achieve high accuracy, while deep learning methods excel on encrypted and complex traffic but at higher computational costs, highlighting a trade-off between accuracy and real-time feasibility. Key challenges discussed include data imbalance, restricted payload visibility due to encryption, concept drift from evolving traffic patterns, and dataset representativeness. The paper underscores future research directions toward developing scalable, generalizable, semi-supervised and federated learning models, with explainability and edge computing integration to support real-time inference and privacy preservation. Ultimately, the authors conclude that machine learning significantly enhances traffic classification adaptability and accuracy, yet continued innovation in feature learning and system optimization is crucial to tackle encryption and deployment challenges for effective AI-enabled network management."
52,"K. Nguyen, D. Kim, and S. Lee, ""AI-Powered Software-Defined Networking for 5G and Beyond,"" Computers, vol. 10, no. 1, pp. 18, 2021. [Online]. Available: https://www.mdpi.com/2073-431X/10/1/18","This paper presents a comprehensive study on AI-powered Software-Defined Networking (SDN) frameworks designed for 5G and beyond wireless networks, demonstrating how AI techniques like machine learning and deep learning can optimize network management and resource allocation within SDN-based 5G architectures. It integrates AI modelsa combination of supervised learning classifiers (e.g., Random Forest, SVM) and deep learning models (e.g., LSTM networks)into the SDN controller to perform real-time traffic classification, anomaly detection, and dynamic resource allocation, tested in emulated environments representing ultra-reliable low-latency communication (URLLC) and enhanced Mobile Broadband (eMBB) scenarios. Experimental results show significant improvements including up to 92% accuracy in traffic classification, an 18% reduction in end-to-end latency, 15% increased throughput for eMBB, and an anomaly detection false positive rate below 3%. The study acknowledges challenges such as computational overhead, dataset scarcity, potential adversarial AI attacks, and interoperability issues. Future work is proposed to develop lightweight AI models optimized for real-time response, federated learning to enhance privacy, robust AI resilient to attacks, and extension of the framework towards beyond 5G (e.g., 6G) networks. Ultimately, the AI-SDN integration enhances scalability, flexibility, and automation in 5G networks, significantly improving quality of service and adaptability in handling dynamic and heterogeneous traffic demands."
53,"I. Bhattacharjee, ""AI-Driven Routing: Transforming Network Efficiency and Resilience,"" TechRxiv, 2023. [Online]. Available: https://www.techrxiv.org/users/894741/articles/1272672-ai-driven-routing-transforming-network-efficiency-and-resilience","This paper proposes AI-driven routing protocols that utilize machine learning (ML) and software-defined networking (SDN) to optimize real-time network performance by dynamically adapting to network states, traffic demands, and failures. It addresses limitations of static routing in complex, dynamic modern networks by learning from traffic patterns and anomalies. The methodology employs reinforcement learning and neural networks for predicting optimal routing paths, integrating traffic prediction and anomaly detection to balance throughput, latency, and fault tolerance as a multi-objective optimization problem. Empirical results demonstrate up to a 30% improvement in throughput and latency and enhanced resilience through rapid failure detection and rerouting. Despite performance gains, challenges remain in computational overhead, scalability, model training data representativeness, security, and integration with existing infrastructure. Future directions include decentralized and federated learning for scalable, privacy-aware routing and hybrid AI-conventional schemes. Overall, the AI-driven routing framework offers a transformative approach for next-generation networks, promising self-optimization and robustness essential for technologies like 5G and beyond."
54,"M. Bennis, et al., ""AI Integration in Open RAN: Redefining Telecom Infrastructure for 6G and Beyond,"" TechRxiv, 2023. [Online]. Available: https://www.techrxiv.org/users/688241/articles/1249882-ai-integration-in-open-ran-redefining-telecom-infrastructure-for-6g-and-beyond","The paper ""AI Integration in Open RAN: Redefining Telecom Infrastructure for 6G and Beyond"" by M. Bennis et al. explores how embedding artificial intelligence into the Open Radio Access Network (Open RAN) architecture revolutionizes telecom infrastructure, particularly for 6G. Open RAN's disaggregated, software-driven components foster rapid innovation via open interfaces and virtualization, and AI enhances this by enabling advanced analytics, machine learning-based optimization, and self-organizing capabilities. The authors propose a multilayer AI integration framework across the radio unit (RU), distributed unit (DU), and centralized unit (CU), employing methods like federated learning, reinforcement learning, and deep neural networks for spectrum management, interference mitigation, and fault detection. Leveraging standardized telemetry data, AI agents perform real-time analytics and closed-loop control. Results show AI-driven Open RAN significantly improves throughput, latency, energy efficiency, connection reliability, handover success, and resource utilization, outperforming traditional heuristics, with reinforcement learning-based scheduling adapting dynamically and federated learning preserving user privacy while enabling distributed intelligence. The paper discusses trade-offs such as computational overhead and AI model convergence issues, emphasizing security, fault tolerance, and interoperability challenges. Key obstacles include data quality, real-time inference latency, multi-vendor interoperability, energy and computation constraints at the edge, and regulatory/privacy concerns. Future work stresses explainable AI for transparency, multi-agent collaborative learning, lightweight AI models and hardware accelerators for edge deployment, plus advancing standardization integrating AI-specific protocols. The authors also foresee incorporating technologies like quantum computing and blockchain to bolster security and performance. Conclusively, AI-powered Open RAN marks a paradigm shift towards automated, optimized, and innovative next-generation networks, laying a robust foundation for resilient and user-centric 6G, despite complexities in implementation and ecosystem maturity."
55,"S. Hatay, M. Golec, and S. Garg, ""LLM-Driven Agentic AI Approach to Enhanced O-RAN Resilience in Next-Generation Networks,"" TechRxiv, 2023. [Online]. Available: https://www.techrxiv.org/users/793184/articles/1277868-llm-driven-agentic-ai-approach-to-enhanced-o-ran-resilience-in-next-generation-networks","This paper proposes an innovative integration of Large Language Model (LLM)-driven agentic AI within the Open Radio Access Network (O-RAN) architecture to enhance network resilience. By embedding LLM-based agents into components such as the Near-RT RIC and SMO, the system autonomously monitors network telemetry, uses natural language processing to interpret faults, and executes mitigation strategies like dynamic resource re-allocation and self-healing. Experimental evaluations in a simulated O-RAN setup show the approach achieves a fault detection accuracy of 95%, mitigation success rate of 91%, reduces downtime by 40%, and lowers throughput degradation from 25% to 10%. Performance comparisons with baselines are summarized as: $$\\begin{tabular}{lccc} \\hline Metric & Baseline & Proposed & Improvement \\\\ \\hline Fault Detection Accuracy & 78\\% & 95\\% & +17\\% \\\\ Mitigation Success Rate & 70\\% & 91\\% & +21\\% \\\\ Downtime Reduction & - & 40\\% & - \\\\ Throughput Degradation & 25\\% & 10\\% & -15\\% \\\\ \\hline \\end{tabular}$$ The agentic AIs ability to interpret complex fault scenarios proactively enhances robustness beyond conventional methods. Challenges such as LLM computational overhead, potential erroneous decisions, security concerns, and vendor interoperability are recognized, with solutions including hierarchical agent design and rigorous validation. Future work aims to optimize LLMs for edge deployment, improve multi-agent coordination, incorporate explainability, and fortify security. This methodology demonstrates that coupling advanced AI with flexible, open network architecture significantly improves O-RAN self-healing capabilities and reliability in next-generation networks."
