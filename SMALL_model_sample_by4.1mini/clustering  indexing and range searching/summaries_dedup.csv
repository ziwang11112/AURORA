Index,Citation,Summary
1,"K. G. Larsen, ""On Range Searching in the Group Model and Combinatorial Discrepancy,"" SIAM Journal on Computing, vol. 43, no. 2, pp. 673–686, 2014. [Online]. Available: https://epubs.siam.org/doi/10.1137/120865240","This paper proves a tight lower bound on the space complexity of 2D orthogonal range counting data structures in the group model, establishing that any structure using $S$ memory words of size $w$ must satisfy $S w = \Omega\bigl(n (\log n / \log \log n)^2\bigr)$. This matches the best known upper bounds, resolving a major open problem in geometric data structures. The authors develop a novel information-theoretic encoding argument connecting combinatorial discrepancy lower bounds to the data structure's memory usage. By leveraging discrepancy theory for 2D orthogonal ranges and exploiting the invertibility but limited algebraic structure of groups, they show that any data structure with lower space complexity would contradict known discrepancy entropy estimates. The results imply optimality of existing constructions, extend to several range searching variants, and highlight the power of discrepancy techniques for proving data structure hardness beyond query time lower bounds. The work also discusses potential directions such as higher-dimensional extensions, other algebraic models, and improved discrepancy bounds, emphasizing the deep interplay between combinatorial geometry, discrepancy theory, and data structure complexity."
2,"P. K. Agarwal, J. Matoušek, and M. Sharir, ""On Range Searching with Semialgebraic Sets. II,"" SIAM Journal on Computing, vol. 42, no. 6, pp. 2039–2062, 2013. [Online]. Available: https://epubs.siam.org/doi/10.1137/120890855","The paper addresses the problem of range searching in $\mathbb{R}^d$ for queries defined by constant-complexity semialgebraic sets, leveraging the polynomial partitioning technique introduced by Guth and Katz. It constructs a linear-size data structure of size $O(n)$ that answers range counting queries in time $O(n^{1-1/d+\varepsilon})$ for any $\varepsilon > 0$, improving previous bounds that required either superlinear space or had worse query time. The method recursively partitions the point set using a polynomial $p$ of degree $D$ such that $\mathbb{R}^d \setminus Z(p)$ is divided into $O(D^d)$ cells, each containing roughly equal numbers of points, while points on the zero set $Z(p)$ form a lower-dimensional problem managed by stratification and projection to control complexity. This balanced approach allows nearly optimal query times and linear storage. The paper discusses challenges including handling points on $Z(p)$ and balancing recursion depth with polynomial degree $D$, while outlining future work toward dynamic data structures, practical implementations, and related query types such as approximate counting. Overall, the results demonstrate the power of algebraic geometry techniques in geometric data structures and mark significant progress in efficient semialgebraic range searching."
3,"C.-W. Mortensen, ""Fully Dynamic Orthogonal Range Reporting on RAM,"" SIAM Journal on Computing, vol. 35, no. 5, pp. 1268–1303, 2006. [Online]. Available: https://epubs.siam.org/doi/10.1137/S0097539703436722","This paper addresses the problem of orthogonal range reporting in a fully dynamic setting on the RAM model, presenting a data structure that supports insertions and deletions in worst-case $O(\log^d n)$ time and answers queries in worst-case $O(\log^d n + k)$ time, where $k$ is the output size. Leveraging multi-level balanced binary search trees (range trees) combined with probabilistic hashing techniques such as dynamic perfect hashing and dynamic fractional cascading, the structure efficiently manages $d$-dimensional point sets with near-optimal space $O(n \log^{d-1} n)$. This resolves the longstanding gap between static and fully dynamic orthogonal range searching by matching complexities traditionally achieved only in static or partially dynamic contexts. Despite the probabilistic approach introducing randomness in guarantees, this method significantly advances the theoretical foundations of dynamic geometric data structures. Future work may focus on deterministic variants, practical implementation overhead, extensions to higher dimensions or other query types, and adapting to external memory or streaming models."
4,"Y. A. Malkov and D. A. Yashunin, ""Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 4, pp. 824-836, Apr. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/8594636","This paper introduces the Hierarchical Navigable Small World (HNSW) graph algorithm for approximate nearest neighbor (ANN) search, which constructs a multi-layer graph with nested subsets of elements distributed across layers selected by an exponentially decaying probability $P(\max\_layer \ge l) = p^l$. Each layer forms a proximity graph connecting nodes to closest neighbors at distinct scale levels, enabling a coarse-to-fine search strategy starting from the top layer and moving downwards. This design achieves logarithmic complexity scaling and outperforms prior graph-based and tree/partition-based methods in recall versus search speed on benchmarks like SIFT1M and GIST1M. The incremental nature of HNSW allows dynamic insertions without full rebuilds, and a heuristic for neighbor selection improves performance on clustered data. Its hierarchical structure overcomes flat graph limitations by reducing search space efficiently while maintaining robust connectivity, drawing an analogy to skip lists for potential distributed implementations. Challenges include tuning parameters such as maximum connections and hierarchical probability $p$, particularly for very high-dimensional or structured data. Future work aims to extend HNSW to disk-based and distributed systems for billion-scale data, incorporate learned metrics, and deepen theoretical analysis. Overall, HNSW demonstrates state-of-the-art performance in ANN tasks with scalability and robustness suitable for diverse metric spaces and large-scale applications."
5,"Q. Liao, S. Li, and X. Hu, ""Point Set Registration for 3D Range Scans Using Fuzzy Correspondences,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 9, pp. 2167-2180, Sept. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9026868","This paper introduces a novel fuzzy correspondence framework for point set registration of 3D range scans, addressing fundamental limitations of traditional hard correspondence methods like ICP, which are sensitive to noise, outliers, and partial overlaps. Instead of one-to-one matches, the method assigns probabilistic fuzzy correspondences between points, allowing simultaneous estimation of the rigid transformation (rotation and translation) and correspondences through a unified probabilistic optimization. The objective function minimizes the fuzzy correspondence-induced alignment error by iteratively updating probabilistic weights representing match likelihoods, effectively solving for transformation parameters and soft assignments. Experiments on multiple public datasets demonstrate superior registration accuracy and robustness compared to baseline ICP and probabilistic algorithms, with significant improvements in convergence and error metrics. The approach reduces computational complexity via fuzzy cluster representations and gracefully handles uncertainty in point matching. Challenges involve parameter tuning for fuzzy memberships and optimization risks of local minima; future work aims to extend this framework to non-rigid registration, incorporate deep learning for adaptive fuzzy membership estimation, and enhance real-time performance. Overall, this method offers a robust, accurate, and flexible solution for 3D scan alignment tasks in computer vision and robotics."
6,"R. Yao, J. Liu, and H. Li, ""Hunter: Exploring High-Order Consistency for Point Cloud Registration,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 12, pp. 14760-14776, Dec. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10246849","This paper introduces Hunter, a novel framework for point cloud registration that advances beyond conventional pairwise correspondences by leveraging high-order geometric consistency captured through hypergraph matching. Hunter constructs a hypergraph where nodes represent correspondences and hyperedges encode invariant geometric relations among multiple points, formulating registration as a global optimization problem: $$\max_{x} \sum_{e \in E} w_e \prod_{v \in e} x_v \quad \text{s.t.} \quad x \in \{0,1\}^n,$$ where $x_v$ denotes the selection of correspondences, $E$ are hyperedges, and $w_e$ their consistency weights. Experiments on datasets like 3DMatch, KITTI, and ModelNet40 demonstrate Hunter's superior robustness and accuracy, especially under challenging conditions of partial overlap (as low as 20%) and noise, outperforming baselines such as RANSAC and learning-based approaches. The methods relaxation-based optimization effectively addresses the NP-hard hypergraph matching, while maintaining computational efficiency suitable for real-time use. Hunter's integration of richer geometric constraints enables better outlier rejection and disambiguation of correspondences compared to pairwise-only methods. Limitations include dependency on initial correspondence quality and parameter tuning, with future work aimed at adaptive hyperedge selection, end-to-end feature learning integration, scalability improvements, and applications to non-rigid registration. Overall, Hunter significantly advances 3D registration by encoding complex spatial relations beyond pairwise constraints, establishing a robust foundation for future developments."
7,"W. Choi, C. Shim, I. Yun, and H. Shin, ""Scalable Algorithms for Maximizing Spatiotemporal Range Queries,"" Electronics, vol. 9, no. 3, p. 514, 2020. [Online]. Available: https://www.mdpi.com/2079-9292/9/3/514","The paper ""Scalable Algorithms for Maximizing Spatiotemporal Range Queries"" by W. Choi et al. addresses efficient processing of spatiotemporal range queries in large-scale datasets by proposing scalable algorithms that optimize data structures and query methods to maximize the number of satisfied queries. The authors introduce partitioning of the spatiotemporal data space, dynamic programming and greedy heuristics for query coverage maximization, pruning techniques to exclude irrelevant data early, and parallel/distributed computation models to enhance scalability. The core optimization problem is formalized as maximizing $\sum_{q \in Q} \mathbf{1}_{\{data \cap range(q) \neq \emptyset\}}$, where $Q$ is the query set and $range(q)$ is the spatiotemporal range of query $q$. Experiments on real and synthetic datasets demonstrate superior runtime performance and query satisfaction compared to baseline methods, with near-linear scalability when adding computational resources. The discussion highlights effective handling of high-dimensional spatiotemporal data, balancing efficiency and accuracy, and success in parallelization amid communication overhead. Challenges include real-time handling of dynamic data streams, index updates without full rebuilds, and distributed system load balancing. Future work suggests extending to more complex queries, employing learning-based tuning, and improving fault tolerance. Overall, the algorithms provide a scalable, effective solution for maximizing spatiotemporal range queries and advance the state-of-the-art through innovative indexing and parallel processing."
8,"V. Plevris, G. Kalivas, and I. Andreadou, ""Pure Random Orthogonal Search (PROS): A Plain and Efficient Method for Global Optimization,"" Applied Sciences, vol. 11, no. 11, p. 5053, 2021. [Online]. Available: https://www.mdpi.com/2076-3417/11/11/5053","Pure Random Orthogonal Search (PROS) is a novel and efficient global optimization algorithm for continuous problems that generates search points by combining random directions with orthogonal vectors. This strategy allows effective exploration and exploitation of the search space without requiring gradient or Hessian calculations, making it suitable for problems where derivative information is unavailable or unreliable. PROS achieves competitive performance against existing global optimization methods on benchmark functions, demonstrating good convergence rates while maintaining low computational overhead."
9,"H. Wang, ""Unit-Disk Range Searching and Applications,"" in Proc. 18th Scandinavian Workshop on Algorithm Theory (SWAT), 2022. Available: https://arxiv.org/abs/2204.08992","We investigate the unit-disk range searching problem, where given a planar set $S$ of $n$ points, the goal is to preprocess $S$ into a data structure enabling queries for points inside any unit disk $D$. The proposed structure achieves query time $O(k + \log n)$, where $k$ is the output size, with space and preprocessing time $O(n \log n)$, representing an improvement over previous methods. The core innovation is a geometric decomposition of the plane based on the arrangement of unit circles centered at points in $S$, creating cells associated with subsets of $S$. Utilizing balanced binary and range trees, and leveraging properties unique to fixed-radius disks, the data structure supports efficient membership tests and bounds recursion branches during queries. Theoretical analysis confirms the stated complexities, and the approach finds applications in clustering, geometric approximation, and wireless communication. The paper discusses challenges managing combinatorial complexity and balancing output size with logarithmic overhead, while suggesting future work on variable radii, higher dimensions, other shapes, and dynamic updates. The final contribution is a novel range searching structure with optimal query performance and near-linear space, advancing both theory and potential applications."
10,"T. M. Chan, Q. He, and Y. Nekrich, ""Further Results on Colored Range Searching,"" arXiv preprint arXiv:2003.11604, 2020. Available: https://arxiv.org/abs/2003.11604","This paper addresses the classical problem of colored range searching through three main variants: 2D colored orthogonal range reporting, 3D halfspace colored range reporting, and 2D orthogonal type-2 range counting. It introduces novel data structures combining traditional geometric techniques with algorithmic tools such as fractional cascading, fusion trees, and hierarchical decompositions. Notably, the 2D colored orthogonal range reporting structure achieves $O(n \log \log n)$ space and $O(\log n + k)$ query time, improving prior $O(\log n \log \log n + k)$ bounds. The 3D halfspace reporting achieves near-linear space with $O(n^{2/3} \log^{c} n + k)$ query time. For 2D type-2 counting, the structure attains $O(n \log n)$ space and sub-logarithmic $O(\log n / \log \log n)$ query time. These improvements rely on color-sensitive summaries, geometric duality, and bit-packing for efficient color counting. The work balances space and query efficiency while acknowledging challenges in managing color data complexity and suggesting future extensions toward dynamic data structures, higher dimensions, and practical implementations. Overall, it advances the state of the art in colored geometric queries with rigorous analysis and new algorithmic insights."
11,"P. Afshani, P. Cheng, A. B. Roy, and Z. Wei, ""On Range Summary Queries,"" to appear in Proc. 50th International Colloquium on Automata, Languages, and Programming (ICALP), 2023. Available: https://arxiv.org/abs/2305.03180","The paper addresses the construction of succinct data structures for range summary queries on a line of $n$ colored points from a palette of $k$ colors. It develops data structures enabling efficient queries on intervals $[a,b]$ for statistics such as the number of distinct colors, frequencies, and top-$t$ frequent colors. The approach partitions the input into blocks and applies compressed bit vectors with rank/select operations, wavelet trees, and range trees to encode frequency vectors succinctly. The main result is a data structure using $O(n \log k)$ bits that supports distinct color queries in $O(\log n)$ time and top-$t$ queries in $O(\log n + t)$ time, with proven matching lower bounds within pointer machine models. The methodology balances space and query efficiency by exploiting combinatorial properties and succinct encoding to reduce redundancy. Experiments confirm superior performance over prior art. The paper also discusses challenges like supporting dynamic updates and tight lower bounds and suggests future work toward multidimensional queries and applications in text indexing and compressed pattern matching. Overall, it provides near-optimal trade-offs and advances the theoretical and practical state of data structures for colored range summaries."
12,"B. Moseley, J. Wang, and M. Wang, ""Average Linkage, Bisecting K-means, and Local Search,"" Journal of Machine Learning Research, vol. 24, no. 1, pp. 1-39, 2023. [Online]. Available: https://www.jmlr.org/papers/volume24/18-080/18-080.pdf","This paper advances theoretical understanding of hierarchical clustering by analyzing its performance relative to Dasguptas dual objective, which rewards early merges of highly similar clusters. The authors prove that the classical average linkage agglomerative clustering algorithm achieves a constant-factor approximation guarantee with ratio at most approximately 1.397, expressed as $\text{dualObj}(T_{AL}) \geq \frac{1}{1.397} \cdot \text{dualObj}(T^*)$, where $T_{AL}$ is the average linkage tree and $T^*$ is optimal. In contrast, bisecting k-means is shown to perform poorly with arbitrarily bad approximation ratios under this objective. The study introduces two novel divisive hierarchical clustering algorithms based on local search heuristics that offer provable constant-factor approximation guarantees, achieving ratios around 2 and 3. These results rely on rigorous combinatorial and probabilistic analyses, and are validated through synthetic experiments. The work highlights the importance of objective-specific algorithm design for hierarchical clustering, reinforcing the robustness of average linkage and opening pathways for algorithmic innovations leveraging local search. Future directions include extending these theoretical bounds to other clustering objectives, addressing noise in similarity data, and exploring links with deep learning methods. Overall, the paper provides foundational guarantees for hierarchical clustering algorithms and practical approaches for quality-controlled hierarchical learning."
13,"S. Klepper, C. Heuss, S. L. Campêlo, and S. Hildebrandt, ""Clustering with Tangles: Algorithmic Framework and Guarantees,"" Journal of Machine Learning Research, vol. 24, no. 1, pp. 1-55, 2023. [Online]. Available: https://www.jmlr.org/papers/volume24/21-1160/21-1160.pdf","This paper introduces a novel clustering framework based on the concept of tangles, originally from graph theory, generalized here to abstract separation systems. A tangle is defined as a consistent orientation $T \subseteq S$ of separations in a ground set $U$ that satisfies axioms ensuring high connectivity and avoidance of small cuts. The authors develop polynomial-time algorithms leveraging submodularity and order functions to detect tangles via oracle queries and reconstruct clusters by identifying connected regions induced by these tangles. Empirical results on synthetic and real-world datasets demonstrate superior performance over k-means, spectral, and density-based clustering methods in terms of adjusted Rand index and robustness to noise. Challenges include designing oracles and tuning parameters, while future work aims to improve scalability and explore dynamic data scenarios. Overall, this framework offers a principled combinatorial approach to clustering that captures complex data structures and unifies various clustering paradigms within a rigorous algorithmic foundation."
14,"Y. Huang, M. Chen, and T. Li, ""Incorporating domain ontology information into clustering heterogeneous networks,"" WIREs Data Mining and Knowledge Discovery, vol. 11, no. 3, e1413, 2021. [Online]. Available: https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1413","Recent advances in heterogeneous networks with multiple node and link types pose clustering challenges that traditional methods overlook by ignoring domain-specific semantic information. This paper proposes a novel approach that integrates domain ontology knowledge to enhance similarity measures between nodes, combining ontology-based semantic similarity $Sim_{ont}(v_i, v_j)$ with structural similarity $Sim_{struct}(v_i, v_j)$ into a composite similarity score $Sim(v_i, v_j) = \alpha \cdot Sim_{struct}(v_i, v_j) + (1-\alpha) \cdot Sim_{ont}(v_i, v_j)$, where $\alpha$ balances these effects. Using an adapted spectral clustering algorithm on this composite similarity matrix, the method robustly clusters heterogeneous networks, accounting for diverse node and relationship types. Experiments on bibliographic and biomedical real-world datasets demonstrate improved clustering metricssuch as Normalized Mutual Information (NMI), Rand Index, and F-measureoutperforming baseline methods by 510% in NMI, with clusters semantically more coherent. The framework is flexible for various ontologies but depends on ontology quality and increases computational cost. Future directions include enhancing scalability, applying deep learning for automatic ontology construction, and adapting to dynamic networks, making this approach a promising domain-aware technique for better network mining outcomes."
15,"T. Ullmann, S. Schubert, and A. Seidl, ""Validation of cluster analysis results on validation data: A formal framework,"" WIREs Data Mining and Knowledge Discovery, vol. 12, no. 1, e1444, 2022. [Online]. Available: https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1444","Cluster analysis groups similar data points, but validating these groupings on separate validation data is crucial. This paper offers a rigorous framework formalizing validation for clustering by considering a data generating distribution $P$ and separate learning and validation datasets $D_{learn}$ and $D_{val}$. Given a clustering algorithm $A$ producing clusters $C_{learn}$ on $D_{learn}$, validation aims to test $C_{learn}$s generalization to $D_{val}$. Validation approaches are categorized as (1) external validation comparing to ground truth labels, (2) internal validation using intrinsic cluster properties measured by indices $I(D_{val}, C_{learn})$, and (3) stability-based validation assessing similarity $S(C_{learn}, C')$ between original and perturbed-data clusterings. The framework provides formal definitions and theoretical insights, showing under assumptions like cluster separation that internal indices can reliably assess cluster quality, while stability methods relate to generalization error bounds. External validation remains a gold standard but is often impractical. The discussion highlights trade-offs: internal methods are flexible but potentially biased, stability methods are robust but computationally expensive, and no single method suffices universally. Key challenges involve lack of true labels, algorithm variability, metric standardization, and practical limitations of theoretical guarantees. Future work includes relaxing distributional assumptions, adapting validation to new clustering paradigms, automating hyperparameter tuning, and enhancing interpretability. Overall, the framework advances the principled validation and reliability of cluster analysis results."
16,"T. H. Sardar, M. A. Saleh, and I. Atoum, ""Reflecting on a decade of evolution: MapReduce‐based clustering of big data,"" WIREs Data Mining and Knowledge Discovery, vol. 14, no. 1, e1566, 2024. [Online]. Available: https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1566","This paper provides a comprehensive reflection on a decade of MapReduce-based clustering research in big data, reviewing how the MapReduce model has addressed large-scale data clustering challenges by adapting fundamental algorithms such as k-means, hierarchical, and density-based clustering. It categorizes these adaptations into partitioning, hierarchical, density-based, and model-based methods, detailing their MapReduce implementations, including data partitioning, load balancing, and iterative processing patterns. Results highlight significant improvements in scalability and processing speed, such as near-linear scalability for MapReduce k-means variants, though trade-offs in accuracy and convergence exist. The discussion acknowledges MapReduce's limitations like iterative overhead and communication costs, especially for density-based and hierarchical methods, prompting hybrid frameworks integrating in-memory processes. Challenges identified include handling multiple data passes, data skew, communication overhead, convergence assurance, and real-time processing constraints due to MapReduces batch nature. Future directions advocate for intelligent data partitioning, deep learning integration, support for complex data types, enhanced iterative processing, and seamless cross-framework interoperability, emphasizing adaptability in cloud-native and edge computing contexts. The paper concludes that despite intrinsic MapReduce limitations, its adaptations and hybrid approaches have significantly advanced scalable clustering for big data analytics, setting the stage for more robust and intelligent distributed clustering solutions."
17,"N. E. Garcia-Pedrajas and G. Cerruela-García, ""PYRAMID: A label hierarchical clustering approach for multilabel classification,"" Machine Learning: Science and Technology, vol. 6, no. 3, article 035013, 2025. [Online]. Available: https://doi.org/10.1088/2632-2153/adde0e","This paper introduces PYRAMID, a novel hierarchical clustering method for multilabel classification that organizes labels into a tree-like hierarchy to exploit label dependencies and reduce computational complexity. The method computes a label similarity matrix as $S = \alpha C + (1-\alpha) F$, combining label co-occurrence $C$ and feature similarity $F$ with balance parameter $\alpha$, then applies agglomerative clustering to form the hierarchy. Predictive models are trained at each node, enabling a divide-and-conquer approach for efficient multilabel prediction. Experiments on benchmark datasets such as Bibtex and Delicious show PYRAMID consistently outperforms flat classifiers like Binary Relevance and other hierarchical methods in accuracy, F1-score, and training/prediction efficiency, reducing time by up to 30%. The approach allows flexible adaptation to diverse datasets but requires careful tuning of $\alpha$ and cluster granularity. PYRAMID demonstrates improved scalability for large label sets by leveraging structured label representation, highlighting the benefits of hierarchical label modeling for multilabel classification."
18,"C. N. Melton, S. L. Johnson, N. C. Plumb, M. Radovic, M. Hashimoto, P. D. C. King, and D. F. McMorrow, ""K-means-driven Gaussian Process data collection for angle-resolved photoemission spectroscopy,"" Machine Learning: Science and Technology, vol. 1, no. 4, article 045015, 2020. [Online]. Available: https://doi.org/10.1088/2632-2153/abab61","This paper presents a machine learning-driven adaptive data collection framework for angle-resolved photoemission spectroscopy (ARPES) that combines Gaussian Process (GP) regression with K-means clustering to optimize measurement locations. The framework initially collects sparse ARPES data, fits a GP model with a covariance kernel $k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2 \exp \left(-\frac{1}{2l^2} \| \mathbf{x}_i - \mathbf{x}_j \|^2 \right)$, then uses K-means clustering on the GP-predicted uncertainty map to select new measurement points as cluster centroids, thereby maximizing information gain while reducing redundant sampling. Validated on synthetic and real ARPES datasets, the method achieved up to 70% reduction in sampling points and maintained high fidelity in capturing key electronic structure features like Fermi surfaces and band edges. It adaptively balances exploration and exploitation by focusing on uncertain regions, enabling faster data acquisition without quality loss. The approach's data-driven nature suggests broad applicability to other spectroscopic techniques, though challenges remain in scaling GP computations and managing initial sampling. Future work includes scalable GP models, alternative clustering algorithms, physics-informed kernels, and real-time closed-loop experimental integration. Overall, this study demonstrates a powerful adaptive sampling strategy that transforms ARPES workflows through efficient, autonomous data collection."
19,"M. Ho, X. Zhao, and B. D. Wandelt, ""Ordered embeddings and intrinsic dimensionalities with information-ordered bottlenecks,"" Machine Learning: Science and Technology, vol. 6, no. 3, article 035008, 2025. [Online]. Available: https://doi.org/10.1088/2632-2153/ade94d","We propose Information-Ordered Bottlenecks (IOB), a novel framework for adaptive semantic compression that organizes latent embeddings by their mutual information contribution, enforcing $I(x; z_1) \geq I(x; z_2) \geq \cdots \geq I(x; z_n)$ to enable an interpretable, information-prioritized coding scheme. The method modifies the variational inference loss to include an ordering penalty term, yielding the objective $$\mathcal{L} = \mathbb{E}_{q(\mathbf{z}|x)}[-\log p(x|\mathbf{z})] + \beta \sum_{i=1}^n KL(q(z_i|x) \| p(z_i)) + \gamma \, \Omega(\mathbf{z}),$$ where $\Omega(\mathbf{z})$ enforces monotonic information ordering. Experiments on MNIST, CIFAR-10, and semantic scene data demonstrate that IOB achieves 15-20% lower reconstruction loss at similar compression rates compared to baselines, with improved semantic fidelity and interpretable intrinsic dimensionalities reflecting data semantic complexity. This ordered representation supports flexible truncation for dynamic bandwidth allocation critical in applications like edge computing. Challenges include balancing ordering constraints with expressiveness and tuning regularization parameters. Future work aims to scale training, extend to multimodal data, and explore reinforcement learning for adaptive rate control. Overall, IOB advances semantic compression by enabling informed, ordered latent embeddings that reveal underlying data structure while improving compression performance."
20,"W. Cai, F. Yang, B. Yao, C. Li, and G. Sun, ""An adaptive k-means clustering algorithm based on grid and domain centroid weights for digital twins in the context of digital transformation,"" J. Big Data, vol. 12, art. no. 130, 2025. [Online]. Available: https://doi.org/10.1186/s40537-025-01180-z","Digital twins for digital transformation (DT-DT) face challenges in efficiently clustering large-scale historical and streaming data. This paper proposes the GDCW-AKM algorithm, combining fixed grid partitioning, domain centroid weighted sampling, and a weighted adaptive k-means clustering that automatically determines the optimal cluster number $k$ and initial centroids using the Calinski-Harabasz index. The method reduces data size by representing grid cells and domains with weighted centroids before clustering, yielding a complexity of $O(Nd + (Ktk + m)nd)$ for data size $N$ and dimension $d$. Experimental results on datasets up to 10 million samples show that GDCW-AKM maintains clustering accuracy within 2% of classical k-means but achieves drastic runtime improvements, processing million-scale data in seconds compared to hours or days traditionally. The algorithm supports incremental updates for streaming data with sub-second latency, surpassing STREAM algorithms. While effective in typical industrial DT contexts, performance may slightly decline on ultra-high-dimensional or non-spherical data due to grid partitioning limitations. The approach requires minimal parameter tuning (grid size $s$ and domain radius $T$), simplifying usability. This work offers a scalable, user-friendly solution for real-time digital twin data mining, with future directions aimed at enhancing adaptability to complex cluster shapes and higher-dimensional data."
21,"S. Sieranoja and P. Fränti, ""Fast agglomerative clustering using approximate traveling salesman solutions,"" J. Big Data, vol. 12, art. no. 21, 2025. [Online]. Available: https://doi.org/10.1186/s40537-024-01053-x","The paper presents a novel fast variant of Ward's agglomerative clustering method that reduces the number of distance calculations from $O(N^2)$ to $O(gN)$ by constraining merges to neighboring nodes on a novel fully connected TSP-graph composed of multiple approximate Traveling Salesman Problem (TSP) solutions. This approach requires only $O(TN)$ memory, where $T$ is the number of TSP paths, and is applicable to any data type with a defined distance function. The TSP-graph overcomes limitations of k-nearest neighbor graphs by guaranteeing connectivity and avoiding isolated components. Experiments on large benchmark datasets (up to 1.28 million points) demonstrate speedup factors up to 625:1 with only moderate degradation in clustering quality (NMI dropping from 0.90 to 0.80) compared to exact Ward implementations. The method generalizes beyond Euclidean data, efficiently handling string data via edit distances and dramatically reducing memory use. The TSPg-clu algorithm uses a heap with lazy evaluation to efficiently track nearest neighbors during merges. The TSP-graph construction is $O(N \log N)$ and can serve broader applications such as nearest neighbor searches and improving other graph-based clustering methods. This work enables practical hierarchical clustering on massive, diverse datasets previously infeasible due to computational and memory constraints."
22,"R. Haripriya, N. Khare, M. Pandey, and S. Biswas, ""Decentralized big data mining: federated learning for clustering youth tobacco use in India,"" J. Big Data, vol. 11, art. no. 179, 2024. [Online]. Available: https://doi.org/10.1186/s40537-024-01042-0","This study analyzes youth smoking patterns across India's regions using the Global Youth Tobacco Survey dataset within a federated learning framework employing K-Means, DBSCAN, and Hierarchical Clustering. The approach ensures data privacy by keeping data local and exchanging only model parameters, further secured by integrating differential privacy through Gaussian noise addition. Data preprocessing involved z-score normalization, PCA reducing dimensions to three principal components explaining ~85% variance, and mean imputation for missing values. DBSCAN outperformed other algorithms with the highest Silhouette Score (0.48), lowest communication cost (450 KB), and fastest convergence (15 rounds), demonstrating efficient handling of non-IID, noisy data. While K-Means had moderate resource demand and Hierarchical Clustering incurred higher computational costs, all methods balanced privacy and utility, with accuracy decreasing as differential privacy noise increased but remaining acceptable. The study highlights federated averaging for global model updates and scalability to 36 clients, reflecting Indian states' diversity. Challenges included heterogeneous data distribution, communication overhead, and handling outliers and missing data without compromising structure. Future directions point to exploring alternative aggregation schemes like FedSGD, supervised learning, adaptive privacy budgets, transfer learning, and federated applications beyond healthcare. Ultimately, this work provides a robust, privacy-preserving blueprint for decentralized clustering in public health analytics, enabling tailored anti-tobacco interventions based on regional patterns, where the key privacy guarantee is maintained by sharing only model updates plus controlled Gaussian noise according to differential privacy, preserving individual data confidentiality."
23,"J. Paparrizos, F. Yang, and H. Li, ""Bridging the Gap: A Decade Review of Time-Series Clustering Methods,"" arXiv preprint arXiv:2412.20582, Dec. 2024. [Online]. Available: https://arxiv.org/abs/2412.20582","Time-series clustering, a vital tool for analyzing sequential data across diverse fields, has evolved significantly from classical distance-based and feature-based approaches to modern neural network methods. This survey presents a unified taxonomy covering three main categories: (a) shape-based approaches that use raw time series with similarity measures like Dynamic Time Warping (DTW), Edit Distance on Real sequences (EDR), and Longest Common Subsequence (LCSS); (b) feature-based methods that transform series into vectors using statistical, Fourier, or wavelet features; and (c) model-based and deep learning approaches employing generative models or neural networks such as RNNs and CNNs to learn embeddings for clustering. Empirical results reveal no single superior method; traditional distance-based techniques excel in smaller, interpretable settings, while deep learning methods suit large, complex datasets despite challenges like computational cost and hyperparameter tuning. Key issues include defining robust similarity measures, scaling to vast and heterogeneous datasets, managing evolving data streams, and standardizing evaluation protocols. Future work emphasizes self-supervised representation learning, multi-modal clustering, interactive frameworks integrating expert feedback, and explainability in deep models. Summarizing, the survey consolidates advances, challenges, and prospects that will drive continued innovation in uncovering latent temporal patterns in complex sequential data."
24,"Zimeng Zhou, Chenyun Yu, Sarana Nutanong, Yufei Cui, Chenchen Fu, Chun Jason Xue, ""A Hardware-Accelerated Solution for Hierarchical Index-Based Merge-Join,"" IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 1, pp. 91-104, Jan. 2019. doi: 10.1109/TKDE.2018.2833615. [Online]. Available: https://ieeexplore.ieee.org/document/8317091","This paper presents a novel hardware-accelerated solution for efficiently processing hierarchical index-based merge-join queries in database systems, specifically targeting scenarios with low match rates. The approach leverages a hierarchical index structure to enable early pruning of non-matching entries, thereby reducing memory accesses and computation overhead. The designed hardware architecture, implemented on FPGAs, includes modules for index traversal, key comparison, and join result generation and exploits parallelism to optimize performance. Experimental results demonstrate up to a 5x speedup compared to traditional software implementations, with gains more significant as the join match rate decreases. The solution effectively addresses bottlenecks of traditional merge-join operators in handling large datasets with low join selectivity. Despite challenges such as hardware resource constraints and integration with existing systems, this work showcases the potential of hardware acceleration combined with hierarchical indexing to enhance database join operations, with future directions including extending support to other join predicates and dynamic data scenarios."
25,"Chaoyue Niu, Zhenzhe Zheng, Fan Wu, Xiaofeng Gao, Guihai Chen, ""Achieving Data Truthfulness and Privacy Preservation in Data Markets,"" IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 1, pp. 105-119, Jan. 2019. doi:10.1109/TKDE.2018.2850348. [Online]. Available: https://ieeexplore.ieee.org/document/8321498","The paper proposes TPDM, a framework that efficiently integrates data truthfulness and privacy preservation in data markets by combining cryptographic techniques and differential privacy. TPDM uses an Encrypt-then-Sign structure with partially homomorphic encryption and identity-based signatures, enabling batch verification, data processing, and outcome verification while maintaining data confidentiality and identity privacy. It employs homomorphic hashes for data signatures, zero-knowledge proofs to validate computations without revealing sensitive data, and differential privacy to protect contributors during data release. Instantiated for profile matching and distribution fitting services and evaluated on Yahoo! Music and 2009 RECS datasets, TPDM achieves accurate data truthfulness verification, strong privacy guarantees, and low computational and communication overhead, making it practical for large-scale data markets. The approach balances privacy and utility trade-offs, with potential extensions to streaming data and adaptive adversary robustness, thus laying a foundation for trustworthy, privacy-aware data trading platforms."
26,"G. Cong, W. You, and J. Gehrke, ""Machine Learning for Databases: Foundations, Paradigms, and Future Directions,"" ACM Trans. Database Syst., vol. 48, no. 1, pp. 1–45, 2024. [Online]. Available: https://dl.acm.org/doi/pdf/10.1145/3626246.3654686","Machine learning (ML) integration in database systems, termed ML4DB, is an emerging field aimed at enhancing database functionalities such as query optimization, indexing, and data integration by embedding ML models within database management systems (DBMS). This paper surveys foundational concepts and paradigms like learned indexes and ML-assisted query optimization, highlighting their ability to outperform classical methods by reducing storage and improving latency and plan efficiency. Key challenges include balancing model accuracy, efficiency, and interpretability, handling diverse and dynamic workloads, and integrating ML without compromising DBMS accuracy and consistency guarantees. Future work emphasizes expanding ML4DB to complex database functions, developing evaluation benchmarks, and improving model adaptation and explainability. Overall, ML4DB promises to revolutionize DBMS by leveraging adaptive, predictive ML models to enhance data processing capabilities."
27,"K. Huang, J. Zhang, J. Li, and W. Chen, ""The Past, Present and Future of Indexing on Persistent Memory,"" ACM Trans. Database Syst., vol. 48, no. 1, pp. 2:1–2:35, 2023. [Online]. Available: https://dl.acm.org/doi/pdf/10.14778/3554821.3554897","Persistent Memory (PM) is a transformative technology combining memory-like speed and byte-addressability with storage-level durability, reshaping indexing structures in databases and file systems. Traditional indexes like B-trees, hash tables, and skip lists must be adapted for PM to address challenges such as consistency, durability, crash recovery, and concurrency control. PM indexing techniques include logging, cache-line flushing and fencing, hardware transactional methods, and hybrid volatile-persistent structures. Benchmarks show PM-native indexes outperform disk-based indexes in update and recovery speed, approaching the throughput of volatile memory indexes despite PM's relatively slower writes. Critical trade-offs remain between speed and durability, complicated by hardware diversity, memory fragmentation, wear-leveling, and atomicity across cache lines. Future directions emphasize hybrid, adaptive, and hardware-aware indexing designs, alongside improved concurrency, recovery models, and security on PM. Overall, PM blends persistent storage and memory access speed, demanding innovative indexing algorithms that leverage its unique properties to fully realize its potential."
28,"Y. Zhang, H. Tang, and S. Chen, ""Longshot: Indexing Growing Databases using MPC and Differential Privacy,"" ACM Trans. Database Syst., vol. 47, no. 4, pp. 46:1–46:30, 2022. [Online]. Available: https://dl.acm.org/doi/pdf/10.14778/3594512.3594529","With the rapid growth of data in dynamic applications, Longshot offers a novel system for indexing evolving databases while ensuring privacy through secure multiparty computation (MPC) and differential privacy (DP). It addresses the limitations of static or trusted-environment assumptions by enabling private, incremental index updating using MPC protocols like garbled circuits and secret sharing combined with DP noise addition calibrated via privacy budgets $(\epsilon, \delta)$. Longshot's design integrates secure multiparty partial aggregation, incremental B+-tree indexing, and differential privacy mechanisms, allowing collaborative index construction without revealing sensitive data. Experimental results demonstrate practical performance with update times around 150 ms and query latencies near 20 ms under a privacy budget of $\epsilon=1.0$, outperforming baseline secure and non-private methods as summarized in the table $\begin{tabular}{l|c|c|c} \hline \textbf{Method} & \textbf{Update Time (ms)} & \textbf{Query Latency (ms)} & \textbf{Privacy Budget} \\ \hline \text{Longshot} & 150 & 20 & \epsilon=1.0 \\ \text{Baseline MPC} & 300 & 15 & \text{None} \\ \text{Non-private} & 50 & 10 & \text{None} \\ \hline \end{tabular}$. The system balances strong privacy guarantees with efficient query support, and its incremental protocol reduces computational overhead compared to full re-indexing. Limitations include scalability concerns and parameter tuning trade-offs, with future work aimed at supporting more complex queries, enhancing MPC efficiency, and adapting privacy dynamically, making Longshot a foundational step towards secure data management in sensitive, evolving database environments."
29,"G. Wu, “A case study for Adaptive Radix Tree index,” Information Systems, vol. 104, no. C, pp. 101–113, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S0306437921001228","Adaptive Radix Tree (ART) is an indexing technique that leverages adaptive multiway radix trees with dynamic node sizes (Node4, Node16, Node48, Node256) to optimize both lookup speed and memory usage. This paper presents a case study of ART, detailing its design and implementation, including optimizations like path compression and lazy expansion to enhance traversal efficiency. Compared to traditional B+ trees and hash indices, ART demonstrates up to a 10x faster lookup speed, especially in range and prefix queries, benefiting from its compact node representation and adaptive resizing. Evaluations across synthetic and real-world datasets confirm ART's superior throughput and reduced memory footprint. While ART shows robust performance gains, challenges remain in handling dynamic datasets where frequent updates require balanced node expansion and contraction. Future research aims to improve update mechanisms, concurrency control, and hybrid index integration. Overall, ART stands out as a promising index structure for in-memory databases, combining the efficiency of radix trees with adaptability to key distributions, achieving significant performance and storage benefits, as summarized in key findings like lookup speed improvements and memory optimization through adaptive node sizing and compression."
30,"N. Bahri, “On indexing evidential data,” Information Systems, vol. 84, pp. 1–14, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0888613X18303566","The paper addresses the challenge of efficiently accessing uncertain and imprecise data modeled by Evidence theory (Dempster-Shafer theory) by proposing a specialized indexing approach. Traditional indexing methods fail to manage the inherent uncertainty and imprecision, which this approach overcomes by structuring evidential data through belief functions and mass assignments and mapping them into an index optimized for query processing under uncertainty. Experimental results validate that this indexing significantly improves query response times and scalability compared to exhaustive searches. The paper discusses trade-offs related to index construction overhead and granularity, while acknowledging challenges such as the combinatorial explosion of mass functions and maintaining efficiency in large evidential databases. Future work aims to extend the method to dynamic data and more complex query types. Overall, this work contributes a novel uncertainty-aware indexing structure that enhances retrieval performance in evidential databases and supports further advances in uncertain data management."
31,"J. M. Medina, J. F. Nieves, and J. A. Pulido, “Indexing techniques to improve the performance of database systems: Overview and current trends,” Information Systems, vol. 72, pp. 42–56, 2018. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S030643791830097X","This paper provides a comprehensive overview of database indexing techniques, examining classical structures like B-Trees, hash indexes, and bitmap indexes alongside recent developments tailored for big data, distributed, columnar, and NoSQL systems. It emphasizes the importance of indexing in reducing disk access for query efficiency and explores a range of indexing categoriestree-based, hash-based, bitmap, and specialized multidimensional and text indexeswith analyses of their storage overhead, update costs, and query performance. Key findings highlight that B-Trees remain dominant for structured data due to balanced costs, hash indexes suit equality searches, bitmap indexes excel in read-intensive scenarios, and specialized methods like LSM-Trees and distributed B-Trees enhance scalability and write performance in big data contexts. The discussion stresses aligning indexing strategies with workload patterns, addressing challenges such as maintaining consistency in distributed environments, and integrating indexing with query optimizers and modern hardware advancements. Challenges include handling dynamic large-scale data, supporting complex query types, and balancing consistency and availability. Future research directions involve adaptive, self-tuning indexes, machine learning integration for optimization, and indexes for emerging data models like graph databases and multimedia queries. The paper concludes that effective indexing is pivotal for database performance and must evolve continuously with changing data characteristics and technologies, providing a state-of-the-art reference as of 2018."
32,"A. Sharma, S. Hajj, and B. Bhuyan, ""PalmHashNet: Palmprint Hashing Network for Indexing Large Databases to Boost Identification,"" IEEE Access, vol. 9, pp. 43522-43534, 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9585462/","This paper introduces PalmHashNet, a deep learning-based palmprint hashing network designed to generate compact binary hash codes for efficient indexing and identification within large-scale palmprint databases. Utilizing a convolutional neural network combined with a hashing layer, PalmHashNet optimizes a multi-task loss function $L = L_{\text{classification}} + \lambda L_{\text{hashing}}$ to ensure intra-class compactness and inter-class separability in Hamming space. This allows fast similarity searches through simple Hamming distance calculations, greatly enhancing search efficiency and scalability in biometric systems. Experiments on datasets like IITD and CASIA demonstrate that PalmHashNet achieves superior identification accuracy (95.7%) and significantly reduced search time (45 ms) compared to traditional CNN methods (92.3% accuracy, 350 ms). The hashing approach contributes to memory efficiency and accelerated queries, albeit with challenges related to quantization errors and robustness to variations such as illumination and rotation. Overall, PalmHashNet offers a practical solution for real-time large-scale palmprint recognition by effectively balancing recognition performance and computational efficiency."
33,"G. P. Licks and F. Meneguzzi, ""Automated Database Indexing using Model-free Reinforcement Learning,"" arXiv preprint arXiv:2007.14244, Jul. 2020. [Online]. Available: https://arxiv.org/abs/2007.14244","This paper proposes a model-free reinforcement learning (RL) framework for automated database indexing that formulates index selection as a Markov Decision Process, optimizing query latency while managing storage costs without relying on traditional cost models or database internals. The RL agent, trained using Proximal Policy Optimization (PPO), learns indexing policies from query execution feedback, balancing query performance gains and index storage overhead through a reward function. Evaluated on benchmarks such as TPC-H and Join Order Benchmark with PostgreSQL, the RL approach achieved up to 30% reduction in average query latency compared to heuristic and cost-model-based methods and adapted effectively to workload variations. Challenges include handling large state-action spaces, noisy reward signals, and training overhead, while future work aims at transfer learning, hierarchical RL, and extension to distributed systems. Overall, the approach demonstrates robust, adaptive, and autonomous database tuning by learning directly from query latencies and storage costs, offering an effective alternative to conventional index tuning methods."
34,"A. Al-Mamun, H. Wu, Q. He, J. Wang, and W. G. Aref, ""A Survey of Learned Indexes for the Multi-dimensional Space,"" arXiv preprint arXiv:2403.06456, Mar. 2024. [Online]. Available: https://arxiv.org/abs/2403.06456","This survey comprehensively reviews the state-of-the-art in learned multidimensional indexing, focusing on methods that leverage machine learning models to predict data positions within multidimensional spaces, improving upon traditional spatial indexes like R-trees and kd-trees. The taxonomy categorizes approaches into model-based grid partitioning, tree-based learned indexes, hybrid methods, and direct multi-index modeling using neural networks or learned hash functions. Key algorithms include recursive model indexes (RMI) and piecewise linear models, supporting range and nearest neighbor queries. Benchmarks reveal that learned indexes can outperform classical counterparts in query latency and memory use, particularly under skewed data distributions, though challenges remain with high-dimensional and dynamic data involving frequent updates. The discussion highlights a trade-off between indexing accuracy and adaptability, impacted by inference overhead and the curse of dimensionality, suggesting integration with classical indexes requires careful design. The paper identifies critical challenges including dynamic updates, scalability beyond 3-4 dimensions, robustness to workload changes, and benchmarking standards. Promising future work includes adaptive self-tuning indexes, advanced deep learning architectures, integration with approximate query processing, and support for multi-modal data types. Overall, learned multidimensional indexing is positioned as a rapidly advancing field with substantial potential to enhance spatial and multi-feature data management across diverse applications."
35,"S. Nakazono, Y. Bessho, H. Kawashima, and T. Nakamori, ""Griffin: Fast Transactional Database Index with Hash and B+-Tree,"" arXiv preprint arXiv:2407.13294, Jul. 2024. [Online]. Available: https://arxiv.org/abs/2407.13294","The paper proposes Griffin, a hybrid transactional database index combining a hash table and a B+-tree (implemented via BwTree) to optimize point and range operations. The hash table provides O(1) access for point queries, while the BwTree supports efficient range queries. To ensure serializability without extra B+-tree traversals typically required for phantom avoidance, Griffin employs a precision locking mechanism that locks minimal index regions during range queries. This hybrid architecture transparently delivers linearizable operations through a single-index interface. Evaluations show Griffin achieves up to 3.1x higher throughput for point operation dominant workloads and up to 5.4x for range operation dominant workloads compared to a BwTree-only baseline. The precision locking approach notably reduces synchronization overhead, balancing performance gains against added complexity. Overall, Griffin effectively integrates the strengths of hash tables and B+-trees, representing a promising solution for transactional database indexing with mixed workloads."
