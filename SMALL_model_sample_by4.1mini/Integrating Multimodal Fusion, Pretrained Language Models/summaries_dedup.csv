Index,Citation,Summary
1,"C. Du, K. Fu, and H. He, ""Human-like object concept representations emerge naturally in multimodal large language models,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 548–559, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00435-2","Understanding how humans conceptualize natural objects provides insights into cognition, and this study investigates whether Large Language Models (LLMs) and Multimodal LLMs (MLLMs) develop similar object representations. Using 4.7 million triplet similarity judgments on 1,854 natural objects, the authors derived 66-dimensional sparse, non-negative embeddings via the Sparse Positive Similarity Embedding (SPoSE) method. These embeddings closely predict human choice behavior in odd-one-out tasks and exhibit semantic clustering that parallels human conceptual structures. Dimensions include semantic categories (e.g., animals, food) and perceptual features (e.g., hardness, texture), with MLLMs additionally capturing spatial and color attributes. Representational Similarity Analysis demonstrated strong alignment between these model embeddings and neural activity in category-selective brain regions such as EBA, PPA, RSC, and FFA. The results show that while LLM representations are not identical to human ones, they share fundamental semantic and perceptual dimensions reflecting key aspects of human conceptual knowledge. This convergence highlights the potential of multimodal LLMs to form human-like conceptual embeddings that can inform the development of more human-like artificial cognitive systems, advancing understanding in both cognitive neuroscience and AI."
2,"F. P. Mahner, L. Muttenthaler, and M. N. Hebart, ""Dimensions underlying the representational alignment of deep neural networks with humans,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 575–588, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00437-4","This study investigates how deep neural networks (DNNs) align with human cognitive representations by analyzing similarity structures in visual and semantic domains using behavioral data and computational modeling. Representational similarity analysis (RSA) combined with dimensionality reduction techniques like PCA and CCA revealed that alignment is governed by latent dimensions reflecting visual features such as shape and texture in early DNN layers and semantic category membership in higher layers. The variance in alignment explained by these dimensions varies across model architectures and object categories, with models trained on semantic tasks showing stronger semantic alignment, particularly for naturalistic stimuli. Despite this hierarchical alignment, some representational dimensions remain uniquely human and underrepresented in models, pointing to challenges including dataset biases and the complex interplay of semantic versus visual information. The work emphasizes the need for multimodal, context-aware models and enriched behavioral datasets to better capture human representational complexity, ultimately contributing insights toward more human-like AI systems."
3,"R. Whitehead, A. Nguyen, and S. Järvelä, ""Utilizing Multimodal Large Language Models for Video Analysis of Posture in Studying Collaborative Learning: A Case Study,"" Journal of Learning Analytics, vol. 12, no. 1, pp. 186–200, 2025. [Online]. Available: https://doi.org/10.18608/jla.2025.8595","Incorporating non-verbal data streams is crucial for understanding interaction dynamics in collaborative learning environments, yet their complexity, especially when gathered in the wild, requires advanced analysis methods. The study addresses this by exploring how multimodal large language models (MLLMs), empowered by generative AI, can be utilized for feature extraction of postural behavior within multimodal learning analytics. Using a case study of 52 pre-service teachers in a physics-based collaborative task, the research demonstrates the potential of MLLMs to efficiently process complex non-verbal data. This integration of generative AI into learning research opens new avenues for deeply understanding and enhancing collaborative learning interactions."
4,"R. AlSaad, A. Abd-alrazaq, S. Boughorbel, A. Ahmed, M.-A. Renault, R. Damseh, and J. Sheikh, ""Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook,"" Journal of Medical Internet Research, vol. 26, 2024, Art. no. e59505. [Online]. Available: https://www.jmir.org/2024/1/e59505/","The paper presents a comprehensive review of multimodal large language models (M-LLMs) in health care, highlighting their potential to integrate diverse medical data typessuch as images, time-series data, audio, text, videos, and omics datato enhance clinical decision-making. It outlines foundational principles including modality-specific encoders, embedding alignment, cross-modal interaction, and pretraining/fine-tuning approaches. The review covers practical applications like CONCH, a vision-language model for pathology, and discusses significant technical challenges such as complex data fusion, large dataset requirements, computational scalability, and interpretability, along with ethical concerns including bias, informed consent, privacy, and safety. Proposed solutions involve advanced fusion methods, synthetic data and federated learning, parameter-efficient tuning, interpretability frameworks, diversified and bias-mitigated data, improved patient communication, privacy safeguards, and dynamic ethical alignment. Future directions emphasize generating multimodal outputs beyond text, establishing domain-specific benchmarks, evolving temporal explainability, improving interoperability in hospital systems, developing regulatory frameworks, and fostering multidisciplinary collaboration. The paper argues that M-LLMs mark a paradigm shift toward integrated, data-driven medical AI capable of improving diagnosis, treatment, and operational efficiency, but stresses the need for ongoing research, ethical oversight, and practical clinical evaluation to realize their transformative promise safely and equitably."
5,"J. Wu, W. Gan, Z. Chen, S. Wan, and P. S. Yu, ""Multimodal Large Language Models: A Survey,"" in IEEE BigData 2023, 2023, pp. 1–10. [Online]. Available: https://arxiv.org/abs/2311.13165","This paper surveys multimodal large language models (MLLMs), which integrate diverse data types such as images, text, audio, and video to overcome the limitations of traditional text-based large language models. It covers fundamental concepts, historical development, major products, and technical architectures including foundation models like vision and audio transformers, multimodal fusion techniques such as early fusion, late fusion, and cross-modal attention, as well as training strategies like multitask learning and contrastive learning. The survey reviews performance gains demonstrated in tasks including image captioning, visual question answering, and audio-visual speech recognition, highlighting advances from cross-modal attention and instruction tuning. Challenges addressed include the scarcity of large annotated multimodal datasets, computational demands, modality alignment complexities, evaluation difficulties, and ethical issues including bias and privacy. Future directions emphasize unified architectures, improved self-supervised learning, model efficiency, cross-disciplinary inspirations, interpretability, and ethical AI development. Overall, MLLMs represent a promising frontier in AI, enabling richer multimodal understanding and generation with broad applications, but requiring continued research to address existing challenges and responsibly harness their potential."
6,"S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, ""A Survey on Multimodal Large Language Models,"" Nat. Sci. Rev., vol. 11, no. 6, 2023, doi: 10.1093/nsr/nwae403. [Online]. Available: https://arxiv.org/abs/2306.13549","The paper ""A Survey on Multimodal Large Language Models"" presents a comprehensive overview of recent developments in MLLMs, which integrate large language models with multiple data modalities such as images, audio, and video. It surveys foundational architectures categorized into single-stream, dual-stream, and modular types, including models like Flamingo, PaLM-E, and GPT-4, highlighting their trade-offs in performance and scalability. The training methodologies discussed encompass joint multimodal pretraining, sequential fine-tuning, and instruction tuning, utilizing combined loss functions that mix language modeling with contrastive or reconstruction losses tailored to vision and audio data. Diverse evaluation benchmarksranging from VQA to image captioning and audio classificationare examined alongside metrics like accuracy, BLEU, METEOR, CIDEr, and retrieval recall, with attention to challenges in reasoning and cross-modal generalization. The paper addresses the critical issue of multimodal hallucination, classifying its types, proposing detection and mitigation techniques such as data curation, calibration, and human feedback alignment. Advanced techniques including prompt tuning, multimodal chain-of-thought reasoning, and knowledge injection are reviewed, while core challenges like scalability, data scarcity, cross-modal alignment, robustness, and interpretability are detailed. Future directions emphasize enhanced multimodal reasoning, commonsense integration, expanded modality support, spatiotemporal benchmarks, hallucination reduction, and efficient training approaches. The survey concludes by underscoring the transformative potential of MLLMs and the necessity for interdisciplinary collaboration to advance the design and deployment of future multimodal AI systems."
7,"J. Sublime, ""The AI Race: Why Current Neural Network-based Architectures are a Poor Basis for Artificial General Intelligence,"" Journal of Artificial Intelligence Research, vol. 80, 2024. [Online]. Available: https://jair.org/index.php/jair/article/view/15315","Artificial General Intelligence (AGI), the concept of an AI surpassing human intellect, remains elusive despite rapid advances in deep learning and neural networks that excel at specialized tasks such as image recognition and natural language processing. This paper analyzes why current deep neural network architecturesincluding deep feedforward networks, recurrent networks, and transformersare inherently limited for AGI due to poor compositional generalization, brittleness outside training distributions, interpretability challenges, and scalability issues. Through literature review, theoretical analysis, and empirical evidence from benchmark tasks, it reveals failures in abstraction and learning efficiency critical for AGI. The discussion highlights that purely statistical pattern recognition lacks essential cognitive faculties, advocating for symbolic reasoning, explicit knowledge structures, and meta-learning as necessary complements. Identified challenges include overcoming entangled representations, ensuring robustness, achieving incremental learning without catastrophic forgetting, and bridging symbolic and sub-symbolic methods. Future directions emphasize hybrid neuro-symbolic architectures, cognitive theory-inspired models, meta-learning, and lifelong learning frameworks, arguing for a paradigm shift from mere scaling to incorporating reasoning, abstraction, and transparency. The conclusion stresses that despite successes, neural networks alone are insufficient bases for AGI, urging interdisciplinary, conceptual innovation over incremental scaling to realize true general intelligence."
8,"E. La Malfa, A. Petrov, S. Frieder, C. Weinhuber, R. Burnell, R. Nazar, A. Cohn, N. Shadbolt, and M. Wooldridge, ""Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,"" Journal of Artificial Intelligence Research, vol. 80, 2024, Aug. 26. DOI: 10.1613/jair.1.15865. [Online]. Available: https://jair.org/index.php/jair/article/view/15865","The paper addresses the emerging paradigm of Language-Models-as-a-Service (LMaaS), where state-of-the-art proprietary language models are accessed remotely via API rather than local deployment. While this model provides scalability, ease of use, and maintenance benefits, it introduces challenges in transparency, reproducibility, reliability, and trustworthiness due to limited access to model internals. The authors systematically analyze issues related to benchmarking, evaluation, data privacy, and intellectual property, highlighting how the black-box nature of LMaaS complicates performance assessment and user control. They review current services, licensing schemes, and technical architectures, and emphasize risks of vendor lock-in and ethical concerns over user data and model updates. Concluding that LMaaS heralds a shift in AI deployment, the paper advocates for future work on privacy-preserving mechanisms, explainability tools, standardized black-box benchmarks, and regulatory frameworks to ensure transparent, accountable, and sustainable LMaaS ecosystems. The study integrates technical, legal, and ethical dimensions and provides synthesized tables of LMaaS licenses and architectural diagrams to support its comprehensive overview."
9,"P. Moschoula, P. Singh, A. Chakraborty, Y. Oruganti, M. Milletari, S. Bapat, and K. Jiang, ""The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,"" Journal of Artificial Intelligence Research, vol. 80, 2024, Aug. 26. DOI: 10.1613/jair.1.15960. [Online]. Available: https://jair.org/index.php/jair/article/view/15960","This paper presents a comprehensive taxonomy, termed the RL/LLM Taxonomy Tree, categorizing methods and research directions at the intersection of reinforcement learning (RL) and large language models (LLMs). It systematically reviews state-of-the-art techniques such as RL fine-tuning of LLMs with methods like RLHF and PPO, LLMs as RL agents performing sequential decision making, and hybrid frameworks integrating multimodal inputs. The taxonomy serves to structure the rapidly evolving field by delineating branchesRL for LLMs, LLMs for RL, and hybrid systemsbased on interaction style, training objectives, and applications. Key results demonstrate improved model alignment and multi-step reasoning capabilities, though challenges remain in sample inefficiency, reward design complexity, interpretability, scalability, and safety concerns. Future directions focus on developing sample-efficient RL algorithms, novel reward modeling, integration of symbolic reasoning, scalable hardware, lifelong learning paradigms, and ethical safeguards. The work offers critical insights and a roadmap to advance synergy between RL and large-scale language modeling, aiming to foster innovation and collaboration across disciplines."
10,"N. Pontikos, W. A. Woof, and M. Michaelides, ""Next-generation phenotyping of inherited retinal diseases from multimodal imaging with Eye2Gene,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 594–608, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-01040-8","Rare inherited retinal diseases (IRDs), which are a leading cause of childhood and working-age blindness, pose challenges for timely genetic diagnosis critical for effective treatment. The authors present Eye2Gene, a deep learning algorithm trained on a large multimodal dataset (n = 2,451 patients, 63 gene classes) comprising fundus autofluorescence (FAF), infrared (IR), and spectral-domain optical coherence tomography (SD-OCT) images. Eye2Gene integrates predictions from modality-specific CoAtNet0 convolutional neural networks, employing ensemble averaging and weighted cross-entropy loss to address data imbalance, achieving a top-five gene prediction accuracy of 83.9% across internal and external validations from five international centers. This performance surpasses eight expert ophthalmologists tested on FAF images (29.5% top-five accuracy versus Eye2Gene's 76%). Eye2Gene also improves gene prioritization compared to Human Phenotype Ontology-based methods in 75% of a 130-patient cohort. Visualization of penultimate network layer embeddings using UMAP reveals clustering consistent with genotype and phenotype similarity, including for genes unseen during training, facilitating novel gene-phenotype discovery. Additionally, a binary classifier discriminates IRD from similar non-genetic retinal conditions with AUROC of 0.98. Interpretability is enhanced with attention maps highlighting image regions influencing predictions. Limitations include a bias toward common genes (63 of ~281 known IRD genes) and comparison with experts limited to images without clinical context. The tool is accessible online for research (app.eye2gene.com) and aims to augment genetic testing workflows by accelerating diagnosis, variant prioritization, and expanding phenotype-driven gene discovery. Eye2Gene exemplifies AI's translational potential in precision medicine for rare retinal disorders, with future work planned to expand gene coverage, incorporate federated learning, and augment training datasets to address rare genes and ancestry diversity."
11,"G. Feretzakis, A. Rivas, S. D. Georgakopoulos, and S. Mitrokotsa, ""Trustworthy AI: Securing Sensitive Data in Large Language Models,"" AI, vol. 5, no. 4, p. 134, 2024. [Online]. Available: https://www.mdpi.com/2673-2688/5/4/134","This paper introduces a comprehensive framework to embed trust mechanisms into Large Language Models (LLMs) for securing sensitive data by controlling disclosure dynamically based on user trust profiles and data sensitivity. It consists of three interconnected modules: User Trust Profiling, Information Sensitivity Detection, and Adaptive Output Control. The User Trust Profiling module combines Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC) to assign dynamic trust scores based on credentials, behavior, and context. Information Sensitivity Detection employs Named Entity Recognition (NER) enhanced with domain-specific dictionaries and semantic analysis to identify sensitive information with over 92% accuracy. Adaptive Output Control then modulates response detail using differential privacy, redaction, and summarization to balance information utility and privacy. Experimental evaluation in healthcare and finance showed accurate trust-based disclosure control, minimal latency increase (<12%), and improved privacy compliance. Challenges include accurate real-time trust estimation, handling ambiguous or emerging sensitive data, and minimizing computational overhead. Future work aims to enhance trust modeling with machine learning, extend sensitivity detection to multi-modal inputs, optimize differential privacy parameters contextually, and develop explainability features. Overall, the framework provides a pioneering dynamic, privacy-preserving approach for trustworthy LLM deployments in critical regulated domains, combining RBAC, ABAC, NER, semantic analysis, and differential privacy to secure sensitive data while maintaining utility."
12,"J. N. Acosta, G. J. Falcone, P. Rajpurkar, and E. J. Topol, ""Multimodal biomedical AI,"" Nature Medicine, vol. 28, pp. 1773–1784, 2022. [Online]. Available: https://doi.org/10.1038/s41591-022-01981-2","The paper reviews the burgeoning field of multimodal artificial intelligence (AI) in biomedical applications, enabled by the increasing availability of diverse data types such as biobank records, medical imaging, wearable sensors, and multi-omics sequencing. It highlights the shift from traditional single-modality AI models to multimodal approaches integrating genetic, proteomic, microbiome, imaging, clinical, and environmental data to capture the complexity of human health and disease. Key technical innovations include fusion strategies for multimodal data, transformer architectures like Perceiver, and approaches to handle missing or irregular data. Applications span personalized precision health, digital clinical trials with real-time monitoring, remote patient surveillance, pandemic tracking, digital twin modeling, and virtual health assistants. The paper outlines significant challenges around data harmonization, high-dimensional integration, interpretability, privacy-preserving techniques (e.g., differential privacy, federated learning, homomorphic encryption), ethical considerations, and computational scalability. Future directions emphasize developing pretrained multimodal models, curated high-quality datasets, large-scale data sharing collaborations under strict privacy, and rigorous clinical validation including randomized controlled trials. The authors conclude that with multidisciplinary collaboration and advances in AI architectures and privacy frameworks, multimodal biomedical AI holds transformative potential to enhance personalized medicine and health outcomes across diverse clinical domains."
13,"S. Sun, W. An, F. Tian, F. Nan, Q. Liu, J. Liu, N. Shah, and P. Chen, ""A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future,"" arXiv preprint arXiv:2412.14056, Dec. 2024. [Online]. Available: https://arxiv.org/abs/2412.14056","Artificial intelligence has advanced rapidly, raising challenges in interpreting black-box AI models, especially in contexts involving multimodal data fusion and complex reasoning. Multimodal eXplainable AI (MXAI) emerges to enhance transparency by integrating explanations across diverse data types, evolving through four eras: traditional machine learning (featuring handcrafted features, rule-based and intrinsic interpretable models), deep learning (introducing neural networks with gradient-based and visualization post-hoc methods), discriminative foundation models (leveraging large pretrained multimodal representations and attention-based explainability), and generative large language models (addressing new explanation challenges via dataset curation, graph modeling, and generative post-hoc reasoning). Evaluation of MXAI involves metrics for faithfulness, comprehensibility, and human-grounded assessments, with an emphasis on standardized benchmarks. Key challenges include managing multimodal heterogeneity, ensuring unbiased faithful explanations, and aligning explanations with human cognition through causality and counterfactual approaches. Future directions stress mitigating hallucinations in large models, improving multimodal and cognitive alignment, handling lack of ground truth in explanations, and integrating symbolic reasoning. This comprehensive review establishes foundational knowledge for building transparent, interpretable, and trustworthy MXAI systems compatible with advancing AI paradigms, notably large language models."
14,"W. Fedus, B. Zoph, and D. P. Kingma, ""Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,"" Journal of Machine Learning Research, vol. 23, no. 1, pp. 1–39, 2022. [Online]. Available: https://jmlr.org/papers/volume23/21-0998/21-0998.pdf","The paper introduces the Switch Transformer, a sparse Mixture-of-Experts (MoE) model variant that routes each token to a single expert rather than multiple experts, substantially reducing memory and computational overhead compared to prior MoE architectures. This is achieved via a learned gating mechanism producing a softmax over experts, selecting one expert per token, simplifying routing and enabling scaling to trillion-parameter models with efficient training and inference. The architecture consists of replacing the dense feed-forward layers in Transformers with sparse MoE layers, each expert being a feed-forward subnetwork, and incorporates a load balancing loss to ensure uniform token distribution across experts. Experiments show Switch Transformers outperform dense models and previous MoE setups on language modeling, multilingual translation, and zero-shot tasks, achieving better perplexity and BLEU scores while using fewer FLOPs. Scaling up to 1.6 trillion parameters yields improvements in zero-shot learning and fine-tuning while reducing costs. The paper highlights that by activating only one expert per token, communication bottlenecks and engineering complexity are greatly diminished, although single-expert routing may reduce expressivity compared to multi-expert mixtures. The model trains stably using standard objectives and optimizers such as Adam alongside methodological innovations like expert warmup and initialization strategies. Future avenues include exploring dynamic routing policies, multi-modal applications, and improved load balancing methods. Overall, the Switch Transformer provides a simple, scalable, and efficient sparse architecture advancing large-scale language model training and deployment, with the key conceptual equation for gating defined as $g(x) = \text{softmax}(W_g x)$ selecting a single expert to process token representation $x$, and the load balancing loss encouraging even expert utilization. This work paves the way for practical trillion-parameter models leveraging sparse expert modules to reduce computational resources without sacrificing performance."
15,"J. Pérez, R. L. Uria, P. Pollakis, J. Marecek, K. Muroya, and N. Durrani, ""Attention is Turing Complete,"" Journal of Machine Learning Research, vol. 22, no. 1, pp. 1–24, 2021. [Online]. Available: https://jmlr.org/papers/volume22/20-302/20-302.pdf","This paper proves that Transformer architectures with the standard self-attention mechanism are Turing complete under reasonable assumptions such as infinite precision and adequate resources. By formalizing attention computations, the authors construct explicit simulations of arbitrary Turing Machines, encoding tape symbols and machine states into vector components and using multi-headed attention combined with feed-forward layers to implement the transition function and tape updates. They show that attention weights can effectively read, write, and route information across tape cells, enabling Transformers to perform any computation a classical Turing Machine can. This theoretical result enhances understanding of the expressive power of attention networks beyond finite automaton capabilities, indicating potential for universal computation in natural language processing and machine learning tasks, albeit under idealized conditions. The paper discusses the gap between theory and practical constraints such as finite precision, parameter quantization, and training limitations, and suggests future work on approximate Turing completeness, efficient attention mechanisms, and empirical investigations into algorithmic learning using Transformers. The finding is summarized by demonstrating that with a finite number of attention layers and heads, a Transformer can simulate arbitrary state transitions of a Turing Machine, significantly advancing theoretical insight into attention-based model capabilities."
16,"C. Raffel et al., ""Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"" Journal of Machine Learning Research, vol. 21, no. 1, pp. 1–67, 2020. [Online]. Available: https://jmlr.org/papers/volume21/20-074/20-074.pdf","The paper introduces the Text-to-Text Transfer Transformer (T5), a unified framework that reformulates all natural language processing (NLP) tasks as text-to-text problems, enabling a consistent model architecture, pre-training objective, and decoding approach across tasks such as translation, question answering, and classification. T5 is built on a standard Transformer encoder-decoder architecture and pre-trained on the large Colossal Clean Crawled Corpus (C4) using a span corruption objective, where masked spans in the input are predicted as output. Models range in size up to 11 billion parameters, and scaling model size and training data consistently improves performance. The approach benefits from multitask fine-tuning and varied decoding strategies like beam search. Empirically, T5 achieves state-of-the-art results on benchmarks including GLUE, SuperGLUE, and multiple question answering datasets, highlighting effectiveness across diverse tasks. The paper discusses challenges such as computational cost, task balancing for multitask learning, and robustness, and outlines future work toward more efficient architectures, adaptive fine-tuning, multimodal extensions, and incorporation of external knowledge. Overall, T5 demonstrates that a single, unified text-to-text framework with consistent training and architecture can provide strong, scalable performance on a wide range of NLP tasks, laying a foundation for future unified transfer learning models."
17,"Z. Li, Y. Han, T. Liu, C. Ding, Q. Li, and J. Yin, ""Transformer-based Context-Aware Feature Interactions for Click-Through Rate Prediction,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5433–5446, Sept. 2022. doi: 10.1109/TPAMI.2021.3123474. Available: https://ieeexplore.ieee.org/document/9481941","This paper presents Context-aware Feature Interaction Transformer (CFIT), a novel transformer-based model for click-through rate (CTR) prediction that explicitly incorporates context information into the self-attention mechanism to capture dynamic and rich feature interactions. CFIT enhances traditional transformer self-attention by introducing a context-aware attention mechanism where attention weights are computed as $\tilde{a}_{ij} = \frac{(W_Q x_i)(W_K x_j)^T}{\sqrt{d}} + f(c, x_i, x_j)$, with $f$ as a learnable function adjusting for context $c$, allowing adaptive modeling of feature dependencies conditioned on diverse context fields such as temporal, device, and location specifics. The model embeds user, item, and context features into dense vectors and stacks multiple Context-aware Self-Attention (CASA) layers to capture higher-order interactions, followed by aggregation and sigmoid prediction layers optimized with binary cross-entropy loss. Extensive experiments on benchmark datasets (Criteo, Avazu, and an industrial ecommerce dataset) show CFIT consistently outperforms state-of-the-art baselines including DeepFM, xDeepFM, and transformer methods without context attention, achieving around 0.6% absolute AUC improvement. Ablation studies validate the importance of the context-aware attention component and the architectures efficiency, which remains comparable to standard transformers, suitable for large-scale deployment. The study highlights challenges like the need for quality context features and managing model complexity but suggests future work on richer context encoding and model compression. Overall, CFIT sets a new standard for CTR prediction by effectively leveraging context to yield more informative and discriminative feature interactions in recommender and advertising systems."
18,"H. Wu, W. Wang, F. Wang, X. Chen, and W. Chen, ""End-to-End Transformer-Based Framework for Facial Action Unit Detection,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 3, pp. 1196–1209, Mar. 2022. doi: 10.1109/TPAMI.2020.3033120. Available: https://ieeexplore.ieee.org/document/9520264","This paper introduces a novel end-to-end transformer-based framework for Facial Action Unit (AU) detection that leverages the global self-attention mechanism to model relationships across the entire face directly from images, bypassing the need for facial landmark detection or AU region annotations. The approach divides input face images into fixed-size patches embedded as tokens processed by a multi-layer transformer encoder, which captures spatial dependencies and AU co-occurrence patterns to predict AU presence via a multi-label classification head. Experiments on BP4D and DISFA datasets show superior F1 scores over state-of-the-art CNN and region-based methods, with enhanced robustness to pose variations and occlusions. This landmark-free method simplifies the pipeline and mitigates error propagation from landmark localization. Attention maps also provide interpretability by highlighting facial regions relevant to each AU. The framework achieves state-of-the-art accuracy while maintaining computational efficiency, and future work aims to incorporate temporal video information and develop lightweight models for real-time application."
19,"Y. Chu, Y. Zhang, Q. Wang, L. Zhang, X. Wang, Y. Wang, D. R. Salahub, Q. Xu, J. Wang, X. Jiang, Y. Xiong, and D.-Q. Wei, ""A transformer-based model to predict peptide–HLA class I binding and optimize mutated peptides for vaccine design,"" Nature Machine Intelligence, vol. 4, pp. 300–311, Mar. 2022. [Online]. Available: https://doi.org/10.1038/s42256-022-00459-7","The paper introduces TransMut, a framework combining TransPHLA, a transformer-based model for peptide-HLA (pHLA) binding prediction, and an automatically optimized mutated peptides (AOMP) program for peptide vaccine design. TransPHLA outperforms 14 existing methods by leveraging self-attention mechanisms to capture key sequence features influencing binding affinity between peptides and HLA alleles. Using the attention scores from TransPHLA, AOMP iteratively mutates peptides to enhance binding affinity while preserving sequence homology, thus facilitating rational vaccine optimization. Benchmark tests on neoantigen and HPV vaccine datasets confirm improved prediction accuracy and successful peptide optimization for vaccine design. The study highlights the potential of transformer architectures in understanding peptide-HLA interactions and demonstrates a novel pipeline for automated peptide vaccine candidate generation, with future work aimed at incorporating structural data, class II HLA binding, and experimental validation."
20,"J. Born and M. Manica, ""Regression Transformer enables concurrent sequence regression and generation for molecular language modelling,"" Nature Machine Intelligence, vol. 5, no. 4, Apr. 2023. [Online]. Available: https://www.nature.com/natmachintell/volumes/5/issues/4","The paper introduces the Regression Transformer (RT), a multitask language model that unifies regression and conditional sequence generation by framing regression as a conditional sequence modeling problem. Built upon the XLNet architecture, the RT tokenizes continuous numerical properties into sequences preserving decimal order and incorporates numerical encodings to instill an inductive bias reflecting numerical proximity. It employs an alternating training scheme combining permutation language modeling, property prediction, and conditional generation objectives, enhanced by a self-consistency loss to ensure fidelity between generated sequences and primed property values. The RT achieves competitive or superior performance compared to conventional regression models in predicting biochemical properties such as drug-likeness (QED), protein fluorescence and stability, and reaction yield, while uniquely enabling conditional molecular and protein generation constrained by desired property values. Notably, in benchmarks including MoleculeNet and constrained molecule generation, the RT surpasses specialized models in generating novel molecules optimized for properties like penalized logP, maintaining high novelty (>99%) and structural similarity to seed molecules. Its ability to seamlessly switch between predictive and generative tasks without separate heads, support multi-property conditioning, and apply across small molecules, proteins, and reactions suggests broad applicability in property-driven design and scientific discovery. The RT advances foundation models in materials design by integrating continuous property regression with conditional sequence generation in a unified transformer framework."
21,"Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, ""Efficient Transformers: A Survey,"" ACM Computing Surveys, vol. 55, no. 6, Article 109, pp. 1–28, 2022. [Online]. Available: https://doi.org/10.1145/3530811","Transformer architectures, widely impactful in NLP, vision, and RL, face a quadratic complexity bottleneck in self-attention limiting scalability. This paper surveys recent ""X-former"" variants designed for efficiency, categorizing them into sparse attention (e.g., local windows, global tokens, learned masks), low-rank and kernel-based approximations (e.g., Performers kernelized linear attention), memory/recurrence mechanisms (e.g., Transformer-XLs recurrence, compressive transformers), model compression (quantization, pruning, distillation), and hardware/software optimizations. These models achieve comparable performance to standard transformers with greatly improved speed and memory usage, as shown on benchmarks like language modeling and long-range tasks. Remaining challenges include devising universal efficient transformers, principled accuracy-efficiency trade-offs, standardized benchmarks, theoretical expressiveness, and hybrid designs combining multiple efficiency strategies. Future work aims to integrate sparsity and kernels, improve robustness, and co-design hardware/software solutions. Overall, the survey consolidates advances addressing the $O(n^2)$ complexity in transformers to enable scaling to longer sequences under limited resources."
22,"S. Khan, M. Naseer, M. Hayat, S. Zamir, F. Siddiqui, and M. Shah, ""Transformers in Vision: A Survey,"" ACM Computing Surveys, vol. 54, no. 10s, Article 200, pp. 1–41, 2022. [Online]. Available: https://doi.org/10.1145/3505244","This survey provides a comprehensive overview of Transformer models applied to computer vision, detailing their evolution, architectures, and applications. Unlike convolutional neural networks (CNNs) which focus on local features via convolutions, Transformers leverage self-attention to capture long-range dependencies and global context, enabling superior performance in tasks like image classification, object detection, and segmentation. The paper discusses various Vision Transformer (ViT) architectures such as ViT, DeiT, and Swin Transformer, highlighting architectural innovations like hierarchical feature extraction and shifted windows. Input images are processed into patches and embedded before passing through Transformer encoder layers, with training typically involving large dataset pretraining followed by fine-tuning. Empirical results demonstrate that Transformer-based models achieve state-of-the-art accuracy, robustness, and flexibility across benchmarks like ImageNet, COCO, and ADE20K, although with higher data and computational demands. Challenges include handling high-resolution images efficiently, reducing the quadratic complexity of self-attention, and integrating inductive biases from CNNs to maintain locality alongside global context. Recent developments, such as sparse attention and hierarchical Transformers, aim to improve efficiency and scalability. Looking forward, research aims to develop lightweight models, improve training methodologies, enhance interpretability, and support multi-modal vision-language tasks, with emphasis on unsupervised learning and real-time processing. Overall, Transformers are rapidly establishing themselves as a dominant paradigm in computer vision, offering powerful alternatives to CNNs by effectively modeling complex dependencies and diverse data modalities, despite ongoing challenges in efficiency and data requirements."
23,"B. S. Kanipriya, K. Murugesan, and K. Marudhamuthu, ""An efficient multimodal sentiment analysis in social media using hybrid optimal multi-scale residual attention network,"" Artificial Intelligence Review, vol. 57, no. 2, Art. no. 34, Feb. 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10462-024-10534-3","The paper proposes a hybrid sentiment analysis framework combining an Arithmetic Optimization Algorithm-Hunger Games Search (AOA-HGS) technique with an Ensemble Multi-scale Residual Attention Network (EMRA-Net) to effectively analyze multimodal social media data including text, audio, social links, and video. The hybrid AOA-HGS optimizes feature learning by capturing complementary and comprehensive representations. EMRA-Net comprises two parts: Ensemble Attention CNN (EA-CNN) and Three-scale Residual Attention CNN (TRA-CNN), where TRA-CNN integrates Wavelet transform to reduce loss of spatial domain image texture features. EA-CNN performs feature-level fusion of visual, audio, and textual modalities. Evaluation on Multimodal Emotion Lines Dataset (MELD) and EmoryNLP datasets demonstrates superior performance in recall, accuracy, F score, and precision compared to existing methods like HALCB, HDF, and MMLatch, with faster computation even with varying training set sizes."
24,"A. Holzinger, J. Plass, H. Holzinger, G. Taylor, and C. Kieseberg, ""Towards multi-modal causability with graph neural networks enabling information fusion for explainable AI,"" Information Fusion, vol. 68, pp. 147–162, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1566253521000142","The paper by A. Holzinger et al. (2021) presents a novel framework leveraging Graph Neural Networks (GNNs) for multi-modal information fusion to enhance explainable AI (XAI) through the concept of causability, which quantifies human understanding of causal explanations distinct from pure causality. It addresses the challenge of generating technically sound yet humanly comprehensible explanations in complex domains like medicine by integrating heterogeneous data modalities  images, clinical text, and genomics  into a unified graph representation. The method involves constructing heterogeneous graphs guided by domain-specific knowledge bases, where nodes represent multi-modal features and edges encode cross-modality relationships. The GNN message passing is formalized as $h_v^{(k)} = \sigma\left(\sum_{u \in \mathcal{N}(v)} W^{(k)} h_u^{(k-1)} + b^{(k)}\right)$, with $h_v$ as node features, enabling learning joint embeddings that retain interpretability by linking to knowledge graph nodes. Results demonstrate improved explainability and predictive accuracy in medical datasets, with expert validation showing enhanced clinician understanding via explanations traceable across modalities. The discussion emphasizes causability's role in shifting XAI towards human-centeredness, noting challenges in knowledge base construction, noise management, and scalability. Future directions include temporal dynamics integration, continual learning for knowledge graphs, and broader domain application. Ultimately, this work establishes GNN-driven multi-modal fusion as a pathway to transparent, trustworthy AI systems that align with human causal reasoning."
25,"Y. Shaik, P. P. Roy, T. Hussain, S. Bandhyopadhyay, and S. Bhattacharyya, ""Enhancing trustworthiness in multimodal gas classification when explainable artificial intelligence meets data governance,"" Information Fusion, vol. 83, Jun. 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1566253525005135","The paper develops a novel framework that enhances trustworthiness in multimodal gas classification by integrating explainable artificial intelligence (XAI) techniques with a robust data governance protocol. Targeting critical applications such as environmental monitoring and industrial safety, the approach addresses challenges of model transparency, interpretability, and accountability. The methodology involves fusing multimodal sensor data at the feature level and applying interpretable models such as explainable gradient boosting and attention-based deep neural networks. XAI tools like SHAP and LIME provide local and global explanations, improving user understanding and trust. Experimentally, the proposed model outperforms baselines with accuracy gains of 36% and higher explanation fidelity, as summarized in the table: 
$$
\begin{tabular}{|l|c|c|}
\hline
Model & Accuracy (\%) & Explanation Fidelity \\
\hline
Baseline DNN & 87.2 & 0.65 \\
Proposed XAI-Governed Model & 92.1 & 0.87 \\
\hline
\end{tabular}
$$
The framework ensures data quality management, provenance tracking, and compliance auditing. Discussion highlights the balance between model complexity and explainability, benefits in human-machine collaboration, and challenges including data heterogeneity and scalability. The study concludes that combining XAI with stringent data governance significantly advances transparency and accountability in sensor-based classification, laying groundwork for adaptive, governance-aware explainable systems with future directions in real-time explainability and federated learning."
26,"S. A. Haider et al., ""Use of Multimodal Artificial Intelligence in Surgical Instrument Recognition: Current State and Challenges,"" Bioengineering, vol. 12, no. 1, 2025. [Online]. Available: https://www.mdpi.com/2306-5354/12/1/72","This study evaluates the efficacy of publicly available Large Language Models (LLMs)ChatGPT-4, ChatGPT-4o (multimodal visual-optimized), Google's Geminiand a specialized commercial mobile app, Surgical-Instrument Directory (SID 2.0), in recognizing surgical instruments from images. Using a dataset of 92 high-resolution images of 25 surgical instruments across four categories (retractors, forceps, scissors, trocars), the models were assessed on category-level and subtype-level (e.g., ""Mayo scissors"") identification. ChatGPT-4o achieved the highest category-level accuracy at 89.1%, surpassing SID 2.0 (77.2%), ChatGPT-4 (76.1%), and Gemini (44.6%). However, subtype identification accuracy dropped markedly, with SID 2.0 leading at only 39.1%, and ChatGPT-4o following at 33.7%. Subgroup analysis highlighted strengths such as ChatGPT-4 variants' perfect recognition of trocars and Gemini's consistent identification of scissors. The contrast between high category-level and low subtype-level performance underscores challenges in fine-grained instrument detection. The study utilized standard prompts for LLMs and considered weighted precision, recall, and F1 scores for performance evaluation. Limitations include the small dataset and controlled imaging conditions, with suggestions to improve accuracy by expanding data, incorporating specialized training, and leveraging hybrid AI approaches. The findings demonstrate the promise of AI-driven surgical instrument recognition to enhance workflow efficiency and patient safety in operating rooms, especially in resource-limited contexts, but also reveal the need for further refinement to reliably distinguish detailed subtypes. Future directions involve enhancing multimodal capabilities and employing retrieval-augmented generation frameworks to address rare instrument identification."
27,"A. Ahmed et al., ""From Detection to Action: A Multimodal AI Framework for Real-Time Highway Safety Management,"" Drones, vol. 8, no. 12, 2024, pp. 741. [Online]. Available: https://www.mdpi.com/2504-446X/8/12/741","The paper presents a novel multimodal AI framework for real-time highway safety management by integrating heterogeneous data from drone-captured visual footage, vehicle telemetry, and environmental sensors. Using convolutional neural networks for object detection and advanced sensor fusion, the system detects hazards like congestion, accidents, and unsafe driving with a mean average precision exceeding 90%, enhanced by 15% accuracy under adverse conditions. The framework's decision-making engine combines rule-based and learning approaches to trigger safety interventions within a latency of under 200 ms, potentially reducing accident risk by up to 30%. Key innovations include leveraging UAVs for dynamic monitoring, sensor fusion for robust detection, and a communication layer for real-time alerts. Challenges involve managing data bandwidth, communication reliability, privacy issues, and adapting to complex scenarios. Future work aims to integrate predictive analytics, enhance multi-agent coordination, and validate scalability across diverse settings. Overall, the study demonstrates a comprehensive AI-driven approach to proactive highway safety, advocating for deployment in smart city infrastructures."
28,"S. Munikoti, I. Stewart, S. Horawalavithana, H. Kvinge, T. Emerson, S. E. Thompson, and K. Pazdernik, ""Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities,"" arXiv preprint arXiv:2406.05496, Jun. 2024. [Online]. Available: https://arxiv.org/abs/2406.05496","Generalist multimodal AI models offer versatile capabilities across modalities such as vision, language, and audio by leveraging unified architectures and training methods including masked token prediction, contrastive learning, and multimodal language modeling. These models are trained on large, well-aligned datasets utilizing strategies like weak supervision and instruction tuning, with scalability enabled through block-sparse architectures and distributed optimization. Evaluations show competitive or superior performance on diverse tasks (e.g., image captioning with BLEU = 35.2, visual question answering at 78.5% accuracy, and audio-visual scene understanding with F1 = 69.7%), alongside strong zero-shot and few-shot generalization. Key challenges include data alignment, computational efficiency, evaluation standardization, interpretability, and ethical concerns. Promising future directions involve improved foundation models with reasoning capabilities, novel prompting, self-supervised learning, efficient architectures, and the development of comprehensive benchmarks. Overall, this review highlights the significant progress and open challenges toward building adaptable multimodal AI agents combining vision, language, audio, and more."
29,"O. Rubin and J. Berant, ""Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 645–666, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.47.pdf","The paper introduces the Retrieval-Pretrained Transformer (RPT), a transformer-based language model designed to handle long-range language modeling by incorporating a self-retrieval mechanism that queries an external memory of past hidden states during training and generation. This allows the model to overcome fixed-length context window limitations typical in transformers. RPT computes a retrieval query $q_t = W_q h_t$ from the decoder hidden state $h_t$ at time step $t$ and retrieves memory vectors using scaled dot-product attention weights $\alpha_{t,i} = \frac{\exp(q_t^\top k_i / \sqrt{d})}{\sum_j \exp(q_t^\top k_j / \sqrt{d})}$, where $k_i = W_k m_i$ and $v_i = W_v m_i$ are projected keys and values of stored memory vectors $m_i$. The retrieved vector $r_t = \sum_i \alpha_{t,i} v_i$ is combined with $h_t$ to predict the next token. Pretrained on long documents (e.g., from arXiv and PubMed with documents averaging 8k tokens), RPT achieves improved perplexity (13.7) versus baselines like Transformer-XL (15.3) and fixed-window transformers (17.8). Beyond perplexity gains, RPT excels at zero-shot retrieval-augmented generation tasks requiring integration of distant context. The self-retrieval supports adaptive focus on relevant history, enabling better long-range coherence and factual consistency while efficiently scaling compared to naive full attention. Challenges include scalable memory indexing and managing retrieval noise. Future work aims to extend retrieval to multi-document contexts and integrate external knowledge sources, setting a foundation for scalable, retrieval-augmented language models tailored for document-level understanding and generation."
30,"V. Hofmann, G. Glavaš, N. Ljubešić, J. B. Pierrehumbert, and H. Schütze, ""Geographic Adaptation of Pretrained Language Models,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 412–427, 2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.39.pdf","Pretrained language models (PLMs) like RoBERTa achieve improved performance on texts from underrepresented geographic regions when adapted using region-specific corpora and finetuning strategies. This geographic adaptation addresses biases inherent from training data skewed toward North America, Europe, and parts of Asia, effectively enhancing model performance on regional language variants without reducing general language understanding. The approach involves curated geographically annotated datasets, masked language model finetuning, and evaluation on regional tasks such as sentiment analysis and named entity recognition. Results demonstrate F1 score improvements of approximately 4-5 points in African, Indian, and Caribbean English (e.g., African English adaption yielded an F1 increase from 72.4 to 77.1). Additional benefits include reduced perplexity on regional text and lower error rates in recognizing region-specific lexical and syntactic features. The study highlights challenges like corpus acquisition for low-resource areas and suggests future directions combining geographic, societal, and cultural factors to enhance model fairness and robustness. The key results table is summarized as: $\begin{tabular}{lccc}\hline \text{Region} & \text{Base Model F1} & \text{Adapted Model F1} & \text{Improvement} \\ \hline \text{African English} & 72.4 & 77.1 & +4.7 \\ \text{Indian English} & 70.9 & 75.8 & +4.9 \\ \text{Caribbean English} & 68.3 & 72.6 & +4.3 \\ \hline \end{tabular}$. This work pioneers geographically aware PLM adaptation, advocating for more inclusive, regionally sensitive NLP systems."
31,"X. Chen, L. Li, X. Lin, and J. Zhang, ""Can Pretrained English Language Models Benefit Non-English NLP Systems in Low-Resource Scenarios?"", IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 1061–1074, 2023. [Online]. Available: https://doi.org/10.1109/TASLP.2023.3267618","This paper investigates the potential of large pretrained English language models to improve non-English NLP systems in low-resource settings. By systematically fine-tuning and applying cross-lingual adaptation, the study demonstrates that English pretrained models outperform training from scratch or using smaller native-language datasets. The results underscore the advantage of leveraging English pretrained models to enhance NLP tasks in languages with limited training data, pointing to a promising approach for advancing low-resource language processing."
32,"J. Wu, H. Fan, X. Qian, and H. Meng, ""Non-Autoregressive ASR Modeling Using Pre-Trained Language Models for Chinese Speech Recognition,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, 2022. [Online]. Available: https://doi.org/10.1109/TASLP.2022.3166400","Non-autoregressive (NAR) speech recognition models offer faster inference than autoregressive (AR) ones but usually at the cost of accuracy. This paper presents a novel NAR ASR framework for Chinese that integrates pre-trained language models (PLMs), specifically a Chinese BERT variant, to provide richer linguistic context and improve transcription quality without slowing decoding. The model combines an acoustic encoder extracting frame-level features with a PLM-enhanced non-autoregressive decoder that predicts tokens in parallel. Training aligns acoustic and linguistic embeddings, modeling the probability as $P(Y|X) \approx \prod_{i=1}^N P(y_i | X, \text{PLM}(Y))$ under NAR assumptions. Experiments on benchmarks like AISHELL-1 demonstrate significant accuracy gains over standard NAR models (CER 6.9% vs. 8.2%) while maintaining rapid decoding (RTF 0.35 vs. 0.3), and nearing AR model performance (CER 6.5%, RTF 1.0). Challenges addressed include Chinese-specific tokenization and tone handling, though some issues with homophones and rare words persist. The study concludes that combining acoustic and PLM-based linguistic pretraining enables efficient, high-quality Chinese NAR ASR with potential for further enhancement via larger or speech-text aligned PLMs and prosody integration."
33,"M. Sun, Z. Yang, and J. Yin, ""Hyperbolic Pre-Trained Language Model,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, 2024. [Online]. Available: https://doi.org/10.1109/TASLP.2024.3407575","In recent years, improvements in pre-trained language models (PLMs) have primarily come from scaling parameters and data, incurring high computational costs. This paper introduces an alternative approach by adopting a geometric feature spacespecifically hyperbolic spacethat more effectively encodes intrinsic structured features of text, such as syntactic and semantic relationships. Unlike conventional PLMs that operate in Euclidean spaces, hyperbolic PLMs better capture these structured features, enhancing text understanding. Experimental results demonstrate that hyperbolic PLMs outperform their Euclidean counterparts across various tasks while maintaining the same parameter and data scales. Thus, the work highlights changing the geometry of model representations as a promising direction for improving PLMs without increased resource demands."
34,"T. A. Chang and B. K. Bergen, ""Language Model Behavior: A Comprehensive Survey,"" Computational Linguistics, vol. 50, no. 1, pp. 293–350, 2024. Available: https://direct.mit.edu/coli/article/50/1/293/118131/Language-Model-Behavior-A-Comprehensive-Survey","This comprehensive survey synthesizes over 250 recent studies analyzing English language models (LMs) before task-specific fine-tuning, covering syntax, semantics, pragmatics, world knowledge, reasoning, memorization, bias, privacy, toxicity, misinformation, and personality. LMs show fundamental strengths such as mastery of core syntactic rules (e.g., subject-verb agreement), semantic compositionality, analogies, and basic logical and numerical reasoning, but struggle with complex syntax, negation, implicit pragmatics, and multi-step reasoning. Commonsense knowledge improves with scale but remains limited regarding physical world sensitivity. Memorization grows with model size, posing privacy risks. Ethical concerns include biases reflecting training data, toxic or hateful output, misinformation generation, and inconsistent personality-like behaviors. The survey highlights the critical influence of model scale, architecture, pretraining data, and prompting on LM performance and advocates for multi-level analyses integrating behavioral and mechanistic evaluations. Key challenges remain brittleness to linguistic variability and adversarial inputs, semantic and pragmatic deficiencies, reasoning complexities, and ethical risks like bias and toxicity. Future work should focus on enhancing robustness, deeper semantic and pragmatic understanding, privacy-preserving techniques, and ethical safeguards, leveraging cross-disciplinary methods from linguistics, cognitive science, and machine learning. The paper establishes a roadmap emphasizing robust evaluation, mechanistic interpretation, and responsible deployment as LMs continue to advance NLP and AI applications."
35,"Q. Wang, L. Zhao, H. Wu, and others, ""Measuring and Improving Consistency in Pretrained Language Models,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 901–916, 2023. Available: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00410/107384/Measuring-and-Improving-Consistency-in-Pretrained","This paper investigates the consistency of pretrained language models (PLMs) across three key dimensions: factual consistency, paraphrase consistency, and negation consistency. The authors develop three specialized benchmark datasets to systematically evaluate models such as GPT-2, BERT, RoBERTa, and GPT-3, revealing prevalent inconsistencies with factual consistency often falling below 80%. Consistency is formalized as stable model output across equivalent inputs, and is quantitatively measured by agreement percentages. To address this, they propose a post-hoc probability calibration method employing temperature scaling to adjust output confidences without modifying model weights. This technique yields up to 20% improvements in consistency metrics while maintaining accuracy. The study highlights that many inconsistencies stem from miscalibrated confidence rather than fundamental model limitations. The work provides a framework and tools for future research to enhance PLM reliability by prioritizing consistency in evaluation and deployment, encouraging a shift beyond standard accuracy metrics."
36,"M. Jabreel, M. Arif, and A. Almasri, ""A Deep Learning-Based Approach for Multi-Label Emotion Classification Using Pre-Trained Language Models,"" Applied Sciences, vol. 9, no. 2, pp. 1123, 2019. [Online]. Available: https://www.mdpi.com/2076-3417/9/2/1123","This paper proposes a deep learning framework for multi-label emotion classification leveraging pre-trained language models such as BERT. The approach tokenizes input text, extracts contextual embeddings via the pre-trained model, and applies a fully connected layer with a sigmoid activation to output probabilities for multiple simultaneous emotion labels, trained using a binary cross-entropy loss function. Experimental results demonstrate a 5-7% macro F1-score improvement over baselines like SVM, traditional LSTM, and non-fine-tuned BERT, particularly excelling in detecting emotions like joy, sadness, and anger. The method effectively handles overlapping emotions by fine-tuning contextual embeddings to adapt to emotional nuances beyond generic language tasks. Challenges remain, including data imbalance, distinguishing similar emotions, and model interpretability, with future work aimed at transfer learning, explainability, multimodal integration, and efficiency optimization. Overall, this framework advances state-of-the-art emotion detection from text with applications in sentiment analysis and human-computer interaction."
37,"B. Kasthuriarachchy and P. Senanayake, ""From General Language Understanding to Noisy Text Classification using BERT,"" Applied Sciences, vol. 11, no. 17, pp. 7814, 2021. [Online]. Available: https://www.mdpi.com/2076-3417/11/17/7814","Obtaining meaning-rich representations of social media inputs such as Tweets is challenging due to their deviation from mainstream English usage. This research proposes a generic methodology to derive diverse sentence vectors by combining various linguistic features extracted from multi-layer latent representations of pre-trained language models like BERT. Five new probing tasks for Tweets are introduced to benchmark noisy text comprehension. Experiments show that initial and middle BERT layers better capture linguistic characteristics of noisy texts than latter layers. Moreover, sentence vector length has less impact on linguistic information capture, and the proposed sentence vectors outperform existing state-of-the-art vectors for noisy text classification."
38,"F. Zilong, X. Wei, Z. Baozun, and J. Rui, ""Using Multiple Monolingual Models for Efficiently Semantic Textual Similarity,"" Applied Sciences, vol. 13, no. 9, pp. 5771, 2023. [Online]. Available: https://www.mdpi.com/2076-3417/13/9/5771","This paper introduces a novel approach to identify the most semantically similar conversational sentences between Korean and English. The approach leverages multiple monolingual models to embed sentences, enabling efficient and accurate cross-lingual semantic textual similarity comparisons. This method enhances both accuracy and efficiency in cross-lingual semantic matching tasks by efficiently combining embeddings from separate monolingual models."
39,"J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, ""Pretrained Language Models for Text Generation: A Survey,"" in Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) Survey Track, 2021. Available: https://arxiv.org/abs/2105.10311","This paper provides a comprehensive overview of pretrained language models (PLMs) applied to text generation, an important task in natural language processing. It begins by defining the task and outlining mainstream PLM architectures such as the Transformer, with training objectives including autoregressive modeling (e.g., GPT), masked language modeling (e.g., BERT), and sequence-to-sequence learning (e.g., T5). The paper discusses adaptation techniques like fine-tuning, prompt learning, and few-shot learning to customize PLMs for specific generation tasks. Text generation tasks are categorized into open-ended generation (e.g., stories, dialogues), conditional generation (e.g., translation, summarization), and controllable generation (e.g., attribute-conditioned output), reviewing the performance and adaptation methods for each. Key challenges highlighted include managing output diversity, coherence, adherence to constraints, ethical concerns such as bias, and balancing model size with computational efficiency. Future directions advocate for advanced controllable generation methods, integration with symbolic and structured knowledge, improved interpretability and fairness, and enhanced few-shot/zero-shot learning to reduce data reliance. Overall, the survey synthesizes recent progress and challenges in PLMs for text generation, serving as a roadmap for researchers in this evolving domain."
40,"X. Zhao, S. Ouyang, Z. Yu, M. Wu, and L. Li, ""Pre-trained Language Models Can be Fully Zero-Shot Learners,"" in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023. Available: https://arxiv.org/abs/2212.06950","Pre-trained language models (PLMs) show impressive few-shot capabilities but typically rely on prompt engineering or fine-tuning for good performance. This paper proposes NPPrompt, a novel zero-shot learning method that automatically constructs natural language prompts by mining task-related information from large-scale web corpora without any manual prompt design or task-specific training. NPPrompt retrieves and ranks relevant knowledge snippets using BM25, dense vector search, and a fine-tuned BERT encoder, then synthesizes these into coherent prompt templates to frame NLP tasks comprehensibly for PLMs. Evaluated on tasks including classification (AGNews, DBpedia), natural language inference (MNLI, RTE), and question answering (BoolQ), NPPrompt outperforms baseline zero-shot methods by significant marginsfor instance, achieving 83.2% accuracy on AGNews versus 74.5% for direct zero-shot promptingand rivals few-shot approaches despite no labeled data usage. Ablations confirm both retrieval and ranking stages are crucial, while analysis shows inclusion of external knowledge aids task understanding and disambiguation. Although dependent on quality and availability of external data and introducing potential computational overhead, NPPrompt's fully automated pipeline removes brittle manual prompt engineering and enhances PLM generalization across domains. The method highlights the value of leveraging contextual external knowledge in zero-shot settings and opens avenues for future improvements in neural prompt synthesis, multilingual expansion, and hybrid zero- and few-shot learning paradigms."
41,"T. Korbak, K. Shi, A. Chen, R. Bhalerao, C. L. Buckley, J. Phang, S. R. Bowman, and E. Perez, ""Pretraining Language Models with Human Preferences,"" in Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. Available: https://arxiv.org/abs/2302.08582","This paper proposes augmenting language model pretraining with human preference data to improve alignment with human intentions and reduce undesirable outputs, such as toxicity and hallucinations, without compromising language modeling performance. Unlike typical reinforcement learning from human feedback (RLHF) applied post-pretraining, the authors integrate pairwise human preference judgments directly into the pretraining objectives through a multi-objective framework combining standard next-token prediction with preference ranking loss. They optimize a BradleyTerry model likelihood given by $$\mathcal{L}_{pref} = - \sum_{(x_i, x_j)} y_{ij} \log \sigma(s_i - s_j) + (1 - y_{ij}) \log \sigma(s_j - s_i)$$ where $s_i$ and $s_j$ are preference scores and $\sigma$ the sigmoid function. Experiments demonstrate that joint pretraining with preference objectives yields models that outperform those fine-tuned with preference data post-pretraining, achieving lower toxicity rates and higher preference accuracy while maintaining competitive perplexity. Key results are summarized as:

\[
\begin{tabular}{lccc}
\hline
Method & Perplexity & Toxicity Rate & Preference Accuracy \\
\hline
LM only & 12.3 & 0.15 & 0.50 \\
Fine-tuned pref & 12.5 & 0.10 & 0.68 \\
Joint pretraining & 12.7 & 0.07 & 0.75 \\
\hline
\end{tabular}
\]

The work highlights the benefits of early internalization of human preferences, challenges such as data scarcity and balancing objectives, and suggests future directions including scalable preference data acquisition and combining preference pretraining with RLHF. Overall, integrating human preferences into language model pretraining offers a promising path toward more aligned, trustworthy AI systems."
