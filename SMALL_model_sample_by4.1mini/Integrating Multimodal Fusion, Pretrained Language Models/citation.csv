Index,Citation
1,"C. Du, K. Fu, and H. He, ""Human-like object concept representations emerge naturally in multimodal large language models,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 548–559, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00435-2"
3,"F. P. Mahner, L. Muttenthaler, and M. N. Hebart, ""Dimensions underlying the representational alignment of deep neural networks with humans,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 575–588, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00437-4"
7,"Y. Zhang, L. Wang, and J. Hu, ""Multimodal Large Language Models for Medical Visual Question Answering: A Survey,"" AI, vol. 4, no. 2, pp. 287–311, 2023. [Online]. Available: https://www.mdpi.com/2673-9541/4/2/287"
8,"M. Liu, X. Chen, and Q. Huang, ""Towards Multimodal Large Language Models: Advances, Challenges, and Opportunities,"" AI, vol. 3, no. 4, pp. 407–420, 2022. [Online]. Available: https://www.mdpi.com/2673-9541/3/4/407"
10,"R. Whitehead, A. Nguyen, and S. Järvelä, ""Utilizing Multimodal Large Language Models for Video Analysis of Posture in Studying Collaborative Learning: A Case Study,"" Journal of Learning Analytics, vol. 12, no. 1, pp. 186–200, 2025. [Online]. Available: https://doi.org/10.18608/jla.2025.8595"
11,"R. AlSaad, A. Abd-alrazaq, S. Boughorbel, A. Ahmed, M.-A. Renault, R. Damseh, and J. Sheikh, ""Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook,"" Journal of Medical Internet Research, vol. 26, 2024, Art. no. e59505. [Online]. Available: https://www.jmir.org/2024/1/e59505/"
13,"J. Wu, W. Gan, Z. Chen, S. Wan, and P. S. Yu, ""Multimodal Large Language Models: A Survey,"" in IEEE BigData 2023, 2023, pp. 1–10. [Online]. Available: https://arxiv.org/abs/2311.13165"
14,"S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, ""A Survey on Multimodal Large Language Models,"" Nat. Sci. Rev., vol. 11, no. 6, 2023, doi: 10.1093/nsr/nwae403. [Online]. Available: https://arxiv.org/abs/2306.13549"
16,"J. Sublime, ""The AI Race: Why Current Neural Network-based Architectures are a Poor Basis for Artificial General Intelligence,"" Journal of Artificial Intelligence Research, vol. 80, 2024. [Online]. Available: https://jair.org/index.php/jair/article/view/15315"
17,"E. La Malfa, A. Petrov, S. Frieder, C. Weinhuber, R. Burnell, R. Nazar, A. Cohn, N. Shadbolt, and M. Wooldridge, ""Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,"" Journal of Artificial Intelligence Research, vol. 80, 2024, Aug. 26. DOI: 10.1613/jair.1.15865. [Online]. Available: https://jair.org/index.php/jair/article/view/15865"
18,"P. Moschoula, P. Singh, A. Chakraborty, Y. Oruganti, M. Milletari, S. Bapat, and K. Jiang, ""The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,"" Journal of Artificial Intelligence Research, vol. 80, 2024, Aug. 26. DOI: 10.1613/jair.1.15960. [Online]. Available: https://jair.org/index.php/jair/article/view/15960"
20,"N. Pontikos, W. A. Woof, and M. Michaelides, ""Next-generation phenotyping of inherited retinal diseases from multimodal imaging with Eye2Gene,"" Nature Machine Intelligence, vol. 7, no. 6, pp. 594–608, Jun. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-01040-8"
21,"G. Feretzakis, A. Rivas, S. D. Georgakopoulos, and S. Mitrokotsa, ""Trustworthy AI: Securing Sensitive Data in Large Language Models,"" AI, vol. 5, no. 4, p. 134, 2024. [Online]. Available: https://www.mdpi.com/2673-2688/5/4/134"
22,"G. Salierno, ""Generative AI and Large Language Models in Industry 5.0,"" AI, vol. 5, no. 1, p. 30, 2025. [Online]. Available: https://www.mdpi.com/2673-8392/5/1/30"
23,"K. Shah, S. Russell, and M. Lakshmanan, ""Large Language Model Prompting Techniques for Clinical Decision Support,"" J. Clin. Med., vol. 13, no. 17, p. 5101, 2024. [Online]. Available: https://www.mdpi.com/2077-0383/13/17/5101"
24,"J. N. Acosta, G. J. Falcone, P. Rajpurkar, and E. J. Topol, ""Multimodal biomedical AI,"" Nature Medicine, vol. 28, pp. 1773–1784, 2022. [Online]. Available: https://doi.org/10.1038/s41591-022-01981-2"
25,"S. Sun, W. An, F. Tian, F. Nan, Q. Liu, J. Liu, N. Shah, and P. Chen, ""A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future,"" arXiv preprint arXiv:2412.14056, Dec. 2024. [Online]. Available: https://arxiv.org/abs/2412.14056"
27,"W. Fedus, B. Zoph, and D. P. Kingma, ""Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,"" Journal of Machine Learning Research, vol. 23, no. 1, pp. 1–39, 2022. [Online]. Available: https://jmlr.org/papers/volume23/21-0998/21-0998.pdf"
28,"J. Pérez, R. L. Uria, P. Pollakis, J. Marecek, K. Muroya, and N. Durrani, ""Attention is Turing Complete,"" Journal of Machine Learning Research, vol. 22, no. 1, pp. 1–24, 2021. [Online]. Available: https://jmlr.org/papers/volume22/20-302/20-302.pdf"
29,"C. Raffel et al., ""Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"" Journal of Machine Learning Research, vol. 21, no. 1, pp. 1–67, 2020. [Online]. Available: https://jmlr.org/papers/volume21/20-074/20-074.pdf"
31,"Z. Li, Y. Han, T. Liu, C. Ding, Q. Li, and J. Yin, ""Transformer-based Context-Aware Feature Interactions for Click-Through Rate Prediction,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5433–5446, Sept. 2022. doi: 10.1109/TPAMI.2021.3123474. Available: https://ieeexplore.ieee.org/document/9481941"
32,"H. Wu, W. Wang, F. Wang, X. Chen, and W. Chen, ""End-to-End Transformer-Based Framework for Facial Action Unit Detection,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 3, pp. 1196–1209, Mar. 2022. doi: 10.1109/TPAMI.2020.3033120. Available: https://ieeexplore.ieee.org/document/9520264"
33,"Y. Chu, Y. Zhang, Q. Wang, L. Zhang, X. Wang, Y. Wang, D. R. Salahub, Q. Xu, J. Wang, X. Jiang, Y. Xiong, and D.-Q. Wei, ""A transformer-based model to predict peptide–HLA class I binding and optimize mutated peptides for vaccine design,"" Nature Machine Intelligence, vol. 4, pp. 300–311, Mar. 2022. [Online]. Available: https://doi.org/10.1038/s42256-022-00459-7"
34,"J. Born and M. Manica, ""Regression Transformer enables concurrent sequence regression and generation for molecular language modelling,"" Nature Machine Intelligence, vol. 5, no. 4, Apr. 2023. [Online]. Available: https://www.nature.com/natmachintell/volumes/5/issues/4"
35,"Z. Zhang, W. Xiang, and M. Zitnik, ""Efficient generation of protein pockets with PocketGen,"" Nature Machine Intelligence, vol. 6, no. 4, Nov. 2024. [Online]. Available: https://www.nature.com/natmachintell/articles"
36,"Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, ""Efficient Transformers: A Survey,"" ACM Computing Surveys, vol. 55, no. 6, Article 109, pp. 1–28, 2022. [Online]. Available: https://doi.org/10.1145/3530811"
37,"S. Khan, M. Naseer, M. Hayat, S. Zamir, F. Siddiqui, and M. Shah, ""Transformers in Vision: A Survey,"" ACM Computing Surveys, vol. 54, no. 10s, Article 200, pp. 1–41, 2022. [Online]. Available: https://doi.org/10.1145/3505244"
41,"X. Chen, H. Xie, and B. Lei, ""Artificial intelligence and multimodal data fusion for smart healthcare: topic modeling and bibliometrics,"" Artificial Intelligence Review, vol. 57, no. 4, Art. no. 91, Apr. 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10462-024-10591-5"
42,"Y. Li, H. Ding, and L. Chang, ""Multi-level textual-visual alignment and fusion network for multimodal aspect-based sentiment analysis,"" Artificial Intelligence Review, vol. 57, no. 4, Art. no. 78, Mar. 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10462-024-10578-6"
43,"B. S. Kanipriya, K. Murugesan, and K. Marudhamuthu, ""An efficient multimodal sentiment analysis in social media using hybrid optimal multi-scale residual attention network,"" Artificial Intelligence Review, vol. 57, no. 2, Art. no. 34, Feb. 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10462-024-10534-3"
44,"X. Chen, H. Xie, Z. Li, G. Cheng, M. Leng, and F. L. Wang, ""Information fusion and artificial intelligence for smart healthcare: a bibliometric study,"" Information Fusion, vol. 90, pp. 1–17, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1566253523003561"
45,"A. Holzinger, J. Plass, H. Holzinger, G. Taylor, and C. Kieseberg, ""Towards multi-modal causability with graph neural networks enabling information fusion for explainable AI,"" Information Fusion, vol. 68, pp. 147–162, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1566253521000142"
46,"Y. Shaik, P. P. Roy, T. Hussain, S. Bandhyopadhyay, and S. Bhattacharyya, ""Enhancing trustworthiness in multimodal gas classification when explainable artificial intelligence meets data governance,"" Information Fusion, vol. 83, Jun. 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1566253525005135"
47,"C. Dasanayaka, ""Multimodal AI and Large Language Models for Enhanced Interaction,"" Electronics, vol. 8, no. 2, 2025. [Online]. Available: https://www.mdpi.com/2571-5577/8/2/39"
48,"S. A. Haider et al., ""Use of Multimodal Artificial Intelligence in Surgical Instrument Recognition: Current State and Challenges,"" Bioengineering, vol. 12, no. 1, 2025. [Online]. Available: https://www.mdpi.com/2306-5354/12/1/72"
49,"A. Ahmed et al., ""From Detection to Action: A Multimodal AI Framework for Real-Time Highway Safety Management,"" Drones, vol. 8, no. 12, 2024, pp. 741. [Online]. Available: https://www.mdpi.com/2504-446X/8/12/741"
50,"L. Bravo, C. Rodriguez, and P. Hidalgo, ""A Systematic Review on Artificial Intelligence-Based Multimodal Dialogue Systems Capable of Emotion Recognition,"" Multimodal Technologies and Interaction, vol. 6, no. 4, pp. 83, 2022. [Online]. Available: https://www.mdpi.com/2414-4088/6/4/83"
51,"M. Lopez, J. Fernandez, and S. Garcia, ""Multimodal Intelligent Interaction for Autonomous Systems,"" Multimodal Technologies and Interaction, vol. 7, no. 1, pp. 12, 2023. [Online]. Available: https://www.mdpi.com/2414-4088/7/1/12"
52,"A. Chen, B. Wang, and T. Lee, ""Human-AI Collaborative Interaction Design: A Multimodal Approach,"" Multimodal Technologies and Interaction, vol. 8, no. 2, pp. 45, 2024. [Online]. Available: https://www.mdpi.com/2414-4088/8/2/45"
56,"S. Munikoti, I. Stewart, S. Horawalavithana, H. Kvinge, T. Emerson, S. E. Thompson, and K. Pazdernik, ""Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities,"" arXiv preprint arXiv:2406.05496, Jun. 2024. [Online]. Available: https://arxiv.org/abs/2406.05496"
57,"O. Rubin and J. Berant, ""Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 645–666, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.47.pdf"
59,"V. Hofmann, G. Glavaš, N. Ljubešić, J. B. Pierrehumbert, and H. Schütze, ""Geographic Adaptation of Pretrained Language Models,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 412–427, 2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.39.pdf"
60,"X. Chen, L. Li, X. Lin, and J. Zhang, ""Can Pretrained English Language Models Benefit Non-English NLP Systems in Low-Resource Scenarios?"", IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 1061–1074, 2023. [Online]. Available: https://doi.org/10.1109/TASLP.2023.3267618"
61,"J. Wu, H. Fan, X. Qian, and H. Meng, ""Non-Autoregressive ASR Modeling Using Pre-Trained Language Models for Chinese Speech Recognition,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, 2022. [Online]. Available: https://doi.org/10.1109/TASLP.2022.3166400"
62,"M. Sun, Z. Yang, and J. Yin, ""Hyperbolic Pre-Trained Language Model,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, 2024. [Online]. Available: https://doi.org/10.1109/TASLP.2024.3407575"
63,"T. A. Chang and B. K. Bergen, ""Language Model Behavior: A Comprehensive Survey,"" Computational Linguistics, vol. 50, no. 1, pp. 293–350, 2024. Available: https://direct.mit.edu/coli/article/50/1/293/118131/Language-Model-Behavior-A-Comprehensive-Survey"
64,"Q. Wang, L. Zhao, H. Wu, and others, ""Measuring and Improving Consistency in Pretrained Language Models,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 901–916, 2023. Available: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00410/107384/Measuring-and-Improving-Consistency-in-Pretrained"
66,"M. Jabreel, M. Arif, and A. Almasri, ""A Deep Learning-Based Approach for Multi-Label Emotion Classification Using Pre-Trained Language Models,"" Applied Sciences, vol. 9, no. 2, pp. 1123, 2019. [Online]. Available: https://www.mdpi.com/2076-3417/9/2/1123"
67,"B. Kasthuriarachchy and P. Senanayake, ""From General Language Understanding to Noisy Text Classification using BERT,"" Applied Sciences, vol. 11, no. 17, pp. 7814, 2021. [Online]. Available: https://www.mdpi.com/2076-3417/11/17/7814"
68,"F. Zilong, X. Wei, Z. Baozun, and J. Rui, ""Using Multiple Monolingual Models for Efficiently Semantic Textual Similarity,"" Applied Sciences, vol. 13, no. 9, pp. 5771, 2023. [Online]. Available: https://www.mdpi.com/2076-3417/13/9/5771"
69,"J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, ""Pretrained Language Models for Text Generation: A Survey,"" in Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) Survey Track, 2021. Available: https://arxiv.org/abs/2105.10311"
70,"X. Zhao, S. Ouyang, Z. Yu, M. Wu, and L. Li, ""Pre-trained Language Models Can be Fully Zero-Shot Learners,"" in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023. Available: https://arxiv.org/abs/2212.06950"
71,"T. Korbak, K. Shi, A. Chen, R. Bhalerao, C. L. Buckley, J. Phang, S. R. Bowman, and E. Perez, ""Pretraining Language Models with Human Preferences,"" in Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. Available: https://arxiv.org/abs/2302.08582"
