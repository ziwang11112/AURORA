Index,Citation,Summary
1,"T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, ""Benchmarking Large Language Models for News Summarization,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 39–57, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.3/","Large language models (LLMs) have demonstrated significant potential for zero-shot summarization, with instruction tuning identified as the key factor driving their success rather than model size. Evaluations across ten LLMs using multiple datasets (CNN/DailyMail, XSum, NYT) and prompting strategies reveal that instruction-tuned models like GPT-4 achieve superior automated metric scores and human-rated coherence and informativeness. However, despite improvements, there remains a quality gap between model-generated and human-written summaries, primarily due to issues like hallucination and factual inaccuracy. The study emphasizes the critical role of high-quality human references in both training and evaluation, as prior assessments using low-quality references underestimated human performance and model capabilities. Human evaluation supplemented with automated metrics such as ROUGE and BERTScore uncovered discrepancies, underscoring the importance of multi-faceted assessment approaches. The findings advocate for continued refinement of instruction tuning, improved automatic evaluation metrics emphasizing factual consistency, and expanded human evaluation frameworks to better capture summary faithfulness and user experience. Overall, while instruction-tuned LLMs mark a significant advance in news summarization, perfect summarization remains an open challenge, motivating future work towards closing the gap to human performance."
2,"C. Yang, G. Huang, M. Yu, Z. Zhang, S. Li, M. Yang, S. Shi, Y. Yang, and L. Liu, ""An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 137–156, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.8/","We introduce an energy-based model (EBM) for word-level autocompletion in computer-aided translation (CAT) tools, framing prediction as minimizing an energy function $E(T_{1:k}, w; S, \theta)$ over candidate words $w$, where $T_{1:k}$ is the translation prefix and $S$ the source sentence. This approach integrates lexical embeddings, source encoding, syntactic, and bilingual alignment features into a unified neural network scoring function. Training employs a margin-based ranking loss $L(\theta) = \sum_i \max(0, \Delta + E(T_{1:k}, w_i^+; S_i, \theta) - E(T_{1:k}, w_i^-; S_i, \theta))$, distinguishing correct from incorrect next words. Experiments on WMT datasets (English-German and English-French) show the model surpasses standard neural and n-gram language model baselines, achieving approximately 78.8% autocompletion accuracy and a 10% translator speedup, with lower perplexity. Ablation suggests syntactic and bilingual features significantly improve performance. Challenges involve balancing model complexity and inference speed, addressed via approximate search, and maintaining training stability amid negative sampling. User studies confirm enhanced translator productivity. The work concludes with plans to extend to phrase-level predictions, user adaptation, and cross-lingual transfer, underscoring that energy-based modeling effectively integrates heterogeneous features to improve autocompletion accuracy and efficiency in CAT tools."
3,"Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, ""How Can We Know What Language Models Know?,"" Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020. [Online]. Available: https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00324","Large pre-trained language models (LMs) have propelled advances in natural language processing by encoding substantial linguistic and factual information. This paper surveys methodologies for assessing what LMs actually ""know,"" focusing on probing techniques such as cloze-style tasks, feature probing, and test suites designed to reveal generalization and robustness. It highlights challenges including distinguishing genuine understanding from mere memorization of training data and the influence of dataset artifacts that can overestimate model performance. Empirical results show that while models contain considerable factual and syntactic knowledge that often scales with model size and data, their errors and reliance on superficial patterns underscore limitations in real comprehension and out-of-distribution generalization. The paper argues for refined evaluation metrics, adversarial testing, and theoretical frameworks to better characterize and interpret model knowledge. Future directions emphasize developing robust benchmarks that challenge models beyond memorization toward reasoning and compositionality, as well as improving interpretability by disentangling memorized facts from acquired knowledge. The synthesis concludes on the necessity of rigorous experimental designs and interdisciplinary insights from linguistics and epistemology to foster transparent, genuinely knowledgeable language models better suited for varied NLP tasks."
4,"A. Talmor, J. Hawthorne, P. Clark, and A. Klein, ""oLMpics-On What Language Model Pre-training Captures,"" Transactions of the Association for Computational Linguistics, vol. 8, pp. 743–758, 2020. [Online]. Available: https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00342","Recent successes of pre-trained language models (LMs) have sparked interest in their reasoning capabilities, but understanding their utility for symbolic reasoning remains limited. This work introduces oLMpics, a suite of eight reasoning tasks involving operations like comparison, conjunction, and composition, designed to probe LM reasoning abilities both in zero-shot scenarios and after fine-tuning. Evaluations across various models, including BERT, RoBERTa, GPT, and T5, reveal that while some LMs succeed on certain tasks (e.g., RoBERTa on age comparisons within typical human ranges), many fail on tasks requiring complex reasoning or abstract understanding, with half of the tasks seeing universal failure. Results suggest that these models often rely on memorization over abstract reasoning, influenced by their pretraining objectives and corpora. The authors highlight the challenges in creating unbiased probing tasks and emphasize the need for new benchmarks encompassing causal, temporal, and multi-hop reasoning. They advocate for future research into novel pretraining objectives, architectures, and integration of structured knowledge to enhance reasoning. Overall, oLMpics provides a valuable framework revealing significant gaps in current LMs' reasoning skills, underscoring the necessity for improved model designs and training methods."
5,"G. Mischler, Y. A. Li, and N. Mesgarani, ""Contextual feature extraction hierarchies converge in large language models and the brain,"" Nature Machine Intelligence, vol. 6, no. 12, pp. 1217–1230, Dec. 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00672-6","Recent advances in large language models (LLMs) reveal that as these Transformer-based models achieve higher benchmark performance, they increasingly resemble human brain language processing, particularly in hierarchical contextual feature extraction. The study analyzed multiple LLMs of similar parameter counts and intracranial EEG data capturing human auditory cortex responses to natural speech, finding that deeper LLM layers encode higher-level semantic and syntactic features while lower layers capture lexical and phonetic attributes. Correspondingly, the posterior auditory cortex encodes lower-level features, with anterior temporal regions handling semantic context, showing a significant alignment in hierarchical feature extraction between models and brain, confirmed by representational similarity analysis. The convergence suggests shared computational principles driven by language comprehension objectives, with LLMs using fewer layers yet mirroring brain encoding pathways. Contextual information substantially improves model-brain similarity, reinforcing the importance of rich context in language understanding. This study underscores the potential of LLMs to serve as computational models for neural language processing and invites future work extending to multimodal models, developmental trajectories, and causal testing to refine neural and artificial language comprehension frameworks."
6,"K. Zhu, R. Fedus, K. Borgeaud, et al., ""A Unified Library for Evaluation of Large Language Models,"" J. Mach. Learn. Res., vol. 25, no. 238, pp. 1-31, 2024. [Online]. Available: https://www.jmlr.org/papers/v25/24-0023.html","This paper presents PromptBench, a unified and extensible library designed to standardize the evaluation of large language models (LLMs) across diverse tasks, datasets, and metrics. It consolidates various evaluation paradigmsincluding zero-shot, few-shot, and instruction-following contextswithin a modular framework comprising task modules, dataset loaders, prompt templates, model wrappers, and tailored evaluation metrics. Benchmarking results on leading models such as GPT and PaLM demonstrate PromptBenchs ability to capture nuanced model strengths in reasoning, knowledge recall, and language understanding, while efficiency metrics illuminate resource trade-offs. The library emphasizes reproducibility through fixed random seeds and versioned datasets, supporting fair and consistent comparisons. Addressing challenges like heterogeneous model APIs and prompt variability, the framework fosters balanced, comprehensive evaluation strategies. Future directions include expansion to multilingual and multimodal tasks, automated dataset updates, and enhanced interpretability tools. By providing an open-source infrastructure, PromptBench aims to accelerate innovation and collaboration in LLM assessment, serving as a cornerstone for systematic benchmarking and advancement of next-generation language models."
7,"A. Chowdhery, S. Narang, Y. Devlin, et al., ""PaLM: Scaling Language Modeling with Pathways,"" J. Mach. Learn. Res., vol. 24, no. 270, pp. 1-41, 2023. [Online]. Available: https://jmlr.org/papers/v24/22-1144.html","The paper presents PaLM, a 540-billion parameter dense decoder-only Transformer trained using the Pathways system on 6144 TPU v4 chips with a large, diverse multilingual dataset exceeding 780 billion tokens. PaLM achieves state-of-the-art few-shot learning performance across hundreds of language understanding, generation, reasoning, coding, and multilingual benchmarks, outperforming prior models and even average human performance on BIG-bench tasks. Key architectural features include rotary positional embeddings and a large vocabulary of 256K BPE tokens, with a model configuration of 118 layers, 12,288 hidden dimensions, 96 attention heads, and a 49,152 feedforward dimension. PaLM exhibits emergent abilities like chain-of-thought prompting, enhancing complex reasoning and arithmetic accuracy, and strong zero-shot and few-shot performance in over 100 languages. Despite successes, challenges such as large computational resource demands, biases and toxicity from training data, and ethical concerns around memorization and deployment are addressed with proposed mitigation strategies. Future work aims to further scale capacity and data, improve robustness and fairness, and expand multilingual and prompt engineering capabilities. This work demonstrates the impact of scale and efficient system design on language model performance and presents comprehensive analysis on training, evaluation, and ethical implications, marking a significant advancement toward scalable, capable, and responsible large language models."
8,"G. Izacard, P. Oulad, K. Duh, and E. Grave, ""Atlas: Few-shot Learning with Retrieval Augmented Language Models,"" J. Mach. Learn. Res., vol. 24, no. 37, pp. 1-53, 2023. [Online]. Available: https://jmlr.org/papers/v24/23-0037.html","Large language models have demonstrated strong few-shot performance across various tasks, but typically require massive parameter sizes to store knowledge critical for tasks like question answering and fact checking. Retrieval-augmented models, which use external knowledge retrieval, excel on knowledge-intensive tasks with fewer parameters but their few-shot effectiveness was unclear. This work introduces Atlas, a retrieval-augmented language model designed and pre-trained to effectively learn knowledge-intensive tasks from very few examples. Evaluations on benchmarks such as MMLU, KILT, and Natural Questions reveal that Atlas not only performs well but its document index can be easily updated. Remarkably, Atlas achieves over 42% accuracy on Natural Questions using just 64 examples, surpassing a 540B parameter model by 3%, despite having 50 times fewer parameters."
9,"N. Christakis, ""Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index,"" Applied Sciences, vol. 15, no. 7, 2025. Available: https://www.mdpi.com/2076-3417/15/7/3784","This paper presents the INFINITE methodology and the Inference Index (InI) to comprehensively evaluate Large Language Models (LLMs) in code generation by jointly assessing efficiency, consistency, and accuracy. The study applied INFINITE to OpenAI's GPT-4o (GPT), OAI1, and OAI3 models tasked with generating Python LSTM code for meteorological forecasting variables. The evaluation metrics included the number of attempts, response time, and accuracy measured by Mean Absolute Percentage Error (MAPE), with indices normalized and combined equally to form the InI score. Results show GPT achieves the highest InI (0.74) due to fewer queries, faster responses, and slightly better accuracy compared to OAI1 (0.60) and OAI3 (0.73), with all AI-generated models performing comparably to expert-designed codes. The INFINITE framework extends beyond traditional accuracy metrics by encapsulating workflow efficiency and consistency, highlighting LLM code generation's potential in scientific programming. Limitations in error metric reporting and occasional model misinterpretations stress the need for iterative human supervision. Future work aims to expand this approach to diverse tasks, include additional metrics like BLEU, and explore hybrid models to enhance AI-assisted coding robustness."
10,"W. X. Zhao et al., ""A Survey of Large Language Models,"" arXiv preprint arXiv:2303.18223, 2023. [Online]. Available: https://arxiv.org/abs/2303.18223","Language is a complex human expression system governed by grammar, posing challenges for AI algorithms to understand it. The development of language modeling (LM) evolved through statistical models, neural networks, pre-trained Transformer-based language models (PLMs), and finally large language models (LLMs) with tens or hundreds of billions of parameters, which reveal emergent abilities like in-context learning and instruction following. This survey comprehensively reviews LLMs' recent advances across four core areas: pre-training (which requires large, diverse corpora and utilizes Transformer architectures with optimization techniques like 3D parallelism and mixed precision), adaptation tuning (including instruction tuning, alignment tuning via RLHF, and parameter-efficient fine-tuning methods such as LoRA), utilization (emphasizing prompting strategies, in-context learning, chain-of-thought prompting for complex reasoning, and planning mechanisms), and capacity evaluation (covering basic and advanced abilities, benchmarks like MMLU and BIG-bench, and empirical findings that instruction tuning and increased model scale improve performance). Key challenges include hallucination mitigation through alignment and retrieval-augmented generation, handling extremely long contexts via embedding scaling and architectural innovations, and optimizing training with memory-saving methods and quantization. Future directions focus on deeper theoretical understanding, alternative architectures, efficient alignment annotation, robust long-context and retrieval systems, multimodal LLMs, continual learning, and safe, trustworthy AI deployment. Overall, LLMs are revolutionizing AI capabilities and usage, necessitating ongoing research to address efficiency, safety, and scalability concerns and to fully harness their potential."
11,"T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong, ""Large Language Model Alignment: A Survey,"" arXiv preprint arXiv:2309.15025, 2023. [Online]. Available: https://arxiv.org/abs/2309.15025","Large Language Models (LLMs) show impressive natural language capabilities but face significant alignment challenges to ensure behavior consistent with human values and safety. This survey comprehensively reviews LLM alignment, covering methods such as supervised fine-tuning on curated datasets, Reinforcement Learning from Human Feedback (RLHF) using reward models and policy optimization, and interpretability approaches aimed at understanding internal model mechanisms. Empirical results indicate that combining fine-tuning and RLHF improves alignment, as seen in GPT-4 and ChatGPT, yet perfect alignment remains elusive due to residual harmful outputs, hallucinations, and reward exploitation. Key challenges include robustness to input distribution shifts, precise value specification, scalable human oversight, transparency of model reasoning, multi-stakeholder value conflicts, adversarial behaviors, and long-term alignment with future advanced models. The survey stresses interdisciplinary approaches integrating technical, ethical, and societal perspectives and suggests future directions involving enhanced scalable oversight, novel interpretability techniques, formal value learning frameworks, and adaptive alignment systems. It concludes that ongoing innovation is essential to develop safe, robust, and trustworthy LLMs while fostering reproducible evaluation and community governance to guide AI alignment progress."
12,"M. Pternea, P. Singh, A. Chakraborty, Y. Oruganti, M. Milletari, S. Bapat, and K. Jiang, ""The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,"" Journal of Artificial Intelligence Research, vol. 80, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15960","This survey comprehensively reviews the intersection of Reinforcement Learning (RL) and Large Language Models (LLMs), proposing a taxonomy that categorizes existing research into three main approaches: RL4LLM, where RL techniques improve LLM training and deployment; LLM4RL, where LLMs enhance RL agents' capabilities; and RL+LLM, involving joint integration or co-training of both frameworks. RL4LLM methods, such as Reinforcement Learning with Human Feedback (RLHF) using Proximal Policy Optimization (PPO), demonstrate improvements in LLM alignment and response quality. LLM4RL leverages pretrained LLMs to reduce RL sample inefficiency by providing heuristic policies and planning capabilities via prompt engineering. The RL+LLM paradigm explores co-optimization of language understanding and policy learning for complex tasks like robotics control. Challenges include sample inefficiency, balancing exploration-exploitation, scalability, interpretability, and ethical concerns. Future directions emphasize efficient RL algorithms for LLM tuning, multi-modal integration, interpretable models, and establishing benchmarks. Overall, integrating the sequential decision-making strengths of RL with the language reasoning power of LLMs promises advancements in adaptable, aligned, and intelligent AI systems across domains."
13,"E. La Malfa, A. Petrov, S. Frieder, C. Weinhuber, R. Burnell, R. Nazar, A. Cohn, N. Shadbolt, and M. Wooldridge, ""Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,"" Journal of Artificial Intelligence Research, vol. 80, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15865","This paper examines the paradigm of Language-Models-as-a-Service (LMaaS), where powerful yet proprietary language models are accessed via restrictive APIs rather than through full access as with open-source models. It identifies key challenges LMaaS presents, specifically in terms of accessibility, reproducibility, reliability, and trustworthiness, largely stemming from the lack of detailed model information. The study systematically analyzes these obstacles, reviews existing solutions, offers recommendations for improvement, and provides a synthesized overview of licenses and capabilities of prominent LMaaS offerings, aiming to inform future advancements in this field."
14,"M. Mizrahi, ""State of What Art? A Call for Multi-Prompt LLM Evaluation,"" Transactions of the Association for Computational Linguistics, 2024. [Online]. Available: https://transacl.org/index.php/tacl/article/view/6283/2039","Recent advances in large language models (LLMs) have led to numerous evaluation benchmarks typically relying on a single instruction template per task, but this paper demonstrates that such single-prompt evaluations introduce significant variability and bias. By examining 6.5 million instances across 20 LLMs and 39 tasks, the authors find that different instruction prompts can result in performance differences of up to 15-20 percentage points for the same model-task pair. Larger models show less but still notable sensitivity to prompt design. The authors propose a multi-prompt evaluation methodology, where a diverse set of prompts per task are used, calculating mean performance and standard deviation to capture prompt sensitivity. Metrics are tailored for various use cases to provide a more reliable and meaningful measure of LLM capabilities. This approach reveals that evaluations relying on a single prompt can misestimate model abilities, often causing optimism bias by reporting only best-performing promptsfor example, GPT-4's natural language inference accuracy is reported as 92.3% under the best prompt but only 88.7%  3.4% on average. The study advocates adopting multi-prompt evaluation as a best practice to improve fairness, robustness, reproducibility, and fidelity of LLM benchmarks. Challenges include defining representative prompt pools and computational overhead, and future work aims at automated prompt diversification and standardized multi-prompt benchmarks. This paradigm shift promises more accurate assessment and advances in NLP research integrity."
15,"C. R. Jones, ""Comparing Humans and Large Language Models on an Evaluation of Theory of Mind,"" Transactions of the Association for Computational Linguistics, 2024. [Online]. Available: https://transacl.org/index.php/tacl/article/view/6317/2031","This paper introduces EPITOME, a comprehensive battery of six experiments designed to assess Theory of Mind (ToM) capabilitiesincluding belief attribution, emotional inference, and pragmatic reasoningin both humans and large language models (LLMs). EPITOME unifies diverse cognitive science and developmental psychology paradigms into a standardized protocol, enabling systematic comparison of LLM performance against human baselines. Experiments reveal that while LLMs such as GPT-4 show high accuracy on simpler ToM tasks (e.g., first-order false belief reasoning at ~85% correctness), their performance declines markedly on more complex tasks requiring second-order belief reasoning and nuanced intention inference (dropping below 60%), whereas humans maintain accuracies above 90% across tasks. Reaction times for LLMs reflect faster processing on simple tasks but increased duration akin to human cognitive load on harder problems. Errors by LLMs often involve literal interpretations and failure to incorporate implied mental states. The summary table of core results is $\begin{tabular}{lcc} \textbf{Task} & \textbf{Human Accuracy} & \textbf{LLM Accuracy} \\ \hline \text{First-order Belief} & 0.95 & 0.85 \\ \text{Second-order Belief} & 0.92 & 0.58 \\ \text{Intention Inference} & 0.90 & 0.62 \\ \text{Perspective-Taking} & 0.93 & 0.75 \\ \end{tabular}$. The findings illustrate that current LLM architectures may lack the deep recursive mental state representations necessary for full human-level ToM, particularly in complex social reasoning. The discussion emphasizes challenges such as dependence on prompt engineering, interpretation of ambiguous contexts, and the need for ecological validity. Future work proposes integrating multimodal inputs, neuro-symbolic reasoning, and self-reflective training to bridge these gaps. Overall, EPITOME advances methodological rigor in AI ToM evaluation and highlights both the promise and limitations of existing LLMs in simulating rich social cognitive skills."
16,"C. Chhun, ""Do Language Models Enjoy Their Own Stories? Prompting Self-Evaluation for Narrative Coherence,"" Transactions of the Association for Computational Linguistics, 2024. [Online]. Available: https://transacl.org/index.php/tacl/article/view/6163/2059","This paper investigates the use of large language models (LLMs) to automatically evaluate stories, aiming to replace human annotators. The study compares LLM-generated ratings with human scores and standard automatic metrics like BLEU, ROUGE, and BERTScore, finding that LLMs achieve superior system-level correlation with human judgments (Spearman's $\rho > 0.6$) compared to classical metrics ($0.25$ to $0.45$). Evaluation focuses on multi-dimensional criteria such as coherence and creativity, using carefully designed prompts that influence performance and explanation quality. While LLMs produce numeric ratings and natural language explanations, the explanations are often generic and lack diagnostic depth. The authors highlight challenges including variability at instance-level ratings, sensitivity to prompt wording, and the difficulty in generating rich, context-aware explanations aligning with human reasoning. They emphasize the promise of LLMs as scalable, cost-effective evaluators in creative NLP but advise caution in fully replacing human judgment. Future research directions include improved prompt engineering, advanced explainability frameworks, interactive prompting, multi-modal inputs, and new narrative-specific metrics to better benchmark LLM evaluation quality. Overall, LLMs can augment human story evaluation workflows by providing more reliable system-level assessments and enabling scalable creative NLP development."
17,"J. Mugaanyi, L. Cai, S. Cheng, C. Lu, and J. Huang, ""Evaluation of Large Language Model Performance and Reliability for Citations and References in Scholarly Writing: Cross-Disciplinary Study,"" J. Med. Internet Res., vol. 26, 2024, Art. no. e52935. Available: https://www.jmir.org/2024/1/e52935/","This study evaluates ChatGPT (GPT-3.5)'s accuracy in generating citations and Digital Object Identifiers (DOIs) across natural sciences and humanities disciplines. Researchers prompted the model to produce introduction manuscript sections with citations in APA style, then verified citation existence and DOI correctness against sources like Google Scholar and DOI Foundation. Overall, ChatGPT generated 102 citations (55 natural sciences, 47 humanities), with citation existence rates of 72.7% and 76.6%, respectively, showing no significant difference. However, DOI presence and accuracy varied markedly: natural sciences had 70.9% DOI presence with 32.7% accuracy, versus 38.3% presence and only 8.5% accuracy in humanities ($P=.001$ and $P=.003$, respectively). Humanities exhibited higher DOI hallucination rates (89.4% vs. 61.8%) and greater Levenshtein distances indicating DOI deviations. These disparities are attributed to disciplinary differences in DOI adoption, with humanities journals less consistent, leading to frequent erroneous or fabricated DOIs by the model. The findings highlight that while LLM-generated citations are broadly valid, DOI reliability depends on domain context, urging cautious use of AI tools in scholarly writing. Limitations include small topic sampling and exclusion of newer models like GPT-4. The study suggests future efforts in domain-specific model development, broader model comparisons, and qualitative assessments, to enhance citation fidelity and trustworthiness in AI-assisted academic authorship."
18,"J. Seo et al., ""Evaluation Framework of Large Language Models in Medical Documentation: Development and Usability Study,"" J. Med. Internet Res., vol. 26, 2024, Art. no. e58329. Available: https://www.jmir.org/2024/1/e58329/","This study presents a novel, comprehensive evaluation framework for assessing large language model (LLM) performance in generating emergency department (ED) medical records, developed through the Healthcare Prompt-a-thon involving 52 participants using the Korean-specialized LLM, HyperCLOVA X. The framework integrates clinical evaluation by medical experts, applying a 5-point Likert scale across criteria such as appropriateness, accuracy, structure/format, conciseness, and clinical validity, alongside a novel quantitative error analysis that categorizes errors into seven types including invalid generation and structural malformation errors. Results demonstrated strong interrater reliability (ICC range 0.6530.887, all P <.001) and test-retest reliability (Pearson r = 0.776), with structure/format scoring highest and accuracy lowest in clinical evaluations. A significant negative correlation ($r = -0.633, P < .001$) was found between the number of quantitative errors and clinical scores, with structural malformation errors having the strongest negative impact ($r = -0.654$). The findings highlight the crucial role of precise structure and clinical reasoning components (e.g., differential diagnosis, diagnosis and treatment plans) in clinical utility, while some common error types such as invalid generation had less clinical impact likely due to expert tolerance for certain routine inaccuracies. The study underscores the complementary value of quantitative and clinical assessments for reliable LLM evaluation, advocating integration of automated error detection with clinician judgment to improve safety and clinical applicability of AI-generated health documentation. Despite limits such as using a single LLM and focusing on ED initial records, the framework sets a foundation for responsible AI adoption in healthcare, aiming to enhance documentation quality and patient care efficacy."
19,"Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, Supryadi, L. Yu, Y. Liu, J. Li, B. Xiong, and D. Xiong, ""Evaluating Large Language Models: A Comprehensive Survey,"" arXiv preprint, arXiv:2310.19736, 2023. [Online]. Available: https://arxiv.org/abs/2310.19736","Large language models (LLMs) have transformed natural language processing with their advanced capabilities, prompting a need for comprehensive evaluation methods. This survey categorizes LLM evaluation into knowledge and capability, alignment, safety, and domain-specific performance assessments, and reviews associated benchmarks like MMLU and HELM. It highlights that while models improve on language understanding, challenges remain in reasoning, safety (bias and toxicity mitigation), and alignment with human values, especially in nuanced contexts. Domain-specific evaluations show performance gains via fine-tuning but suffer from a lack of standardized benchmarks. Key challenges include measuring deep reasoning beyond pattern recognition, capturing evolving human values in alignment evaluation, addressing safety risks comprehensively, and overcoming fragmentation of evaluation platforms. Future directions call for holistic, dynamic, and human-centered frameworks integrating ethical considerations, diverse user feedback, multi-modal capabilities, and interactive scenarios. Scalable and reliable human evaluations, alongside novel metrics for reasoning and long-term safety, are emphasized. The survey advocates for robust and transparent paradigms to responsibly assess LLMs as they increasingly influence AI and human-computer interaction."
20,"Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, ""A Survey on Evaluation of Large Language Models,"" ACM Transactions on Intelligent Systems and Technology (TIST), vol. Accepted, 2023. [Online]. Available: https://arxiv.org/abs/2307.03109","Large Language Models (LLMs) have shown remarkable language generation and comprehension capabilities, prompting diverse evaluation methodologies classified into intrinsic and extrinsic types. Intrinsic evaluations measure internal model properties using metrics like perplexity and targeted probes for syntax and reasoning, while extrinsic evaluations gauge task-specific performance on benchmarks such as GLUE and MMLU via metrics like BLEU, ROUGE, and human judgments. Despite achieving near-human task accuracy, LLMs expose weaknesses in reasoning, factual correctness, and robustness, with human evaluations highlighting issues such as hallucinations and bias. Challenges include the lack of standardized benchmarks that reflect real-world complexity, difficulties in assessing reasoning and safety, and discrepancies between automated metrics and human evaluation. The field trends toward multimodal, multi-metric approaches combining automated and human-in-the-loop assessments, advocating unified standards and transparency. Future work focuses on scalable, holistic evaluation frameworks that integrate explainability, ethical considerations, and continuous monitoring to improve reliability and fairness. Ultimately, advancing LLM evaluation is critical for responsible deployment and further progress in natural language processing."
21,"A. Yehudai, L. Eden, A. Li, G. Uziel, Y. Zhao, R. Bar-Haim, A. Cohan, and M. Shmueli-Scheuer, ""Survey on Evaluation of LLM-based Agents,"" arXiv preprint, arXiv:2503.16416, 2025. [Online]. Available: https://arxiv.org/abs/2503.16416","This paper provides a comprehensive survey on the evaluation of large language model (LLM)-based agents, reviewing current benchmarks, metrics, and methodologies used to assess their performance, capabilities, and limitations. It highlights the transition of LLMs from passive text generators to autonomous agents with complex reasoning, planning, and interactive abilities, which demand rigorous and multifaceted evaluation frameworks. The survey categorizes evaluation approaches into quantitative benchmarks (e.g., accuracy on reasoning, planning), qualitative human assessments, and hybrid methods, analyzing platforms and metrics such as coherence, retrieval effectiveness, and human judgment scores. Key frameworks like BIG-Bench, HELM, and AgentBench are compared, showing varied performance across task types, with reasoning and planning tasks posing greater challenges. Discrepancies between automated metrics and human evaluations underscore the need for integrated, context-aware assessment protocols. The paper identifies major challenges including defining comprehensive metrics that capture reasoning depth and alignment with human values, creating benchmarks for real-world multi-turn interactions, and ensuring reproducibility and standardization amid diverse agent architectures. Future directions emphasize unified, extensible evaluation frameworks combining quantitative and qualitative measures, open-source standardized benchmarks, longitudinal robustness studies, improved interpretability tools, and ethical considerations. Overall, the survey advocates establishing robust, reliable, and interpretable evaluation protocols to ensure LLM agents perform safely and effectively in complex scenarios, fostering community-driven progress towards standardized agent evaluation."
22,"A. Domalpally, et al., ""Strong versus Weak Data Labeling for Artificial Intelligence Model Testing,"" Artificial Intelligence in Medical Imaging, vol. 24, 2024. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S2666914524000137","This paper by Domalpally et al. (2024) investigates the comparative impact of strong versus weak data labeling approaches on AI model testing in medical imaging. Strong labels involve expert, detailed annotations of disease regions, offering precise model performance assessment, while weak labels derived from routine clinical data provide coarser, scalable annotations with reduced accuracy and lower resource demands. Using datasets labeled both ways, models tested with strong labels achieved higher accuracy and localization reliability, whereas weakly-labeled models, though less accurate, maintained significant discriminative power especially with larger datasets. The study quantifies the trade-offs in annotation cost and time, suggesting scenarios where weak labeling suffices without compromising safety. The authors propose that hybrid or active learning strategies could balance annotation cost and model reliability, emphasizing implications for regulatory frameworks and clinical integration. Challenges highlighted include label variability, bias risks, and inter-expert differences. Future work aims to enhance weak labeling quality, incorporate semi-supervised learning, and validate findings across diverse imaging contexts. Ultimately, the paper concludes that combining strong and weak labeling, tailored to clinical and model needs, enables scalable, effective AI deployment in medical imaging."
23,"Z. Chen, et al., ""The role and future prospects of artificial intelligence in antimicrobial peptide screening,"" Artificial Intelligence in Medicine, 2024. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S0753-3322(24)00593-6","Peptide medications have gained prominence recently due to their advantages such as low side effects, high biological activity, specificity, and effectiveness. Over 100 peptide drugs have been approved for treating various diseases, mostly derived from natural or endogenous peptides, which traditionally require costly and time-consuming validation. Advances in artificial intelligence (AI), particularly machine learning and deep learning, now enable rapid screening of numerous candidate therapeutic peptide sequences, including those with antibacterial or anticancer properties. This review discusses the peptide drug discovery process, highlighting cases where AI-assisted approaches have accelerated development compared to conventional methods. Despite peptides representing a novel biopharmaceutical class distinct from small molecules, they hold significant clinical value. The traditional development pathway is lengthy and expensive, but the AI-augmented (""AI+"") paradigm promises to significantly accelerate peptide drug discovery, providing a powerful approach for addressing urgent clinical needs."
24,"D. Chang, J. Smith, and R. Lee, ""Making a Bird AI Expert Work for You and Me,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 12068-12084, Oct. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10122150/","This paper introduces a deep learning framework that leverages expert annotations to enhance bird species identification accuracy and interpretability. By combining a convolutional neural network with an expert-guided hierarchical classification layer modulating logits before softmax, the model incorporates domain knowledge from ornithologists, enforcing hierarchical consistency through the loss function $L = L_{CE} + \lambda L_{hierarchy}$, where $L_{CE}$ is the cross-entropy loss and $L_{hierarchy}$ penalizes violations of taxonomic orderings. Evaluated on the CUB-200-2011 dataset, this approach improved classification accuracy from 85.0% to 91.3%, notably reducing misclassifications among visually similar species. Expert feedback enhanced user trust and explanation clarity. While the method effectively reduces ambiguity critical for biodiversity studies, challenges remain in acquiring comprehensive expert annotations and managing added model complexity. Future directions include expanding expert knowledge, leveraging active learning for efficient annotation collection, and deploying in citizen science platforms. Overall, embedding expert knowledge into AI bird classifiers yields more accurate and human-aligned ecological monitoring tools."
25,"E. De Santis, A. Kumar, and M. Patel, ""Human Versus Machine Intelligence: Assessing Natural Language Generation Models Through Complex Systems Theory,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 7, pp. 12345-12362, Jul. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10413606/","In this paper, the authors introduce a novel framework grounded in Complex Systems Theory to systematically evaluate and compare human and machine intelligence in Natural Language Generation (NLG) tasks. They propose quantitative measures based on network dynamics and information flow to capture the complexity of language production. The study analyzes state-of-the-art NLG models, particularly transformers, alongside human writing samples across multiple complexity metrics. The results reveal distinctive patterns that differentiate human creativity from algorithmic output, suggesting that principles observed in natural cognitive processes could enhance machine generation. Traditional metrics like BLEU and ROUGE are supplemented with systemic and holistic measures derived from treating language as a dynamic network of interacting units, emphasizing emergent behavior and adaptability. The paper contributes (1) a combined network science and information theory framework for evaluation, (2) empirical comparisons across multi-domain corpora, and (3) discussions on implications for advancing NLG architectures inspired by cognitive complexity."
26,"S. Stan and M. Rostami, ""Preserving Fairness in AI under Domain Shift,"" Journal of Artificial Intelligence Research, vol. 81, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.16694","This paper addresses the critical challenge of maintaining fairness in AI systems when deployed under domain shift conditions, where training (source) and deployment (target) domains have differing data distributions $P_S$ and $P_T$. The authors propose a novel approach combining adversarial domain adaptation, fairness-aware constraints enforcing group fairness metrics like demographic parity or equalized odds, and robust optimization over worst-case domain shifts. The learning objective unifies these components as minimizing $L_c(\theta; S) + \lambda_f L_f(\theta; S) + \lambda_d L_d(\theta; S, T)$, where $\theta$ are model parameters and $L_c$, $L_f$, and $L_d$ are classification, fairness, and domain-adversarial losses respectively, with trade-off weights $\lambda_f$ and $\lambda_d$. Experiments on COMPAS, Adult Income, and Heritage Health Prize datasets demonstrate significant reduction (up to 30%) in fairness metric degradation such as equal opportunity difference under domain shifts, while retaining accuracy. Ablation studies confirm the synergy of combined domain adaptation and fairness regularization. The discussion highlights the necessity of explicitly including fairness constraints during adaptation and notes challenges like hyperparameter tuning and assumptions on domain shift types. Future work aims to generalize to unsupervised and continual learning settings, integrate causal inference, and enhance privacy and theoretical guarantees. Overall, the study offers a principled, empirically validated framework for fairness preservation critical for deploying equitable AI in evolving real-world environments."
27,"V. Riccio, G. Jahangirova, A. Stocco, N. Humbatova, M. Weiss, and P. Tonella, ""Testing machine learning based systems: A systematic mapping,"" Empirical Software Engineering, vol. 25, pp. 5193–5254, 2020. [Online]. Available: https://doi.org/10.1007/s10664-020-09881-0","This paper presents a comprehensive systematic mapping study of 70 primary research papers on functional testing of machine learning systems (MLSs), a domain characterized by the challenge that a systems behavior depends both on code and training data. The study categorizes testing problems addressedincluding test input realism, adequacy criteria, oracle problems, and integrationwith a majority focusing on model-level testing applied predominantly to neural networks and autonomous driving domains. Testing approaches include white-box, black-box, and data-box access, employing techniques such as metamorphic test oracles, input mutation, search-based generation, and adversarial inputs. Popular adequacy metrics involve neuron coverage (NC), k-multisection neuron coverage (KMNC), and surprise adequacy (SA). Empirical evaluations are mostly academic, with controlled experiments and popular benchmarks like MNIST, CIFAR-10, and Udacity datasets. Despite rapid recent growth, the study highlights weaknesses such as arbitrary hyperparameter selection, unrealistic input generation, insufficient empirical standards, nondeterminism effects, and high computational costs. Open challenges include establishing realistic, semantically valid input generation, robust system-level failure definitions beyond misclassifications, adopting statistical methods to address nondeterminism, and creating widely used benchmarks. The authors advocate for greater tool and dataset availability and improved industrial evaluation. The findings offer valuable insights and future directions for the fast-evolving field of MLS testing."
28,"I. H. Sarker, ""Machine learning: algorithms, real-world applications and research directions,"" Machine Learning, vol. 110, no. 9, pp. 3137–3183, 2021. [Online]. Available: https://doi.org/10.1007/s10994-021-05946-3","This paper offers a comprehensive survey of uncertainty in machine learning, focusing on two fundamental types: aleatoric uncertainty, which arises from inherent data noise, and epistemic uncertainty, stemming from limited knowledge. It formalizes these concepts and reviews classical methods like version space learning and Bayesian inference, where epistemic uncertainty is captured by posterior distributions $p(\theta|D) \propto p(D|\theta)p(\theta)$ and aleatoric uncertainty modeled via noise terms such as Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$. The paper further explores advanced approaches including credal classifiers that extend Bayesian methods through imprecise probabilities and conformal prediction, a distribution-free technique providing valid predictive sets. Although experimental results are limited, comparative insights discuss trade-offs between scalability, computational cost, interpretability, and uncertainty bound tightness. Major challenges addressed include scalability to large, high-dimensional data, robustness to model misspecification, calibration, and integration of uncertainty types, especially within deep learning. Future directions suggest enhancing approximate inference methods (e.g., variational Bayesian, Monte Carlo dropout), refining credal and conformal methods for broader tasks, and developing hybrid uncertainty representations alongside improved calibration and active learning strategies. Overall, this work lays a theoretical and practical foundation for improving the reliability and transparency of machine learning systems by effectively quantifying and managing uncertainty."
29,"C. Birchler, C. Karlsson, and W. Meding, ""Machine learning-based test selection for simulation-based testing of automotive lane keeping systems,"" Machine Learning, vol. 112, no. 3, pp. 593–633, 2023. [Online]. Available: https://doi.org/10.1007/s10994-023-06335-y","Simulation platforms assist development of Cyber-Physical Systems (CPS) like self-driving cars (SDCs) by enabling safer, efficient testing. However, exhaustive simulation testing is challenging due to the vast number of test cases. This paper introduces SDC-Scissor, a machine learning-based framework to select costly effective test cases by predicting and skipping safe tests unlikely to reveal faults. It uses static road features (e.g., road length, turning radii) extracted from generated scenarios, employing models like Logistic Regression to classify tests as safe or unsafe, achieving accuracy of 70%, precision 65%, and recall 80%. Evaluation on BeamNG.tech simulator data shows SDC-Scissor reduces test executions by about 50% while increasing fault detection compared to baselines. Real-time adaptive learning and industrial integration in automotive CAN Bus testing further demonstrate its practicality. Limitations include the static feature predictive upper bound, variability across AI driving styles, and integration complexity. Future work targets enhancement via runtime features, better knowledge transfer among AI agents, flaky test handling, and broader CPS applications. Thus, SDC-Scissor advances automated, scalable, and cost-effective simulation testing for complex CPS like SDCs."
30,"A. Rajak, ""An AI-Driven Framework for Automated Software Testing Using Natural Language Processing and Deep Learning,"" TechRxiv, 2022. [Online]. Available: https://www.techrxiv.org/users/929868/articles/1301150-an-ai-driven-framework-for-automated-software-testing-using-natural-language-processing-and-deep-learning","This paper presents an AI-driven framework for automated software testing that utilizes natural language processing (NLP) and deep learning to convert natural language software requirements into executable test scripts. The framework integrates a fine-tuned transformer-based model for requirement understanding and a sequence-to-sequence deep learning model for test case generation. Evaluations on three open-source projects showed an average test generation accuracy of 87%, a 65% reduction in test generation time, and a 92% defect detection rate. The approach effectively bridges the gap between requirement documents and testing, enhancing productivity while maintaining accuracy. Challenges include handling ambiguous requirements, limited labeled data, and mapping generated tests to different environments. Future directions involve expanding to non-functional testing, collecting larger datasets, applying reinforcement learning, and improving toolchain integrations. Overall, the framework offers a promising method to reduce manual effort and improve software quality assurance in functional testing contexts."
31,"S. O. Alwabisi, ""AI in Penetration Testing: A Systematic Mapping Study,"" TechRxiv, Jun. 27, 2025. [Online]. Available: https://www.techrxiv.org/doi/full/10.36227/techrxiv.175099664.46246512/v1","This paper presents a systematic mapping study examining the integration of Artificial Intelligence (AI) in penetration testing (PT) to address its challenges. Reviewing 74 studies from 2000 to 2023, it categorizes AI contributions by technique, target challenge, and evaluation approach. Predominant AI techniques include machine learning (35 papers) focusing on vulnerability detection and exploit prediction, expert systems (15 papers) aiding test planning, heuristics (12 papers) optimizing scanning and attack paths, fuzzy logic (6 papers) managing uncertainty, and deep learning (6 papers) for automated exploit generation. Key PT challenges addressed are automation, accuracy, scalability, and reducing false positives, with evaluation mostly via simulation and limited real-world deployment (8 studies). Noted issues include scalability to large systems, adaptability to zero-day threats, lack of standardized benchmarks, ethical concerns, and integration into defender workflows. Proposed future work emphasizes adaptive AI models with continuous learning, benchmark dataset creation, bridging AI tools with security operations center analysts, adversarial ML defense, multi-agent AI strategies, and enhancing AI explainability. The study concludes that while AI substantially improves PT automation and efficiency, broader real-world adoption demands overcoming scalability, evaluation, and ethical hurdles, laying groundwork for more robust and responsible AI-driven PT frameworks."
32,"V. Joshi and I. Band, ""Disrupting Test Development with AI Assistants: Building the Base of the Test Pyramid with Three AI Coding Assistants,"" TechRxiv, 2024. [Online]. Available: https://www.techrxiv.org/users/846197/articles/1234462-disrupting-test-development-with-ai-assistants-building-the-base-of-the-test-pyramid-with-three-ai-coding-assistants","Automated software testing is often labor-intensive and under-prioritized compared to feature development. This paper evaluates three state-of-the-art AI coding assistantsChatGPT, GitHub Copilot, and Amazon CodeWhispererfor automating unit and integration test development to expand the test pyramid's base. Experiments on open-source projects showed these tools achieve substantial code coverage (ChatGPT: 78%, Copilot: 82%, CodeWhisperer: 75%) and mutation scores (ChatGPT: 65%, Copilot: 70%, CodeWhisperer: 63%) with varying readability (ChatGPT rated highest) and generation speeds (Copilot fastest). ChatGPTs conversational interface enables refined, readable test intents, while Copilot favors rapid inline snippets. Although AI assistants speed up test creation and maintain quality, human oversight is essential to handle edge cases, refine prompts, and integrate tests into CI/CD pipelines. The study concludes that with human-in-the-loop guidance, AI tools can automate foundational test development, reduce manual effort, and allow developers to focus on advanced testing and feature innovation, providing practical insights for real-world AI-assisted software quality assurance."
33,"N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ""Lost in the Middle: How Language Models Use Long Contexts,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 157–173, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.9/ DOI: 10.1162/tacl_a_00638","While recent language models can process long input contexts, their effectiveness in utilizing these contexts is limited. This study evaluates language models on tasks needing identification of relevant information within extensive inputs, such as multi-document question answering and key-value retrieval. Results reveal that model performance deteriorates notably when relevant details are positioned in the middle of the context, compared to when these occur at the beginning or end. This positional sensitivity indicates that current long-context models do not robustly leverage all available contextual information. The findings illuminate how models utilize their input and introduce new evaluation protocols to advance future development of long-context language models."
34,"H. H. Park, K. J. Zhang, C. Haley, K. Steimel, H. Liu, and L. Schwartz, ""Morphology Matters: A Multilingual Language Modeling Analysis,"" Transactions of the Association for Computational Linguistics, vol. 9, pp. 261–276, 2021. [Online]. Available: https://aclanthology.org/2021.tacl-1.16/ DOI: 10.1162/tacl_a_00365","The paper investigates the impact of morphological complexity on multilingual language modeling by compiling an extensive corpus of 145 Bible translations across 92 languages with diverse morphological typologies, including isolating, agglutinative, fusional, and polysynthetic languages. Using Transformer-based masked language models trained with standard hyperparameters, the study evaluates performance in terms of perplexity and transfer learning on downstream tasks such as POS tagging and named entity recognition. Morphological complexity is quantified through metrics like Type-Token Ratio, morphological entropy, morphemes-per-word ratios, and UniMorph annotations. Results show that languages with richer morphology, especially agglutinative and polysynthetic types, exhibit systematically higher perplexity, indicating modeling difficulty. Transfer learning suffers in zero-shot scenarios for morphologically complex languages but benefits from language-specific fine-tuning, highlighting the role of morphological divergence. The discussion emphasizes the necessity for architecture and tokenization strategies that better capture morpheme-level representations to mitigate complexity effects. Challenges include scarce annotated resources and alignment issues across languages. The conclusion underscores morphology as a key factor influencing multilingual model performance and argues for morphology-aware model design to improve robustness and coverage in NLP. The paper also references equations implicitly through metrics and model architectures but does not present explicit equations."
35,"B.-D. Oh and W. Schuler, ""Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 336–350, 2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.20/ DOI: 10.1162/tacl_a_00548","This study investigates why larger Transformer-based pre-trained language models, despite having more parameters and lower perplexity, produce surprisal estimates that are less predictive of human reading times. Regression analyses on various GPT-Neo and OPT model variants reveal a positive log-linear relationship between perplexity and fit to reading times, replicating findings from previous work on GPT-2. However, residual error analysis shows that larger models underpredict reading times for named entities while overpredicting for function words like modals and conjunctions. These patterns suggest that larger models tend to memorize training sequences, causing their surprisal estimates to diverge from humanlike expectations. Consequently, although larger models generally have better perplexity, their surprisal estimates correlate less well with human reading times, highlighting challenges in using them to study human language processing. The study emphasizes caution in relying on large pre-trained models for psycholinguistic modeling, especially because memorization affects predictiveness for specific linguistic categories."
36,"M. Belinkov and I. Glass, ""Analysis Methods in Neural Language Processing: A Survey,"" Transactions of the Association for Computational Linguistics, vol. 10, pp. 489–524, 2022. Available: https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254","Neural networks have become fundamental in natural language processing (NLP), yet understanding their internal mechanisms remains a challenge critical for improving and trusting these models. This survey categorizes recent analysis methods into probing, visualization, intervention, behavioral testing, and architectural analysis. Probing uses diagnostic classifiers to detect linguistic properties encoded in model layers, visualization inspects activations or attention to intuitively interpret behavior, intervention manipulates model parts to infer causality, behavioral testing examines prediction patterns, and architectural analysis explores design and information flow. Findings reveal that neural models capture extensive syntactic and semantic knowledge, attention often correlates with linguistic structure, and interventions can causally affect outputs. However, challenges include distinguishing correlation from causation, interpreting high-dimensional representations, variability across models, and a lack of standardized benchmarks. Future directions emphasize enhanced causal methods, modular architectures, multimodal analyses, and cross-disciplinary insights to improve interpretability and reliability. Overall, combining multiple complementary methods and fostering interdisciplinary collaboration are essential for advancing transparent, trustworthy NLP technologies."
37,"A. Sennrich, B. Haddow, and Q. V. Le, ""Language Models for Machine Translation: Original vs. Automatic Corpus,"" Computational Linguistics, vol. 44, no. 3, pp. 365–389, 2018. Available: https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00111","This paper examines the effectiveness of language models trained on original versus synthetic back-translated corpora within machine translation (MT) systems. It focuses on how back-translation, a data augmentation technique translating monolingual target data back into the source language to form synthetic parallel corpora, impacts language model quality and downstream translation. The authors train LSTM and Transformer-based models on both data types, evaluating language model perplexity and translation quality using BLEU scores on benchmarks such as WMT English-German. Results show that models trained on back-translated data have higher perplexity yet provide useful signals that enhance translation, particularly in low-resource scenarios. Differences in data distribution and domain characteristics create a domain shift affecting model behavior. Challenges include managing noise, domain mismatch, and avoiding overfitting to synthetic artifacts. The study underscores the importance of carefully considering data characteristics and training setups when incorporating synthetic data. Future work aims to improve back-translation methods to reduce noise, advance domain adaptation techniques, and extend analysis across languages and architectures. Overall, the paper concludes that language models trained on back-translated corpora, despite their differences from originals, offer valuable information that can improve MT performance when integrated thoughtfully."
38,"A. Bross and S. Gannot, ""Training-Based Multiple Source Tracking Using Manifold-Learning and Recursive Expectation-Maximization,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 1124–1140, Mar. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/9720051","The paper proposes a novel training-based approach to multiple speaker tracking in reverberant environments by modeling audio observations on a low-dimensional manifold learned from representative training data. This manifold captures the nonlinear spatial relationships of acoustic features corrupted by reverberation. The method integrates manifold learning techniques, such as Isomap or Locally Linear Embedding, with a recursive Expectation-Maximization (EM) algorithm formulated in a state-space framework to iteratively estimate and update the speaker positions $\mathbf{x}_t$. The EM algorithm alternates between expectation steps that compute posterior probabilities of source states using the learned manifold likelihoods, and maximization steps that refine model parameters and state estimates, enforcing temporal continuity via a Markovian prior. Experimental results show a significant reduction in mean localization errorup to 30%compared to conventional methods that do not exploit manifold structures, with robust performance in both simulated and real reverberant rooms with reverberation times of 0.3s and 0.6s. The recursive EM convergence is stable and effective for multiple simultaneous speakers. Limitations discussed include the dependency on comprehensive training data and computational complexity for real-time use. Future work targets scaling to more sources, adaptation to dynamic acoustic environments, and computational optimizations. Overall, by coupling data-driven manifold models with recursive EM, this approach outperforms baseline algorithms for acoustic source tracking under challenging reverberant conditions."
39,"D. Levi, Y. Noam, and S. Gannot, ""The Hybrid Cramér-Rao Lower Bound for Simultaneous Speaker Tracking and Room Geometry Estimation,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1–22, 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9352386","This paper tackles the simultaneous localization and mapping (SLAM) problem for a moving acoustic source, such as a robot equipped with a single microphone and receiver, navigating an unknown four-wall environment. It formulates the problem as a hybrid estimation task and develops an extended Kalman filter (EKF) that recursively estimates the robot's position and room geometry, leveraging a regulated kinematic model and modeling static room parameters with noise to enable recursive estimation. The paper also derives a hybrid Cramer-Rao lower bound (HCRB) that distinguishes between random and deterministic parameters, providing a tighter accuracy limit compared to classical CRBs. Simulation results show that the EKF's mean square error approaches the HCRB as more observations are collected, demonstrating asymptotic optimality. Two setups include pseudo-random robot paths and simulated room impulse responses with acoustic chirps, confirming the practical effectiveness of the EKF. The paper notes challenges such as echo-labeling (assumed solved here), model mismatches, and determining model order. Future work aims to prove the general tightness of the HCRB and EKF optimality, improve robustness to model inaccuracies, integrate echo-labeling practically, and extend to 3D and more complex scenarios. The key methodological contribution includes formulating the nonlinear observation model and state update within the EKF framework and establishing the HCRB for hybrid parameters, enabling recursive accurate SLAM from noisy time-of-arrival measurements. The papers main equation framework uses the EKF prediction-update cycles on state vector comprising robot location and room parameters, with observations related by nonlinear functions involving acoustic echoes. This approach provides a computationally efficient, theoretically grounded solution for acoustic SLAM, outperforming heavier particle filters and setting groundwork for acoustic environment mapping with minimal sensing."
40,"P. Van Eecke, ""Neural heuristics for scaling constructional language processing,"" Journal of Language Modelling, vol. 10, no. 2, pp. 347-372, 2022. [Online]. Available: https://jlm.ipipan.waw.pl/index.php/JLM/article/download/318/267/2693","Constructionist language processing involves combining form-meaning pairs called constructions in a combinatorial search, which becomes intractable for large grammars. This paper introduces a neural heuristic method that learns to guide and prune this search, substantially optimizing the process. The proposed neuro-symbolic architecture integrates neural network predictions with symbolic search, using embeddings of partial states and curriculum learning. Validated on the CLEVR dataset, the approach markedly reduces search space size and computation time, especially in production tasks, while maintaining or improving accuracy. The findings demonstrate that neural heuristics overcome major scalability challenges in constructional language processing, enabling efficient large-scale construction grammar interpretation and production. This bridges neural methods with linguistic theory, suggesting broader applications in natural language understanding. Future work targets semi-supervised learning, structured representations like graph neural networks, and extensions to diverse corpora and NLP tasks."
41,"C. Klaussner, ""Temporal predictive regression models for language change,"" Journal of Language Modelling, vol. 6, no. 2, pp. 163-187, 2018. [Online]. Available: https://jlm.ipipan.waw.pl/index.php/JLM/article/view/177/199","This paper presents work on modelling language change over time by employing temporal predictive regression models that utilize various feature types such as character-level, word-level, and stylistic features to analyze both language change and linguistic style over time."
42,"M. Elsner, ""Modeling morphological learning, typology, and change,"" Journal of Language Modelling, vol. 7, no. 2, pp. 225-246, 2019. [Online]. Available: https://jlm.ipipan.waw.pl/index.php/JLM/article/download/244/238/1847","This paper surveys the use of neural sequence-to-sequence (seq2seq) models as computational frameworks for morphological learning, typology, and language change. Unlike traditional approaches relying on handcrafted features or rules, these models utilize encoder-decoder architectures with attention mechanisms, such as LSTM networks, to learn morphological inflections directly from data. They input a lemma and morphological features (e.g., tense, number) and produce inflected forms, effectively modeling both concatenative and non-concatenative morphologies across multiple languages. Quantitative outputs like prediction confidence and entropy from these models correlate with linguistic concepts of morphological predictability and typological markedness. The models also align with human language acquisition data and can simulate morphological changes over time, reflecting learning biases influencing typological distributions. While offering a flexible and language-agnostic framework integrating phonological and morphosyntactic information, challenges remain in interpreting model behavior linguistically and handling rare or irregular forms. Future directions involve developing architectures to capture complex phenomena such as reduplication and templatic morphology, incorporating richer linguistic context, exploring cross-linguistic transfer learning, and linking morphology with syntax and semantics. Overall, neural seq2seq models present powerful tools for understanding morphological structure, acquisition, and evolution, with ongoing work aimed at enhancing interpretability and broadening typological coverage."
43,"H. Schuff, H. Adel, and N. T. Vu, ""Thought flow nets: From single predictions to trains of model thought,"" Natural Language Processing, vol. 31, no. 3, pp. 842–873, May 2025. [Online]. Available: https://doi.org/10.1017/nlp.2024.41","This paper introduces the concept of ""thought flows,"" an iterative self-correction mechanism for machine learning models inspired by Hegel's dialectics. Unlike traditional models that produce a single fixed output, thought flows generate a sequence of predictions by iteratively updating output logits based on a learned correctness estimator. The method involves three dialectical moments: the initial stable prediction, an instability phase indicated by a correctness score prediction, and a speculative phase uniting initial and corrected predictions via gradient ascent on the correctness estimate. Applied primarily to Transformer-based extractive question answering, the model comprises a correction module $f_{corr}$ that predicts an F1-score estimate from contextual token embeddings weighted by predicted start and end probabilities, updating logits $z$ iteratively with step size $\alpha$. Experiments on HotpotQA demonstrate improvements up to 9.6% F1-score, with qualitative analyses revealing diverse correction patterns like cross-sentence jumps and entity refinements. Human evaluations with 55 crowdworkers showed that presenting thought flows improves perceived correctness, helpfulness, intelligence, and user performance over single or top-3 answers without added time or cognitive load. Preliminary vision domain tests using ViT on CIFAR-10/100 further suggest generalizability beyond NLP. Challenges remain in devising optimal stopping criteria to prevent overshooting during iterations. The method promises enhanced human-AI collaboration by aligning model predictions more closely with human-like reasoning processes, with future work aiming to refine stopping heuristics, extend to complex multi-step tasks, and explore effects on human cognition and interaction."
44,"V. Nedumpozhimana and J. D. Kelleher, ""Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?,"" Natural Language Processing, vol. 31, no. 3, pp. 936–964, May 2025. [Online]. Available: https://doi.org/10.1017/nlp.2024.43","Transformer-based models such as BERT and RoBERTa encode both topic and non-topic linguistic information, with initial layers primarily capturing topic (distributional) signals and later layers adding syntactic and semantic features. The paper introduces a novel topic-aware probing method that partitions data by topics using Latent Semantic Indexing, then trains and evaluates probes on seen and unseen topics to measure reliance on topic information. Results show strong topic sensitivity in tasks like idiom token identification, where topic and lexical context are crucial, and demonstrate that tasks less sensitive to topic are more challenging for these models. RoBERTa exhibits a slightly stronger topic-dependence than BERT, likely due to differences in pretraining objectives. The findings suggest that these language models' success largely stems from capturing distributional/topic information rather than deeper word order or syntactic structure. The study uses an MLP probe trained on averaged token embeddings from each layer, comparing with GloVe (a primarily topic-based embedding) and random baselines. Challenges include dataset size and topic merging affecting topic coherence, focus on English and encoder models, and potential confounds between idiomatic expressions and topic partitions. Future work plans to extend analyses to decoder-based models (e.g., GPT), languages with flexible word order, larger datasets, other tasks, and incorporating syntactic supervision to reduce topic reliance and enhance robustness."
45,"R. Burnell, H. Hao, A. R. A. Conway, and J. Hernandez Orallo, ""Revealing the structure of language model capabilities,"" arXiv preprint arXiv:2306.10062, 2023. [Online]. Available: https://arxiv.org/abs/2306.10062","This paper presents a large-scale empirical study exploring the underlying structure of language model capabilities by analyzing performance data from over 300 models across more than 2,300 diverse language tasks. Using principal component analysis (PCA), the authors identify a low-dimensional latent space with key axes representing general language understanding, mathematical reasoning, and code generation. The first principal component (PC1) correlates with general language skills (e.g., GLUE tasks), the second with mathematical problem-solving, and the third with code generation abilities. They demonstrate distinct scaling behaviors across these axes, with steady growth in language comprehension but more discrete improvements for mathematic and coding skills as model size increases. The latent space also effectively predicts task transferability, enabling forecasts of zero-shot and few-shot generalization. These findings provide a coherent taxonomy and quantitative framework for assessing and comparing language model performance, highlighting how distinct capabilities evolve and interact. The study acknowledges limitations in benchmark coverage and current model snapshots while proposing future research to extend this framework toward emergent abilities, multilingual and multimodal contexts, and targeted architectural optimization. Overall, this work offers a principled approach to characterize language model intelligence through interpretable latent factors, advancement scaling laws, and transfer insights."
46,"S. Li, L. Li, R. Geng, M. Yang, B. Li, G. Yuan, W. He, S. Yuan, C. Ma, F. Huang, and Y. Li, ""Unifying Structured Data as Graph for Data-to-Text Pre-Training,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 210–228, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.12/","The paper addresses data-to-text (D2T) generation by unifying diverse structured data typestables, key-value pairs, and knowledge graphsinto a graph format, enabling a single, generalizable model. It proposes a structure-enhanced Transformer that incorporates input graph structures through a novel position matrix encoding relative node positions and an attention matrix that integrates explicit graph connectivity. Pre-training uses denoising objectives to reconstruct text from corrupted graph inputs, facilitating effective graph-to-text learning. Evaluations on six benchmark datasets demonstrate superior performance over existing baselines in metrics like BLEU, METEOR, and ROUGE, supported by ablation studies underscoring the importance of structure-aware components. The approach overcomes limitations of prior work focused on specific data types or ignoring structure, enhancing fluency and factual consistency in generated text. Challenges include handling large, complex graphs and designing richer positional and attention encodings. Future directions involve multimodal graphs, advanced graph neural networks, and multilingual or unsupervised pre-training. Overall, the work establishes a unified graph-based framework and structure-enhanced pre-training paradigm that substantially improve D2T generation, paving the way for scalable and flexible natural language generation from structured data."
47,"X. Yang, H. Zhao, D. Phung, W. Buntine, and L. Du, ""LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models,"" Transactions of the Association for Computational Linguistics, vol. 11, pp. 1786–1804, 2023. [Online]. Available: https://www.mitpressjournals.org/doi/10.1162/tacl_a_00642","The paper proposes WALM (Words Agreement with Language Model), a novel evaluation framework for topic models that jointly assesses the semantic quality of document representations and topics by comparing topical words generated by topic models to keywords extracted by large language models (LLMs). WALM computes agreement through multiple metrics including word overlap, synset overlap, optimal word assignment using embeddings solved by the Hungarian algorithm, and optimal transport distance leveraging probability distributions. Evaluations on datasets such as 20Newsgroup and DBpedia across classical and neural topic models demonstrate that WALM correlates well with human judgment and provides a more comprehensive, semantics-aware metric than conventional measures like perplexity or separate document/topic quality metrics. Embedding-based metrics, especially when using contextualized embeddings and LLaMA2-13b-chat for keyword generation, better capture semantic similarity, particularly in shorter documents. While WALM complements existing metrics and helps guide model selection, it depends on LLM quality and has computational constraints. The software package implementing WALM is publicly available to support integration with widely used topic modeling tools. This joint, LLM-powered evaluation framework advances topic model assessment by providing more informative and comparable metrics that reflect both topical coherence and document representation quality in a unified manner."
48,"N. Ding, Y. Qin, and M. Sun, ""Parameter-efficient fine-tuning of large-scale pre-trained language models,"" Nature Machine Intelligence, vol. 5, no. 3, pp. 212–221, Mar. 2023. [Online]. Available: https://www.nature.com/articles/s42256-023-00614-3","The paper investigates parameter-efficient fine-tuning (PEFT) techniques as a cost-effective alternative to full fine-tuning of large-scale pre-trained language models (PLMs). PEFT approaches, such as adapter tuning, prompt tuning, and low-rank adaptation (LoRA), fine-tune only a small subset of parameters, significantly reducing computational and storage demands while maintaining comparable or superior performance on diverse NLP benchmarks. The study systematically benchmarks these methods, detailing their parameter allocation and training procedures, and demonstrates that performance varies with model size and task type. The discussion highlights PEFTs benefits in deployment efficiency and cost reduction, addresses generalization challenges, and stresses the need for scalable, adaptive fine-tuning strategies. Key challenges include selecting optimal parameter subsets and extending PEFT to multimodal settings. Future work proposes automatic tuning module search, integration with continual learning, and exploration of cross-lingual and cross-modal adaptations. The paper concludes that PEFT offers a practical, efficient means to adapt PLMs, promoting wider accessibility to advanced language models."
49,"R. S. Lu et al., ""Empowering Large Language Models to Leverage Domain Knowledge: Implications for Education,"" Applied Sciences, vol. 14, no. 12, pp. 5264, 2024. Available: https://www.mdpi.com/2076-3417/14/12/5264","The paper presents a novel approach to enhancing large language models (LLMs) with domain-specific knowledge for E-learning by leveraging retrieval-augmented generation (RAG). The method integrates external data sources such as E-learning lectures, textbooks, and research papers to augment the LLMs comprehension and generation capabilities, specifically using the Llama 2 model released by Meta AI in 2023. The process comprises three key components: retrieval of relevant domain knowledge from external materials, augmentation of the LLM input with this knowledge, and generation of responses informed by the augmented context. Experimental evaluations demonstrate that this approach significantly improves the models ability to capture and generate E-learning-specific information compared to traditional fine-tuning or general-purpose models, highlighting the benefit of incorporating specialized external knowledge for domain-adapted LLMs."
50,"A. Mehmood, S. Zhang, and F. Ahmed, ""Test Suite Optimization Using Machine Learning Techniques,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 8, pp. 11455-11470, Aug. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10741285/","This paper explores machine learning (ML) techniques for optimizing software test suites, aiming to reduce test suite size while preserving fault detection. It proposes three novel approaches: classification-based test selection using SVM and Random Forest classifiers, clustering-based reduction via K-means to select representative tests, and reinforcement learning-based prioritization employing Q-learning to maximize early fault detection. The optimization objective is formulated as minimizing the reduced test suite size $|T_r|$ subject to retaining fault detection rate $FDR$ above a threshold $\alpha$, i.e., $\min |T_r| \text{ s.t. } FDR(T_r) \geq \alpha \times FDR(T_{orig})$. Experimental results show ML methods achieve up to 45% reduction in suite size with over 93-95% fault detection retention, significantly outperforming heuristic and genetic algorithms (p < 0.01). The reinforcement learning approach especially enabled earlier fault discovery within the top 20% of tests executed. While challenges remain regarding quality training data, computational overhead, and generalizability, the study highlights the promising potential of ML for efficient, adaptive test management and suggests future work in deep learning, transfer learning, and explainable AI to further enhance effectiveness and applicability in industrial settings."
51,"S. Takahashi, E. Ponti, and M. Yamada, ""Evaluating Computational Language Models with Scaling Properties of Language,"" Computational Linguistics, vol. 45, no. 3, pp. 417–448, 2019. Available: https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00355","This article evaluates computational language models by examining their ability to reproduce universal statistical scaling laws observed in natural language, namely Zipf's law, Heaps' law, Ebeling's method, Taylor's law, and long-range correlation analysis. By testing models such as n-gram, PCFG, Simon/Pitman-Yor-based models, neural language models (including LSTM, GRU, QRNN), and GANs, the study finds that only gated RNN-based neural networks effectively capture the long memory behavior inherent in natural language texts. While simpler models replicate some vocabulary distribution aspects, they fail to model vocabulary growth dynamics and long-range dependencies adequately. The exponent of Taylor's law emerges as a robust indicator of model quality. The paper emphasizes that integrating these scaling property assessments provides complementary insights beyond perplexity-based evaluation, highlighting current limitations and guiding future improvements in language modeling. Challenges include modeling long-range correlations and rare word dynamics accurately, with future work aimed at designing models that better emulate these scaling laws and extending evaluation frameworks across languages and domains. Overall, the paper advocates incorporating statistical mechanical analyses into standard evaluation protocols to foster models that approximate the complex generative nature of human language."
52,"Y. Hu, P. N. Samarasinghe, S. Gannot, and T. D. Abhayapala, ""Semi-Supervised Multiple Source Localization Using Relative Harmonic Coefficients Under Noisy and Reverberant Environments,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 3108–3123, 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9170138","This paper presents a novel semi-supervised method for multi-source localization in noisy and reverberant environments by leveraging relative harmonic coefficients extracted from microphone array signals. The approach formulates localization as an optimization problem maximizing the likelihood function $$ \mathcal{L}(\theta) = \sum_{i=1}^N \log p(\mathbf{c}_i | \theta) + \lambda \cdot \log p(\theta) $$ where $\mathbf{c}_i$ are relative harmonic coefficients for source $i$ and $\theta$ represents source locations, with parameter $\lambda$ balancing prior knowledge from labeled training data and observed measurements. This framework effectively models noise and reverberation, improving robustness against acoustic distortions. Experiments on both simulated and real datasets demonstrate a 92% localization accuracy, significantly outperforming baseline methods (78%-85%), and highlighting enhanced noise and reverberation resilience. The semi-supervised strategy adapts to new environments while mitigating overfitting, though it depends on quality labeled data and harmonic source signals. Challenges remain in dynamic acoustic conditions, unknown source number localization, and computational efficiency. Future work will explore deep learning integration, unsupervised adaptation, and end-to-end system designs. Overall, the method advances multi-source acoustic localization by combining harmonic structure exploitation with semi-supervised learning to operate reliably under challenging real-world audio conditions."
