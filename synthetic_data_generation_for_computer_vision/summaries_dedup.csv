Index,Citation,Summary
1,"H. H. Rashidi, S. Albahra, B. P. Rubin, and B. Hu, ""A novel and fully automated platform for synthetic tabular data generation and validation,"" Scientific Reports, vol. 14, Article number: 23312, 2024. [Online]. Available: https://www.nature.com/articles/s41598-024-73608-0","Healthcare machine learning (ML) is hampered by strict data privacy regulations, but synthetic data offers a route to circumvent these barriers by mimicking real datasets without risking patient privacy. The Synthetic Tabular Neural Generator (STNG) is a fully automated platform that integrates eight parallel synthetic data generation methodsincluding both classic and neural network-based models such as Gaussian copula, copula-GAN, CT-GAN, and TVAE, plus STNGs own multi-function variants. For each dataset, STNG generates synthetic data, then uses an embedded Auto-ML module featuring five classification algorithms (logistic regression, naive Bayes, KNN, SVM, and MLP) applied in 80 pipelines per generator (yielding up to 595,840 ML models per run) to validate and compare synthetic datasets. The quality of synthetic data is measured by a novel STNG ML score, which combines ROC-AUCbased metrics ($\text{Initial Auto-ML Score} = 1 - \min(|\text{AUC}_{sr} - \text{AUC}_{rr}| + |\text{AUC}_{ss} - \text{AUC}_{sr}|, 1)$) and statistical similarity (KL divergence, KS statistic). In empirical studies with 12 real healthcare datasets (covering both binary and multiclass tasks), STNGs multi-function approaches generally produced synthetic datasets with better ML and statistical fidelity to real data than generic generators. For example, in heart disease and stroke datasets, the STNG Gaussian copula generator achieved the highest ML scores and statistical similarity, while modified STNG approaches consistently succeeded where some baseline methods failed (e.g., class imbalance issues in TVAE). STNG is delivered as a secure, dockerized application with enterprise-level access controls and an intuitive UI. Although current limitations include focus on classification and tabular data, future work will extend to regression, time series, and other data modalities. By reducing regulatory obstacles and enabling large-scale synthetic data generation and validation, STNG can accelerate healthcare ML innovation while raising important ethical considerations regarding fairness and data protection."
2,"L. Kühnel, J. Schneider, I. Perrar, T. Adams, S. Moazemi, F. Prasser, U. Nöthlings, H. Fröhlich, and J. Fluck, ""Synthetic data generation for a longitudinal cohort study – evaluation, method extension and reproduction of published data analysis results,"" Scientific Reports, vol. 14, Article number: 14412, 2024. [Online]. Available: https://www.nature.com/articles/s41598-024-62102-2","Access to individual-level health data is vital for scientific advancement, especially for AI-based research, but is often restricted by privacy regulations. This study addresses these limitations by generating fully synthetic datadata produced via random processes that mimic the statistical properties of real datasets without direct correspondence to real individualsusing a state-of-the-art method tailored to nutrition research (DONALD study). Extensive quality assessments were performed, surpassing mere descriptive statistics to include correlation structures, direct dependencies, and the ability to reproduce significant results from real-world analyses. Notably, the team developed VAMBN-MT, an LSTM-based extension of VAMBN, to overcome shortcomings in capturing longitudinal and direct dependency structures; VAMBN-MT generally outperformed the original and enabled accurate reproduction of real-world outcomes. The findings emphasize the importance of use case-specific evaluations and expert input, demonstrating that large sample sizes and joint modeling of key variables enhance synthetic data utility and quality. Future work aims to expand the approach to other longitudinal datasets and rigorously analyze privacy risks to balance utility and confidentiality. While the real data are not publicly accessible due to GDPR constraints, the adapted algorithms code is available at https://github.com/nfdi4health/vambn-extensions-evaluations/."
3,"Z. Qian, T. Callender, B. Cebere, S. M. Janes, N. Navani, and M. van der Schaar, ""Synthetic data for privacy-preserving clinical risk prediction,"" Scientific Reports, vol. 14, Article number: 25676, 2024. [Online]. Available: https://www.nature.com/articles/s41598-024-72894-y","This study explores the application of synthetic datagenerated using advanced privacy-preserving generative adversarial networks (DPGAN, PATEGAN, ADSGAN)to support clinical prognostic modeling pipelines without exposing real patient information. Focusing on ever-smokers from the UK Biobank, the authors created synthetic cohorts and evaluated their usefulness for descriptive statistics, exploratory data analysis (e.g., principal component analysis), feature selection, hyperparameter tuning, and model training to predict 5-year lung cancer risk. Synthetic datasets produced by ADSGAN and PATEGAN closely mimicked the real data's distribution, successfully captured rare subpopulations, and allowed selection of important predictors akin to those from real data, albeit with a modest drop in model discrimination due to privacy-utility trade-offsparticularly for DPGAN, which added excessive noise. Brier scores for models trained on ADSGAN were equivalent to those for real data, supporting the value of synthetic data for model development even under strict ""no-release"" paradigms. Privacy risks, including reidentification and membership inference, were discussed via metrics such as k-anonymity and DOMIAS. The findings highlight the potential of synthetic data to accelerate healthcare research and collaboration while emphasizing the need for preprocessing and further methodological advances to maximize utility and privacy across healthcare domains."
4,"D. Perri, M. Simonetti, and O. Gervasi, ""Synthetic Data Generation to Speed-Up the Object Recognition Pipeline,"" Electronics, vol. 11, no. 1, p. 2, 2022. [Online]. Available: https://www.mdpi.com/2079-9292/11/1/2","This paper introduces a methodology for generating synthetic images, leveraging advanced 3D modeling software, to train neural networks for shape and object recognitionparticularly beneficial in contexts where acquiring real images is challenging, costly, or hazardous. By applying this approach to two use casespictorial style recognition and the detection of people at seathe authors demonstrate its validity, provided rigorous practices and the full capabilities of tools like Blender and Unity3D are utilized. They share GPL-licensed code that automates image transformations and randomizes environmental conditions for comprehensive data augmentation. The results affirm that their method offers a simple, reliable, cost-effective, and safe solution for supplying neural network training data across diverse application domains."
5,"M. Goyal and Q. H. Mahmoud, ""A Systematic Review of Synthetic Data Generation Techniques Using Generative AI,"" Electronics, vol. 13, no. 17, p. 3509, 2024. [Online]. Available: https://www.mdpi.com/2079-9292/13/17/3509","Synthetic data offer promising approaches to overcoming data scarcity, privacy issues, and algorithmic bias in machine learning, as they retain the statistical patterns and behaviors of original datasets while modifying their actual contents. Methods for synthetic data generation include large language models (LLMs), generative adversarial networks (GANs), and variational autoencoders (VAEs), each contributing distinct strengths and facing particular challenges. This systematic review identifies limitations such as high computational demands, instability during training, and insufficient privacy guarantees, all of which currently constrain the practical deployment of synthetic data generation techniques. Addressing these barriers is imperative for wider adoption, which would enhance machine learning and the development of data-driven technologies across multiple domains."
6,"E. Papadaki, A. G. Vrahatis, and S. Kotsiantis, ""Exploring Innovative Approaches to Synthetic Tabular Data Generation,"" Electronics, vol. 13, no. 10, p. 1965, 2024. [Online]. Available: https://www.mdpi.com/2079-9292/13/10/1965","The paper provides a comprehensive review of contemporary data generation methodologies, especially emphasizing statistical and machine learning-based techniques. It highlights novel approaches such as the divide-and-conquer (DC) strategy and advanced models like GANBLR, which address challenges including the preservation of complex data relationships and improving interpretability. Generative adversarial networks (GANs) are also discussed as transformative for synthetic data creation in fields like healthcare, cybersecurity, and retail. The review examines how these methods effectively tackle class imbalance, data scarcity, and privacy concerns, and analyzes evaluation metrics and applications that demonstrate their impact on refining predictive models and decision-making systems. The authors conclude with perspectives on future research and underscore the pivotal role of synthetic data in advancing machine learning and data-driven solutions across various sectors."
7,"D. B. Resnik, M. Hosseini, J. J. H. Kim, G. Epiphaniou, and C. Maple, “GenAI synthetic data create ethical challenges for scientists. Here’s how to address them,” Proceedings of the National Academy of Sciences, vol. 122, no. 9, pp. e2409182122, Mar. 2025. [Online]. Available: https://doi.org/10.1073/pnas.2409182122","Synthetic data generated by generative artificial intelligence (GenAI) models, such as generative adversarial networks or variational auto-encoders, are increasingly used in scientific research for purposes such as filling gaps from missing data, protecting sensitive information, and accelerating experimental processes. However, these advancements present significant ethical challenges, including risks to data integritysince GenAI can generate highly realistic fabricated data indistinguishable from real dataand issues related to privacy, bias amplification, and the potential for research misconduct. The conflation of synthetic and real data threatens the reliability, reproducibility, and ethical standards of science, and may complicate regulatory and policy decisions, as synthetic data can corrupt research records or jeopardize public trust if not properly managed. To address these challenges, the authors propose clear definitions distinguishing synthetic from real data based on provenance, robust disclosure guidelines requiring explanation and identification of synthetic data usage in research, educational initiatives on responsible data use, and technical measures like watermarking, blockchain certifications, and AI-based detection tools. Despite these concerns, the paper concludes that if the scientific community implements appropriate strategies, GenAI-generated synthetic data can yield significant benefits for science and society by enhancing research capabilities while minimizing risks."
8,"D. Adam, “Synthetic data can aid the analysis of clinical outcomes: How much can it be trusted?,” Proceedings of the National Academy of Sciences, vol. 121, no. 32, pp. e2414310121, Aug. 2024. [Online]. Available: https://doi.org/10.1073/pnas.2414310121","This article explores the evolution and promise of synthetic data in clinical research, initiated as a privacy-preserving measure, but now leveraged as a powerful tool for expanding research potential. Synthetic datasets are generated by identifying and mapping key characteristics of real patient data and statistically or algorithmically (e.g., via generative adversarial networks, GANs) creating new artificial data with matching properties, such as age distributions or complex physiological correlations. Such datasets have enabled advances like genotype-disease linkage studies, suicide risk factor analysis, and safety modeling with minimal risk of re-identifying individuals. For example, with a cohort of 2,000 myelodysplastic syndrome patients, a GAN produced synthetic records that captured essential clinical and genetic features without duplicating real individual profiles. While synthetic data can bolster sample sizes, mitigate historical data biases, and fill gaps where real-world data are sparse, challenges remain: the fidelity of artificial data is limited by the quality of its source, and validation against original datasets is still required for clinical acceptance. As machine learning advances, regulatory interest grows in using synthetic patients as control arms in trials, potentially halving the required participants and reducing ethical concerns regarding placebo treatments. Although not yet widely adopted for primary research conclusions, synthetic data now move beyond mere privacy safeguards and stand as a transformative resource awaiting broader realization in biomedical research."
9,"S. Ruggles, “The shortcomings of synthetic census microdata,” Proceedings of the National Academy of Sciences, vol. 122, no. 11, pp. e2424655122, Mar. 2025. [Online]. Available: https://doi.org/10.1073/pnas.2424655122","The U.S. Census Bureau plans to replace the American Community Survey (ACS) public use microdata with fully synthetic data to address confidentiality concerns arising from modern computing power and private sector data proliferation. However, the transition to synthetic microdata jeopardizes the utility of these datasets, which are pivotal for social science research and policy development nationwide. The paper argues that synthetic data are inadequate for most research purposes, and current evidence does not indicate a credible disclosure risk; the only existing study found no substantial threat of reidentification. The author calls for new empirical research to rigorously assess true vulnerabilities in ACS microdata. If such risks are confirmed, targeted risk reduction should be prioritized with minimal impact on data usability, rather than broadly implementing synthetic data, since unwarranted measures could undermine essential scientific and policy tools."
10,"Y. Liu, R. Shen, and X. Shen, ""Novel Uncertainty Quantification Through Perturbation-Assisted Sample Synthesis,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 7813–7824, Dec. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10508110","This paper presents the Perturbation-Assisted Inference (PAI) framework, leveraging the Perturbation-Assisted Sample Synthesis (PASS) method to address uncertainty quantification for complex data, particularly unstructured data handled by deep learning models. PASS utilizes generative models (including diffusion models and normalizing flows) combined with data perturbation to generate synthetic data that closely resembles the original distribution while preserving rank properties, thereby increasing data diversity and privacy. Knowledge is further transferred from large pre-trained generative models, refining statistical estimates through Monte Carlo experiments. PAI provides statistical validity for both pivotal and non-pivotal inference settingsusing either theoretical guarantees or holdout samples for reliable synthetic data generation. The efficacy and reliability of PAI are demonstrated across domains such as image synthesis (e.g., CIFAR-10 with significant FID improvements over DCGAN and GLOW), sentiment word analysis (IMDB, with statistical tests on masked and extracted words), and multimodal text-to-image generation (measuring cosine similarity and FID-pvalues), as well as constructing prediction intervals compared to conformal methods. Theoretical sections discuss the sampling properties and latent variable rank preservation under perturbation, supported by technical proofs. While the framework increases computational demand through extensive Monte Carlo sampling, its validity and adaptability offer robust solutions for uncertainty quantification in black-box deep learning systems, supporting reproducible and credible data-driven inference even with scarce data. PASS further provides integration and privacy via multivariate rank-matching, and future enhancements will focus on computational efficiency, wider applicability, and improved privacy-personalization trade-offs."
11,"V. M. Sánchez-Cartagena, M. Esplà-Gomis, J. A. Pérez-Ortiz, and F. Sánchez-Martínez, ""Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 2, pp. 837–850, Feb. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10321682","When available parallel sentences for training neural machine translation systems are limited, new synthetic samples are typically generated; standard methods assume non-fluent target-side synthetic sentences are harmful. However, this paper shows that even non-fluent synthetic target sentences can boost translation quality if treated as another language within a multilingual machine translation framework. Across experiments involving ten low-resource and four high-resource translation tasks, this approach consistently outperforms state-of-the-art methods, regardless of the original corpus size, and results in systems that are more robust to domain shifts and produce fewer hallucinations."
12,"Y. Lu, L. Chen, Y. Zhang, M. Shen, H. Wang, X. Wang, C. van Rechem, T. Fu, and W. Wei, ""Machine Learning for Synthetic Data Generation: A Review,"" arXiv preprint arXiv:2302.04062, 2023. [Online]. Available: https://arxiv.org/abs/2302.04062","Machine learning applications are often hindered by issues like poor data quality, scarcity, and privacy constraints, making synthetic data generation a critical area of innovation. This paper presents a systematic review of machine learning models used to create synthetic data, discussing applications across computer vision, speech, NLP, healthcare, business, and more. It surveys a spectrum of generative frameworksincluding language models, self-supervised learning, variational autoencoders (VAE), generative adversarial networks (GANs), reinforcement learning, diffusion models, and multimodal learningevaluating their suitability for producing high-fidelity synthetic data. Special attention is paid to privacy and fairness: the review categorizes privacy risks in synthetic data, surveys protection strategies, and discusses challenges unique to foundation models, while also addressing bias mitigation and fairness assessment metrics. Evaluation strategies include human judgment, statistical tests, ML model performance (e.g., TSTRTrain on Synthetic, Test on Real), and application-specific benchmarks. The review outlines key challenges, such as the need for robust evaluation metrics, coverage of diverse data domains, detecting remaining biases, addressing risks in generative models, protecting against threats unique to foundation models, opening tool ecosystems, and differentiating synthetic from real data. Ultimately, the study concludes that synthetic data not only mitigates data shortages and privacy risks, but also enables advancements across industries, although ongoing research is needed to further improve generation quality, fairness, and safe deployment."
13,"E. De Cristofaro, ""Synthetic Data: Methods, Use Cases, and Risks,"" arXiv preprint arXiv:2303.01230, 2024. [Online]. Available: https://arxiv.org/abs/2303.01230","The paper provides a comprehensive overview of synthetic data as a privacy-enhancing technology for sharing sensitive datasets. Traditional approaches such as anonymization and aggregation often fail to prevent re-identification, while differential privacy offers stronger guarantees at the cost of utility. Synthetic data, generated via hand-engineered methods, agent-based models, or state-of-the-art generative machine models (notably GANs and Variational Autoencoders), aims to mimic the statistical properties of real data without directly exposing individuals' information. Key applications include training machine learning models, product/software testing, governance, and regulatory compliance (e.g., GDPR, HIPAA). Despite its promise, synthetic data faces significant privacy risks: linkage and attribute disclosure attacks remain possible, especially when data is not generated using differentially private techniques. State-of-the-art solutions often combine generative models with differential privacy (using mechanisms like Laplace noise, gradient sanitization, or PATE) but encounter a trade-off between privacy and utility. The authors argue that synthetic data is unlikely to universally ""sanitize"" sensitive data due to inherent limitations in privacy protection, especially for outliers or rare attributes, and unpredictable preservation of data signals. Future directions involve auditing differentially private synthetic data algorithms, developing actionable guidelines for safe usage, and further research to explore achievable privacy-utility tradeoffs. The paper concludes that while synthetic data enables new data sharing and analytics opportunities, unresolved privacy and utility challenges necessitate further research and clear guidelines for practitioners and policymakers."
14,"L. Long, R. Wang, R. Xiao, J. Zhao, X. Ding, G. Chen, and H. Wang, ""On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey,"" arXiv preprint arXiv:2406.15126, 2024. [Online]. Available: https://arxiv.org/abs/2406.15126","This survey systematically reviews the burgeoning field of synthetic data generation using Large Language Models (LLMs), motivated by limitations in accessing high-quality real-world data due to cost, scarcity, and privacy concerns. The authors formalize the data generation process as $\mathcal{D}_{gen} \leftarrow \mathcal{M}_p(\mathcal{T}, \mathcal{D}_{sup})$where $\mathcal{M}$ is a pre-trained LLM, $\mathcal{D}_{sup}$ is optional seed/unlabeled data, $p$ denotes the prompt, and $\mathcal{T}$ specifies the generation taskand highlight two central requirements: faithfulness (logical and grammatical coherence without hallucinations) and diversity (mimicking real-world variation). They propose a unified workflow consisting of data generation (with techniques like advanced prompt engineering, conditional prompting, in-context learning, and multi-step sample/dataset decomposition), followed by curation (sample filtering via heuristics, sample re-weighting, label enhancement via human or auxiliary models), and final evaluation (direct metrics for faithfulness and diversity, and indirect benchmarking). The paper notes ongoing challenges, such as balancing faithfulness and diversity, mitigating bias/noise, integrating domain knowledge, and combining human-model collaboration at scale. It maps future research directions including complex task decomposition using agent-style logic, automated domain knowledge use, and hybrid small-large model curation synergies. Applications surveyed span text classification, QA, sentiment analysis, NER, and extend to multimodal tasks. The authors acknowledge their focus on text-based LLM approaches, the need to address ethical concerns, and envision LLM-driven communities generating and refining data for continual self-improvement in AI."
15,"T. Li, L. Biferale, F. Bonaccorso, M. A. Scarpolini, and M. Buzzicotti, “Synthetic Lagrangian turbulence by generative diffusion models,” Nature Machine Intelligence, vol. 6, no. 4, pp. 393–403, Apr. 2024. [Online]. Available: https://doi.org/10.1038/s42256-024-00810-0","This paper introduces a machine learning approach employing state-of-the-art diffusion models (DMs) to generate synthetic single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, a long-standing challenge in the study of Lagrangian turbulence due to vast time scale separations and extreme, non-Gaussian multiscale statistics. The authors solve the 3D incompressible NavierStokes equations numerically and use the derived trajectories to train diffusion modelsboth single- and three-component UNet architectureswhich learn to reverse a Gaussian noise process parameterized by an efficient $\tanh$-based noise schedule. The trained DMs accurately reproduce key turbulence statistics across scales: non-Gaussian velocity and acceleration increments with fat tails (up to 6070 standard deviations), structure functions and flatness up to order $p=8$, cross-component correlations, acceleration correlation functions, and scale-by-scale local scaling exponents $\zeta(p, \tau)$, closely matching those found in direct numerical simulations (DNS) and experiments. Notably, the DMs generalize well to rare, intense events beyond the training set, while outperforming generative adversarial networks (GANs) particularly at small scales. Some underestimation occurs for acceleration correlations below the dissipative time, suggesting avenues for model refinement. The methodology, models, and datasets are openly released, offering a framework to create high-fidelity synthetic Lagrangian datasets unattainable by conventional DNS or experiments, with applications for pretraining and benchmarking in turbulence research and engineering."
16,"F. Fürrutter, G. Muñoz-Gil, and H. J. Briegel, “Quantum circuit synthesis with diffusion models,” Nature Machine Intelligence, vol. 6, no. 5, pp. 515–524, May 2024. [Online]. Available: https://doi.org/10.1038/s42256-024-00831-9","This paper introduces a novel approach to quantum circuit synthesis using denoising diffusion models (DMs), a state-of-the-art generative machine learning method. DMs are leveraged to bypass the exponential overhead of classical simulation by training on corrupted versions of encoded quantum circuits, guided by text or specific task-based conditioning, to generate desired quantum operationssuch as entanglement generation and unitary compilationwithin gate-based quantum circuits. Circuits are represented as three-dimensional tensors encompassing qubit indices, time steps, and gate types, and models are conditioned using pre-trained neural encoders (e.g., CLIP) and, where needed, additional learned representations (such as unitary matrices for compilation). The method demonstrates exceptional flexibility, supporting features like masking (to enforce hardware constraints) and editing (to modify existing circuits), and can be fine-tuned with few samples for tasks where data is scarce. Experimental results show high accuracy in generating circuits that achieve target entanglement (as described by their Schmidt rank vector, SRV) and compiling random unitaries with valid gate decompositions. The model uncovers not only correct, novel, and unique circuit solutions but also adapts to various circuit sizes and constraints. Masking and editing further empower the model to meet hardware limitations and practical applications, with accuracy remaining high even as the complexity of the target increases. Future directions highlighted include interpretability (e.g., using attention maps for circuit insight), scalability to larger circuits and more complex gate types, and better conditionings for large unitaries, making the approach highly extensible to emerging quantum computing platforms and hardware architectures."
17,"I. Igashov, H. Stärk, C. Vignac, A. Schneuing, V. Garcia Satorras, P. Frossard, M. Welling, M. Bronstein, and B. Correia, “Equivariant 3D-conditional diffusion model for molecular linker design,” Nature Machine Intelligence, vol. 6, no. 4, pp. 417–427, Apr. 2024. [Online]. Available: https://doi.org/10.1038/s42256-024-00815-9","Fragment-based drug discovery reduces the vast search space of drug-like molecules by building from small fragments, but effectively connecting these fragments remains a computational challenge, especially in three dimensions and under consideration of target protein pockets. DiffLinker, introduced in this work, is an E(3)-equivariant 3D conditional diffusion model that generates molecular linkers between an arbitrary number of fragments, automatically inferring the number of linker atoms, attachment points, and 3D geometry, and can be conditioned on protein pocket environments. The method represents molecules as 3D point clouds and employs an E(3)-equivariant graph neural network, with a specialized GNN for predicting linker size and a context integration mechanism for protein pockets. Evaluations on benchmarks such as ZINC, CASF, GEOM, and Binding MOAD show that DiffLinker achieves high validity, diversity, and synthetic accessibility, outperforming baselines like DeLinker and 3DLinker, particularly in multi-fragment and protein pocket-conditioned scenarios (e.g., 93% validity, >85% recovery on GEOM multi-fragment tasks, reduced steric clashes in docking). Case studies highlight successful recapitulation of experimentally validated linkers and diverse scaffold generation. Limitations include lower validity due to bond inference post hoc rather than explicit valency handling, and challenges in applications with long linkers like PROTACs owing to distribution mismatch. Potential improvements involve incorporating explicit bond and valency information, guiding generation with synthetic accessibility, and broadening DiffLinker's scope to additional drug design stages such as molecule growing or de novo design. Overall, DiffLinker represents a versatile and state-of-the-art step forward for structure-based drug design leveraging FBDD."
18,"F. A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, ""Diffusion Models in Vision: A Survey,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10850–10869, 2023. [Online]. Available: https://doi.org/10.1109/TPAMI.2023.3261988","Denoising diffusion models are an emerging class of deep generative models that have recently achieved impressive results in computer vision, especially in generating high-quality, diverse samples. These models rely on a two-stage process: a forward diffusion stage that gradually perturbs data with Gaussian noise, and a reverse stage where a neural network learns to reconstruct the original data, step by step. The survey thoroughly reviews developments in three main frameworksdenoising diffusion probabilistic models (DDPMs), noise conditioned score networks (NCSNs), and stochastic differential equations (SDEs)supplying rigorous mathematical formulations connecting these approaches. It maps out applications ranging from unconditional and conditional image synthesis to inpainting, super-resolution, segmentation, video, 3D generation, and medical imaging, presenting an extensive annotated table (not shown here due to brevity) cataloging variants, targets, architectures, and datasets. The models are noted for surpassing GANs in sample quality and diversity, with advantages like stable training and resistance to mode collapse, yet face limitations in computational efficiency due to the need for many denoising steps at inference. Current research priorities include accelerating sampling, improving conditioning schemes (e.g., classifier-guidance or multimodal conditioning), expanding to new tasks and domains, and leveraging the structure of their latent spaces. Notable future directions are more efficient optimization and sampling, utilizing latent space for representation learning ($z$-space manipulation), addressing long-term dependencies in video generation, and building multi-purpose, multi-task diffusion models. The survey argues that despite their computational demands, diffusion models have rich theoretical connections to other generative paradigms (VAEs, GANs, flows, EBMs) and are poised to be central in future advances in generative AI. Mathematical appendices provide detailed derivations, such as the variational lower bound for training objectives."
19,"M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, ""MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 6, pp. 4115–4128, June 2024. [Online]. Available: https://doi.org/10.1109/TPAMI.2024.3355414","Human motion modeling, essential for graphics applications but traditionally demanding professional expertise, can now be approached by laymen using natural language-driven motion generation methods. Yet, such methods often struggle with generating diverse and fine-grained motions. To address this, the authors propose MotionDiffuse, a novel diffusion model-based framework for text-driven motion generation. Unlike traditional deterministic mappings, MotionDiffuse employs probabilistic mapping via denoising steps, resulting in richer motion variations. The model excels at synthesizing realistic and vivid motion sequences, supports multi-level manipulation (e.g., controlling specific body parts), and can generate arbitrary-length motions responsive to time-varying text prompts. Experiments show that MotionDiffuse significantly outperforms state-of-the-art approaches in both text- and action-conditioned motion generation, with qualitative analysis confirming its superior controllability and expressiveness."
20,"J. Chen, H. Chen, K. Chen, Y. Zhang, Z. Zou, and Z. Shi, ""Diffusion Models for Imperceptible and Transferable Adversarial Attack,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 2, pp. 961–977, Feb. 2025. [Online]. Available: https://doi.org/10.1109/TPAMI.2024.3480519","Many existing adversarial attacks generate $L_p$-norm perturbations in the image RGB space, but such perturbations are often easily perceptible by humans. Recent approaches explore unrestricted attacks without $L_p$-norm constraints for improved imperceptibility but struggle with transferability to black-box models. This work introduces DiffAttack, a novel adversarial attack that leverages diffusion models to jointly utilize their generative and discriminative capacities. Instead of perturbing pixel space directly, perturbations are crafted in the latent space of diffusion models and combined with content-preserving structures to create human-insensitive, semantically meaningful adversarial examples. To enhance transferability, the attack further distracts the diffusion modelacting as an implicit recognition surrogatefrom target regions. Extensive experiments across various models, datasets, and defense strategies demonstrate that DiffAttack achieves superior imperceptibility and transferability compared to existing methods, marking the first use of diffusion models in adversarial attack research."
21,"M. U. Akbar, W. Wang, and A. Eklund, ""Beware of diffusion models for synthesizing medical images—a comparison with GANs in terms of memorizing brain MRI and chest x-ray images,"" Machine Learning: Science and Technology, vol. 6, no. 1, p. 015022, 2025. [Online]. Available: https://doi.org/10.1088/2632-2153/ad9a3a","Diffusion models, originally developed for text-to-image generation and now prominent in creating synthetic medical images, tend to memorize and potentially reproduce training data more than StyleGANs, especially when trained on small or highly similar datasets such as BRATS brain MRIs. This study systematically compared StyleGAN and diffusion models using BRATS20, BRATS21, and chest x-ray pneumonia datasets, employing pixel-wise correlation and other similarity metrics to assess memorization. Results indicated that diffusion models produced synthetic images with higher similarity to training data, a problem exacerbated by smaller dataset size and the use of 2D slices from 3D volumes. Notably, models yielding superior traditional image quality metrics (lower FID, higher IS) were also more prone to memorization, raising an important ethical and privacy concern in medical imaging. The number of trainable parameters and choice of dataset influenced memorization rates, with larger diffusion models more likely to memorize x-rays; however, memorization was less severe with more heterogeneous datasets like chest x-rays than with homogeneous ones. The authors stress the necessity for careful evaluation and mitigation of memorization in generative models to avoid privacy breaches, especially in regulated domains like healthcare, and advocate for ongoing development of techniques to balance utility and data protection when sharing or using synthetic medical images."
22,"T. Chakraborty, U. Reddy K S, S. M. Naik, M. Panja, and B. Manvitha, ""Ten years of generative adversarial nets (GANs): a survey of the state-of-the-art,"" Machine Learning: Science and Technology, vol. 5, no. 1, p. 011001, 2024. [Online]. Available: https://doi.org/10.1088/2632-2153/ad1f77","Over the past decade, Generative Adversarial Networks (GANs) have emerged as a powerful class of generative models, revolutionizing the field of artificial intelligence. This survey presents an in-depth analysis of the evolution of GANs, highlighting key advances, challenges, and state-of-the-art techniques that have shaped their development since their inception in 2014. The paper delves into the foundational concepts of GANs, explores notable architectural innovations, and examines the diverse range of applications across domains such as computer vision, natural language processing, and scientific research. Additionally, it addresses the persistent challenges faced by GANs, including mode collapse, training instability, and evaluation metrics, while discussing recent breakthroughs aimed at overcoming these obstacles. By synthesizing the latest research and offering valuable insights, this survey serves as a comprehensive resource for researchers and practitioners seeking to understand the current landscape and future directions of GANs."
23,"J. Rydzewski, J. M. A. Grimme, and M. W. F. Fischer, ""Manifold learning in atomistic simulations: a conceptual review,"" Machine Learning: Science and Technology, vol. 4, no. 3, p. 031001, 2023. [Online]. Available: https://doi.org/10.1088/2632-2153/ace81a","Analyzing large, high-dimensional datasetssuch as those from atomistic simulationsnecessitates dimensionality reduction to uncover informative, low-dimensional manifolds embedded within complex data. This review centers on unsupervised machine learning techniques that extract such manifolds, specifically focusing on methods leveraging Markov transition probabilities between high-dimensional samples, rather than broader manifold learning approaches. These techniques facilitate deeper insight into complex physical processes by enabling meaningful sampling of long-timescale phenomena and estimation of free energies. The review methodically discusses these tools' theoretical frameworks, operational contexts (standard and enhanced sampling simulations), and their conceptual limitations, distinguishing itself from other reviews by its exclusive emphasis on Markov-based manifold constructions."
24,"M. Chen, S. Mei, J. Fan, and M. Wang, ""An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization,"" arXiv preprint arXiv:2404.07771, 2024. [Online]. Available: https://arxiv.org/abs/2404.07771","Diffusion models have emerged as powerful generative AI technologies, excelling in fields such as computer vision, audio synthesis, reinforcement learning, and biological modeling by enabling flexible high-dimensional data generation controlled by guidance techniques. Despite vast empirical advancementsoutperforming models like GANs and VAEs, and supporting conditional sampling for tailored contentthe theoretical foundations have lagged, with recent progress centering on statistical guarantees, score function learning, minimax optimality, and efficient adaptation to conditional tasks. The paper systematically explores mathematical underpinnings (such as stochastic differential equation formulation for forward and backward processes), cutting-edge applications, and theoretical results on sample complexity and guidance tuning, with sample and distribution estimation scaling favourably in low-dimensional or structured data settings. The work showcases conditional diffusion models efficiency in approximating conditional distributions and highlights the reformulation of high-dimensional black-box optimization as a conditional sampling task, providing theoretical guarantees on optimality and fidelity. Challenges persist in theoretical development, efficient sampling, principled guidance design, robustness, and extending diffusion concepts to discrete data. Future research directions include expanding connections to stochastic control and robust optimization, advancing the theory for discrete-state diffusion, and improving guidance and fine-tuning strategies for complex domains. Overall, the paper provides a comprehensive overview integrating empirical impact, theoretical results, methodological challenges, and promising research avenues for diffusion models in generative modeling."
25,"S. Nie, F. Zhu, Z. You, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and C. Li, ""Large Language Diffusion Models,"" arXiv preprint arXiv:2502.09992, 2025. [Online]. Available: https://arxiv.org/abs/2502.09992","This paper introduces LLaDA, a diffusion-based large language model trained from scratch with a pre-training and supervised fine-tuning (SFT) pipeline, challenging the dominance of autoregressive models (ARMs) in large language modeling. LLaDA employs a forward masking and reverse denoising process: each token in input $x_0$ is independently masked with probability $t$, and a mask predictora vanilla Transformer without causal maskingis trained to recover all masked tokens simultaneously, optimizing a likelihood upper bound via cross-entropy over masked positions. Trained on 2.3 trillion tokens and scaled up to 8B parameters, LLaDA demonstrates strong scalability on benchmarks, outperforming self-constructed ARM baselines and surpassing LLaMA2 7B, while matching LLaMA3 8B in in-context learning and general tasks. SFT enhances instruction following, yielding competitive results in general, mathematical, coding, and multilingual benchmarks. Notably, LLaDA excels in reversal reasoning tasks (e.g., poem completion), outperforming GPT-4o and showing minimal performance degradation between forward and reversed tasksan area where ARMs, due to left-to-right bias, struggle. The approach offers unique strengths in bidirectional context computation and robustness for complex reasoning, with the paper providing detailed algorithms, pseudocode, and benchmark protocols in the appendices. Despite limitations such as compute constraints, sensitivity to inference hyperparameters, and the absence of RL-based alignment, LLaDA establishes diffusion models as a robust, performant, and non-autoregressive alternative to ARMs, thereby refuting the notion that core LLM capabilities are inherently tied to autoregressive formulations."
26,"L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang, ""Diffusion Models: A Comprehensive Survey of Methods and Applications,"" arXiv preprint arXiv:2209.00796, accepted by ACM Computing Surveys, 2024. [Online]. Available: https://arxiv.org/abs/2209.00796","Diffusion models have emerged as a state-of-the-art class of deep generative models, surpassing earlier paradigms such as GANs in domains including image synthesis, video and molecule generation, and extending to applications across computer vision, natural language processing, temporal data, and interdisciplinary sciences. This comprehensive survey categorizes advancements in diffusion models into three main areas: (1) efficient sampling, (2) improved likelihood estimation, and (3) handling data with special structures, such as those with permutation or rotational invariance or manifold data. Foundational variants including denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and Score SDEs all operate by iteratively corrupting data with noise and then learning to reverse this process, a principle unifying their mathematical underpinnings. Major application fields and methodological connections to VAEs, GANs, normalizing flows, autoregressive, and energy-based models are also reviewed. The survey highlights key open challenges, such as revisiting assumptions about the diffusion process, improving theoretical understanding (e.g., optimal stopping, parameter selection), and exploring new directions in latent representation learning, model scaling, and integration with large pre-trained and multimodal foundation models. This work provides both a detailed taxonomy of diffusion modeling research and a roadmap for future investigations, underlining the broad impact and rapid development of the field."
27,"L. Fan, Y. Yang, F. Wang, N. Wang, Z. Zhang, Z. Cao, and D. Lin, ""Super Sparse 3D Object Detection,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 12490–12505, Oct. 2023. doi: 10.1109/TPAMI.2023.3286409. Available: https://ieeexplore.ieee.org/document/10153690","As LiDAR perception ranges expand, efficient long-range 3D object detection becomes critical for autonomous driving. Traditional dense feature map approaches scale poorly as computation grows quadratically with range. The proposed fully sparse detector (FSD) builds on sparse voxel encoding and introduces a novel Sparse Instance Recognition (SIR) module, which groups points into instances and performs efficient instance-wise feature extraction, overcoming center feature limitations of previous sparse architectures. To further enhance sparsity and efficiency, FSD++ incorporates temporal information, generating residual points that capture changes between consecutive frames. Coupled with selected prior foreground points, this forms a super sparse input, dramatically reducing redundancy and computation. Comprehensive experiments on the Waymo Open Dataset demonstrate state-of-the-art results, and additional tests on Argoverse 2 (with $200\,\mathrm{m}$ range versus Waymo's $75\,\mathrm{m}$) highlight the method's superiority for long-range detection. The code is publicly available."
28,"J. Ding, N. Xue, Y. Long, G.-S. Xia, Q. Lu, A. Bai, W. Yang, and X. Yao, ""Object Detection in Aerial Images: A Large-Scale Benchmark and Challenges,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 7778–7796, Nov. 2022. doi: 10.1109/TPAMI.2021.3117988. Available: https://ieeexplore.ieee.org/document/9560031","In the past decade, object detection has advanced significantly in natural images but lags behind in aerial images, largely due to challenges like scale and orientation variations inherent in the bird's-eye perspective and the absence of large-scale benchmarks. This paper introduces DOTA, a large-scale dataset for Object deTection in Aerial images, comprising 1,793,658 object instances across 18 categories with oriented bounding box annotations from 11,268 aerial images. Using this comprehensive dataset, the authors establish baselines with 10 state-of-the-art algorithms over 70 configurations, evaluating both speed and accuracy. Additionally, they provide a code library and a website to evaluate algorithms, with DOTA challenges engaging more than 1300 global teams. The authors assert that the expanded DOTA dataset, baselines, and community resources will significantly promote robust algorithm development and reproducible research in aerial image object detection."
29,"Y. Chen, X. Yuan, R. Wu, J. Wang, Q. Wang, L. Zhang, and M.-M. Cheng, ""YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-Time Object Detection,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, to appear. Accepted 2023, published online Feb. 2025. doi: 10.1109/TPAMI.2025.10872821. Available: https://www.computer.org/csdl/journal/tp/5555/01/10872821/24481ZkGRDG","YOLO-MS is a new real-time object detector designed to enhance multi-scale feature representation by innovatively rethinking the roles of multi-branch building blocks and convolutional kernel sizes. The method introduces the MS-Blocka hierarchical multi-branch unit promoting rich inter-branch diversity and improved feature granularityand integrates Global Query Learning (GQL), an attention-inspired mechanism that provides efficient cross-stage guidance to foster better multi-scale learning. Complemented by a Heterogeneous Kernel Size Selection (HKS) protocol, which increases convolutional kernel size deeper in the network ($3 \rightarrow 5 \rightarrow 7 \rightarrow 9$), YOLO-MS robustly expands the effective receptive field. Extensive experiments on MS COCO and various detection benchmarks show YOLO-MS outperforms state-of-the-art detectors including YOLOv8/9, RTMDet, and Gold-YOLO, offering improvements up to +2% AP with comparable or even lower parameter counts. The architecture demonstrates consistently superior results across tasks like general detection, instance segmentation (+2.3% AP), oriented object detection, and challenging conditions (crowded, foggy, underwater), and can serve as a plug-and-play upgrade to existing YOLO models, increasing AP while reducing computational cost. Ablation studies confirm the efficacy of each component, and though the system can introduce slight implementation complexity, its generalizability and efficiency suggest promising avenues for scaling and future research in both vision-centric and broader multi-scale representation domains."
30,"H. Sheng, S. Cai, N. Zhao, B. Deng, Q. Liang, M.-J. Zhao, and J. Ye, ""CT3D++: Improving 3D Object Detection with Keypoint-induced Channel-wise Transformer,"" International Journal of Computer Vision, 2024. Available: https://dblp.org/rec/journals/corr/abs-2406-08152","This paper addresses limitations in flexibility and scalability of current 3D object detectors for point clouds by introducing two novel frameworks: CT3D and CT3D++. CT3D employs raw-point-based embedding, a standard Transformer encoder, and a channel-wise decoder to refine proposals without reliance on hand-crafted voxel features. CT3D++ further enhances detection by fusing geometric and semantic features and incorporating a point-to-key bidirectional encoder that leverages cross-attention between points and proposal keypoints, improving both efficiency and proposal refinement. Extensive experiments on the KITTI and Waymo Open datasets demonstrate state-of-the-art performance, especially in challenging scenarios, and ablation studies validate key architectural choices such as the point-to-key bidirectional encoder (PBC), which outperforms traditional self-attention mechanisms. The frameworks demonstrate flexibility in pairing with various RPNs, efficient feature fusion, and scalability, challenging the necessity of voxel features for high-quality 3D detection and supporting practical deployment in autonomous driving. The authors emphasize the potential for broad industry and academic impact, with code availability to further research."
31,"P. Chen, X. Yu, X. Han, K. Wang, G. Li, L. Xie, Z. Han, and J. Jiao, ""P2Object: Single Point Supervised Object Detection and Instance Segmentation,"" International Journal of Computer Vision, published online May 3, 2025. DOI: 10.1007/s11263-025-02441-3. Available: https://link.springer.com/article/10.1007/s11263-025-02441-3","This paper introduces the P2Object framework, comprising P2BNet and P2MNet, for efficient object detection and segmentation using single-point supervision. P2BNet constructs balanced, instance-level proposal bags by generating anchor-like proposals around annotated points and refines these proposals from coarse-to-fine with a cascade structure, utilizing multiple instance learning (MIL) loss and negative sampling to suppress the background. Recognizing the limitations of discrete proposal sampling, the authors develop P2BNet++ with a spatial self-distillation strategy for continuous anchor regression, and further extend to P2MNet, which leverages dynamic convolution and a boundary self-prediction (BSP) module combining low-level image cues for accurate pixel-wise object perception. Experiments on COCO, VOC, SBD, and Cityscapes show that P2MNet-FR achieves up to 25.9 AP (COCO-17) and 30.3 AP with a Swin-Transformer, with segmentation results approaching fully-supervised baselines. Ablation studies validate the effectiveness of each component. The approach significantly reduces annotation cost (requiring only ~0.9s/object), yet faces challenges in dense scenes, with occasional failures in group prediction and boundary estimation. Future work aims to apply these methods to oriented detection, noisy supervision, integration with foundation models, and few-shot learning. The proposed framework demonstrates state-of-the-art performance across detection and segmentation, narrowing the gap with fully-supervised learning using minimal supervision."
32,"B. Yin, X. Zhang, L. Liu, M.-M. Cheng, Y. Liu, and Q. Hou, ""Camouflaged Object Detection with Adaptive Partition and Background Retrieval,"" International Journal of Computer Vision, published online March 22, 2025. DOI: 10.1007/s11263-025-02406-6. Available: https://link.springer.com/article/10.1007/s11263-025-02406-6","This paper introduces AdaptCOD, a novel framework for camouflaged object detection (COD) that explicitly leverages contextual background information to enhance object localization and segmentation. AdaptCOD decouples COD into three sequential modules: global localization provides a coarse prediction using an encoder-decoder with FPN, local segmentation adaptively expands the foreground region based on a context-sensitive rule $h_e = h \cdot (1 + \gamma \cdot r_t)$ and $w_e = w \cdot (1 + \gamma \cdot r_t)$ (where $r_t$ is the proportion of predicted foreground), and background retrieval employs masked average pooling followed by cosine similarity to refine object boundaries via comparison with background features. Training involves progressive side supervision and joint optimization, with strong empirical gains shown across three major COD benchmarksCAMO, COD10K, and NC4kusing metrics like $S_m$, $wF$, and $\alpha E$. Ablation and efficiency studies confirm that both the adaptive context selection and background retrieval modules contribute significant performance improvements (2.13.2% on key metrics), outperforming 22 state-of-the-art competitors and effectively generalizing to other COD and medical segmentation models. Limitations include sensitivity to the initial localization and the need for further advances in background-foreground similarity and expansion strategies. Overall, AdaptCOD offers a practical and generalizable improvement for detecting camouflaged objects in challenging visual conditions."
33,"Y. Sun, J. Zhang, Y. Liu, X. Li, ""The evolution of object detection methods,"" Pattern Recognition, vol. 148, 2024, 110438. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S003132032400136X","This paper reviews the traditional object detection pipeline and provides a brief overview of the history of deep learning, then focuses on categorizing deep learning-based object detection algorithms, specifically those utilizing convolutional neural networks and transformer-based frameworks, examining the challenges these methods encounter and potential directions for future research."
34,"M. Yang, X. Wang, G. Zhao, ""Single-shot object detection with enriched semantics,"" Pattern Recognition, vol. 106, 2020, 107404. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0031320319303711","The paper introduces Detection with Enriched Semantics (DES), a novel single-shot object detection network that enriches detection features with semantic information via two mechanisms: a weakly-supervised segmentation branch and a global activation module. The segmentation branch leverages weak segmentation ground-truth, requiring no extra annotations, to supply high-level, class-aware features for activating and calibrating lower-level detection maps, thereby addressing the lack of semantic richness in standard SSD-type architectures. Concomitantly, the global activation module learns object class-channel relationships in a self-supervised manner, providing global context and facilitating channel-wise feature learning. Experiments on PASCAL VOC and MS COCO datasets demonstrate the methods efficacy, achieving an mAP of 81.7 on VOC2007 and 32.8 on COCO test-dev at a speed of 31.5 ms per image (Titan Xp). A lower-resolution DES reaches 79.7 mAP at 13.0 ms per image. DES is simple to implement, minimally invasive to existing frameworks, and extensible to more advanced single- or two-stage detectors. Key architectural detailsincluding the segmentation map generation and global activation formulationsare substantiated with ablation studies, and comprehensive results are reported, such as:

\[
\begin{tabular}{l|c|c}
\textbf{Model} & \textbf{VOC2007 mAP} & \textbf{COCO mAP} \\
\hline
DES (VGG16) & 81.7 & 32.8 \\
DES (VGG16, low-res) & 79.7 & - \\
\end{tabular}
\]

The approach provides a unified, efficient, and effective path for integrating semantic segmentation into single-shot detection, enhancing both performance and speed without requiring significant network redesign."
35,"L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietikäinen, ""Deep learning for generic object detection: A survey,"" Pattern Recognition, vol. 102, 2020, 107198. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0031320319303851","Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research."
36,"L. Jiao, F. Zhang, F. Liu, S. Yang, L. Li, Z. Feng, and R. Qu, ""A Survey of Deep Learning-Based Object Detection,"" IEEE Access, vol. 7, pp. 128837–128868, 2019. [Online]. Available: https://ieeexplore.ieee.org/document/8825470","In this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first."
37,"S. Y. Alaba and J. E. Ball, ""Transformer-Based Optimized Multimodal Fusion for 3D Object Detection in Autonomous Driving,"" IEEE Access, vol. 12, pp. 50165–50176, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10493018",This work presents a multimodal fusion model that improves 3D object detection by combining the benefits of LiDAR and camera sensors to address these challenges in autonomous driving.
38,"X. Jiang and Y. Wu, ""Remote Sensing Object Detection Based on Convolution and Swin Transformer,"" IEEE Access, vol. 11, pp. 38643–38656, 2023. [Online]. Available: https://doi.org/10.1109/ACCESS.2023.3267435","Remote sensing object detection faces challenges such as varied target scales, complex backgrounds, and densely packed small objects, making standard object detection algorithms less effective. This paper introduces RAST-YOLO, which integrates a Region Attention (RA) mechanism with a Swin Transformer backbone to enhance feature extraction by expanding feature map interactions and leveraging object background information, thus improving detection in complex scenes. Additionally, a C3D module fuses deep and shallow semantic data to address multi-scale issues and enhance small object detection. Extensive experiments on DIOR and TGRS-HRRSD datasets show that RAST-YOLO achieves state-of-the-art accuracy, with mean average precision (mAP) improvements of 5% and 2.3% over baseline on the respective datasets, and maintains real-time detection speed due to its lightweight design."
39,"N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, ""End-to-End Object Detection with Transformers,"" arXiv preprint arXiv:2005.12872, 2020. [Online]. Available: https://arxiv.org/abs/2005.12872","This paper introduces DETR (DEtection TRansformer), a novel object detection approach that replaces traditional pipelines with a direct set prediction framework, eliminating hand-crafted components like non-maximum suppression and anchor boxes. DETR utilizes a transformer encoder-decoder architecture and employs a set-based global loss enforced by bipartite matching to produce unique predictions from a small, fixed set of learned object queries. On the COCO benchmark, DETR achieves competitive results without specialized libraries, matching or outperforming the Faster R-CNN baseline with metrics such as AP: $0.4847$, AP$_{50}$: $0.5868$, AP$_{75}$: $0.5129$ across various object sizes, and AR$_{100}$: $0.5723$. The model generalizes to related tasks like panoptic segmentation, showing notable improvements over strong baselines. Auxiliary methods involve bounding box conversion, Generalized IoU loss, and standard metric evaluations. Training code and pretrained models are made available by the authors."
40,"A. Wang, H. Chen, L. Liu, K. Chen, Z. Lin, J. Han, and G. Ding, ""YOLOv10: Real-Time End-to-End Object Detection,"" arXiv preprint arXiv:2405.14458, 2024. [Online]. Available: https://arxiv.org/abs/2405.14458","Over recent years, the YOLO family has led real-time object detection due to its strong performance-efficiency balance, yet its reliance on non-maximum suppression (NMS) in post-processing and suboptimally examined architectural design limit true end-to-end deployment and inference latency. YOLOv10 addresses these with two core contributions: (1) a consistent dual assignment strategy allowing NMS-free, end-to-end detection by jointly training with both one-to-many and one-to-one label assignmentsusing a unified matching metric $m(\alpha,\beta) = s \cdot p^\alpha \cdot \text{IoU}(\hat{b}, b)^\beta$ (where $p$ is classification score, $b$ and $\hat{b}$ are bounding boxes, $s$ is spatial prior, and $\alpha,\beta$ harmonize the assignments) to align training and inference supervision; and (2) a holistic, efficiency-accuracy driven model designintroducing a lightweight classification head (depthwise separable convolutions), spatial-channel decoupled downsampling, rank-guided block design (applying compact inverted blocks per measured redundancy), large-kernel depthwise convolutions for small models, and a partial self-attention module partitioned by channel after $1\!\times\!1$ conv for efficient global context. On COCO benchmarks, YOLOv10 achieves state-of-the-art accuracy-latency tradeoffs: e.g., YOLOv10-S outpaces RT-DETR-R18 by 1.8$\times$ in speed with 2.8$\times$ fewer parameters, and YOLOv10-B halves latency and shrinks parameter count by 25% compared to YOLOv9-C at similar AP. Extensive ablations confirm these advances, with performance robust in challenging conditions (e.g., low light, dense scenes). While a residual NMS-free accuracy gap remains for smallest models, it narrows as model capacity grows. The authors highlight possible improvements via large-scale pretraining, further gap reduction, and broader application. YOLOv10 is well-suited to real-time, resource-constrained computer vision deployments, advancing end-to-end object detection efficiency and performance."
41,"X. Wang, R. Girdhar, S. X. Yu, and I. Misra, ""Cut and Learn for Unsupervised Object Detection and Instance Segmentation,"" arXiv preprint arXiv:2301.11320, 2023. [Online]. Available: https://arxiv.org/abs/2301.11320","The paper introduces Cut-and-LEaRn (CutLER), a simple and effective method for unsupervised object detection and segmentation that requires no human labels. CutLER leverages MaskCut, which uses features from a self-supervised Vision Transformer (ViT) to generate coarse masks of multiple objects in an image by creating a patch-wise similarity matrix and applying Normalized Cuts iteratively to isolate different object masks. These masks serve as pseudo-labels to train a detector with a robust loss-dropping strategy that mitigates issues from missed objects. Further improvements are made via self-training on model predictions. CutLER is compatible with various detection architectures, detects multiple objects, and excels as a zero-shot detector, surpassing previous state-of-the-art methods by over 2.7 times in detection performance (AP50 and AR) across 11 diverse benchmarks including video frames, paintings, and sketches. When finetuned with limited labels, CutLER outperforms MoCo-v2 by 7.3% APbox and 6.5% APmask on COCO using only 5% labeled data."
42,"Y. Rong, T. Leemann, T.-t. Nguyen, L. Fiedler, P. Qian, V. V. Unhelkar, T. Seidel, G. Kasneci, and E. Kasneci, “Towards Human-Centered Explainable AI: A Survey of User Studies for Model Explanations,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 4, pp. 2104–2122, Apr. 2024. [Online]. Available: https://dblp.org/rec/journals/pami/RongLNFQUSKK24","Explainable AI (XAI) is increasingly essential for advancing AI research, yet understanding the needs of XAI users and conducting human-centered model evaluations remain challenging. This paper presents a systematic literature review of 97 core papers from the past five years examining user studies in XAI through the lens of human-computer interaction (HCI) and AI research, classifying them by key explanatory qualities: trust, understanding, usability, and human-AI collaboration performance. It finds that XAI proliferates more quickly in some domains, such as recommender systems, while user studies remain infrequent and seldom integrate insights from cognitive or social sciences. The work provides practical guidelines for designing user studies and underscores open research directions, particularly the integration of psychological science into human-centered XAI."
43,"X. Li, H. Ding, H. Yuan, W. Zhang, J. Pang, G. Cheng, K. Chen, Z. Liu, C. C. Loy, “Transformer-Based Visual Segmentation: A Survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 6, pp. 10138–10163, Jun. 2024. [Online]. Available: https://dblp.org/rec/journals/pami/LiDYZPCCLL24","This survey presents a comprehensive and detailed review of transformer-based methods in visual segmentation, outlining the evolution from traditional, hand-crafted feature approaches and CNNs to the current dominance of transformer architectures for tasks such as semantic, instance, panoptic, video, and 3D point cloud segmentation. The survey first contextualizes the segmentation problem, key datasets (e.g., Pascal VOC, COCO, ADE20K, Cityscapes), and standard evaluation metrics including mean Intersection over Union (mIoU), mean Average Precision (mAP), and Panoptic Quality (PQ), providing their formulas and usage scenarios. It then systematizes transformer-based segmentation approaches through a unified DETR-like meta-architecture comprising components such as a feature backbone (CNN/ViT), neck (FPN), object query, transformer decoder, mask representation, and bipartite matching, organizing the design space into detailed categories: strong representations, improved decoder cross-attention designs, optimized object queries, association across frames and tasks, and conditional query fusion (e.g., for open-vocabulary or few-shot segmentation). Numerous tables benchmark over 120 models (including Mask2Former, OneFormer, SegFormer, K-Net, Mask DINO, SegNext, etc.) across all major segmentation tasks and popular datasets, unifying results for fair comparison and highlighting top-performing architectures. The discussion evaluates the strengths of transformers in unifying pipelines, improving generalization, and outperforming CNNs, and identifies open challenges such as creating truly universal architectures, multi-modal and language-driven segmentation, scalability to long video segments, robust domain adaptation, generative approaches, and annotation/model efficiency. The survey concludes by emphasizing the rapid progress of vision transformers in segmentation, summarizing future directions like lifelong learning, generative segmentation, reasoning-augmented architectures, and extension to 4D data and mobile platforms, and supplements its findings with extensive appendices detailing metrics, ablations, and implementation nuances."
44,"C. Zhu and L. Chen, “A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 8954–8975, Dec. 2024. [Online]. Available: https://dblp.org/rec/journals/pami/ZhuC24","As foundational tasks in scene understanding, object detection and segmentation have advanced considerably with deep learning, yet they are constrained by limited, pre-defined annotated categories stemming from high manual labeling costs. The paper surveys developments in Open-Vocabulary Detection (OVD) and Segmentation (OVS), where models can classify objects beyond fixed vocabularies. It introduces a taxonomy organizing tasks and methods based on levels and types of weak supervision, covering techniques such as visual-semantic space mapping, novel visual feature synthesis, region-aware training, pseudo-labeling, knowledge distillation, and transfer learning. This taxonomy is broadly applicable across detection, semantic/instance/panoptic segmentation, and extends to 3D and video tasks. Strengths, weaknesses, main design principles, and challenges for each method are systematically reviewed, with task benchmarks and vital components detailed in supplementary materials and a maintained online resource. The survey concludes by highlighting promising research directions for future exploration in OVD and OVS."
45,"Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li, ""HiLM-D: Enhancing MLLMs with Multi-scale High-Resolution Details for Autonomous Driving,"" International Journal of Computer Vision, published online 7 May 2025. DOI: https://doi.org/10.1007/s11263-025-02433-3 [Full Text URL](https://link.springer.com/article/10.1007/s11263-025-02433-3)","The paper introduces HiLM-D, a novel, resource-efficient, two-stream multimodal large language model (MLLM) framework tailored for interpretable risk object localization, intention, and suggestion prediction (ROLISP) in autonomous driving. Unlike prior approaches focusing mainly on planning, HiLM-D enhances visual perception by combining a temporal reasoning stream (processing video with spatial-temporal adapters on CLIP-ViT) and a lightweight spatial perception stream (processing a single high-resolution image with a custom HRS encoder and P-Adapter for multi-scale features). This fused representation is fed to a language model (e.g., Vicuna) which provides descriptions, intentions, and planning suggestions, while a query-aware detector aligns bounding box prediction with textual reasoning via cross-attention. Experiments on the DRAMA-ROLISP benchmark and Shikra-RD dataset show HiLM-D outperforms state-of-the-art MLLMs, improving BLEU-4 captioning scores by 3.7% and detection mIoU by 8.7% with only 1.36M trainable parameters. Ablations confirm the importance of each component for detecting small objects, maintaining resolution context, and achieving memory efficiency. The approach generalizes well across datasets, and plug-and-play spatial stream integration boosts different MLLMs. Limitations include dataset diversity and closed-loop planning, inspiring future work in constructing richer datasets and real-time systems. Code and data supporting HiLM-D are publicly available."
46,"J. Wu, W. Yao, S. Jia, T. Jiang, W. Zhou, C. Ma, X. Chen, ""Gradient-based sparse voxel attacks on point cloud object detection,"" Pattern Recognition, vol. 160, Art. no. 111156, 2025. [Online]. Available: https://doi.org/10.1016/j.patcog.2024.111156","The paper introduces a gradient-based sparse voxel attack (GSVA) algorithm, specifically targeting voxel-based 3D point cloud object detectors to overcome the limitations of existing adversarial attacks in this domain. Traditional methods often generate fake obstacles or remove objects but face constraints in efficacy and stealthiness, particularly with voxel-based detectors. The proposed method leverages voxel density clusters (VDCs) within point clouds to enhance feature extraction and employs gradient-based optimization in a sparse voxel context, resulting in more effective and efficient adversarial attacks."
47,"J. Tao, S. Chan, Z. Shi, C. Bai, S. Chen, ""FocTrack: Focus attention for visual tracking,"" Pattern Recognition, vol. 160, Art. no. 111128, 2025. [Online]. Available: https://doi.org/10.1016/j.patcog.2023.111128","Transformer trackers have achieved significant success due to their attention mechanisms, which typically model long-range dependencies for global context. However, this differs from human visual tracking behavior, where attention first skims apparent regions and then discriminates between similar areas. Addressing this, the proposed FocTrack integrates a focus attention module using an iterative binary clustering function (IBCF) before self-attention to mimic these human behaviors; each cluster treats others as easily distinguished tokens during clustering, while subsequent self-attention emphasizes discriminative learning on the cluster of interest. Additionally, a local template update strategy (LTUS) selectively updates local templates during tracking, maintaining reliability and low computational cost. Experimental results demonstrate that FocTrack achieves state-of-the-art performance with 71.5% AUC on LaSOT, 84.7% AUC on TrackingNet, and operates at approximately 36 FPS, surpassing prevalent approaches."
48,"J. Yang, B. Hu, H. Li, Y. Liu, X. Gao, J. Han, F. Chen, X. Wu, ""Dynamic VAEs via semantic-aligned matching for continual zero-shot learning,"" Pattern Recognition, vol. 160, Art. no. 111199, 2025. [Online]. Available: https://doi.org/10.1016/j.patcog.2023.111199","Continual Zero-shot Learning (CZSL) aims to classify unseen categories across sequential tasks but faces the challenge of catastrophic forgetting, often mitigated by experience replay of past dataa solution that has limitations including potential data leakage and scenario specificity. Many CZSL approaches also inadequately represent semantic-visual correlations. This work introduces dynamic Variational Autoencoders (VAEs) with semantic-aligned matching, using both semantic and visual VAEs to improve backward knowledge transfer and deploying generative experience replay to alleviate forgetting. Evaluated on five datasets (aPY, AWA1, AWA2, CUB, SUN), the model demonstrates superior performance over baseline methods."
49,"M. R. Islam, S. H. Myeong, M. S. Morshed, D. H. Kim, J. A. Lee, and J. H. Kim, ""Deep Learning and Computer Vision Techniques for Enhanced Quality Control in Manufacturing Processes,"" IEEE Access, vol. 12, pp. 121449-121479, 2024. doi: 10.1109/ACCESS.2024.3332226. Available: https://ieeexplore.ieee.org/document/10663422/","The document reviews advancements in Deep Learning (DL) and Computer Vision (CV) technologies for automated defect detection in manufacturing processes, highlighting their potential to enhance quality control. It discusses the limitations of traditional quality control methods and presents a systematic analysis of state-of-the-art techniques, applications, and challenges in various manufacturing sectors, including automotive and electronics. The study aims to identify research gaps and suggest future directions for leveraging DL and CV in improving manufacturing quality and efficiency."
50,"I. Ahmed, T. Xie, A. K. Bashir, and A. D. Jurcut, ""Computer Vision Based Transfer Learning-Aided Transformer Model for Plant Disease Recognition,"" IEEE Access, vol. 12, pp. 28798-28809, 2024. doi: 10.1109/ACCESS.2024.3363345. Available: http://ieeexplore.ieee.org/document/10440637/","This study presents a new framework for identifying and forecasting fall risks, utilizing a novel transformer model trained on 2D poses."
51,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever, ""Learning Transferable Visual Models From Natural Language Supervision,"" arXiv preprint arXiv:2103.00020, 2021. [Online]. Available: https://arxiv.org/abs/2103.00020","State-of-the-art computer vision systems are typically limited by their reliance on fixed categories and the need for additional labeled data to recognize new visual concepts; this paper addresses these challenges by pre-training models to predict which caption matches which image using 400 million (image, text) pairs from the internet. This simple pre-training approach, leveraging raw text as supervision, enables the model to achieve competitive performance across over 30 diverse vision benchmarksincluding OCR, action recognition, geo-localization, and fine-grained classificationwithout any dataset-specific training. Notably, it matches the zero-shot ImageNet accuracy of supervised ResNet-50 while bypassing the need for its 1.28 million labeled examples, demonstrating the power of scalable natural language supervision for visual representation learning. The authors acknowledge that while benchmark performance can still lag behind alternative methods, their approach unlocks significant potential for flexible and reusable computer vision models, and they provide open access to their pre-trained models and code at https://github.com/OpenAI/CLIP."
52,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo, ""Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows,"" arXiv preprint arXiv:2103.14030, 2021. [Online]. Available: https://arxiv.org/abs/2103.14030","This paper introduces Swin Transformers, a new vision transformer architecture designed as a general-purpose backbone for computer vision tasks. Unlike traditional transformers with global self-attention, which incurs quadratic computational complexity, Swin Transformers employ a shifted window mechanism: self-attention is computed locally within non-overlapping windows, and window locations are shifted between consecutive layers. This enables both computational efficiencywith complexity reduced from quadratic to linear in image sizeand effective modeling of cross-window dependencies. The architecture features a hierarchical design, starting with patch partitioning and linear embedding, and utilizes patch merging layers to produce multi-scale feature maps, facilitating integration with dense prediction frameworks like Feature Pyramid Networks and U-Net. Swin Transformers outperform previous state-of-the-art methods in image classification (ImageNet-1K), object detection (COCO), and semantic segmentation (ADE20K). Architectural variants such as Swin-T, Swin-S, Swin-B, and Swin-L are described. Additionally, incorporating a relative positional bias in self-attention is shown to enhance modeling, and the approach is generally beneficial for MLP-based architectures, reinforcing Swin's role as a competitive backbone for vision similar to transformers for NLP and CNNs for vision."
53,"Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick, ""Mask R-CNN,"" arXiv preprint arXiv:1703.06870, 2017. [Online]. Available: https://arxiv.org/abs/1703.06870","Mask R-CNN is a simple and flexible framework for object instance segmentation that extends Faster R-CNN by adding a branch for predicting segmentation masks in parallel with bounding box recognition for each Region of Interest (RoI). A key innovation is the RoIAlign layer, which addresses misalignments to achieve pixel-accurate mask predictions, significantly improving accuracy (with relative gains of 1050%) over previous quantized schemes like RoIPool. The method predicts a binary mask per class using a per-pixel sigmoid and decouples mask from class prediction, enhancing both flexibility and performance. Mask R-CNN is efficient (about 200 ms per frame, 5 fps), modular, and easy to train, showing state-of-the-art results on COCO in instance segmentation, bounding box detection, and person keypoint estimation, surpassing previous COCO 2016 winners without excessive engineering. The framework is readily extensible to new tasks, such as human pose estimation, and robust to architectural variations, yielding consistent improvements from multi-task learning and advanced training regimes with deeper networks and data augmentation. Mask R-CNNs contributionsparticularly RoIAlign and multitask modularityestablish it as a strong baseline that is fast, accurate, and highly extensible, with open-source code available to advance future research in instance-level recognition."
54,"Y. Wang, G. Huang, S. Song, X. Pan, Y. Xia, and C. Wu, ""Regularizing Deep Networks With Semantic Data Augmentation,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3733-3748, Jul. 2022. DOI: https://doi.org/10.1109/TPAMI.2021.3056495 [Direct URL](https://ieeexplore.ieee.org/document/9332260)",Data augmentation is widely known as a simple yet surprisingly effective technique for regularizing deep networks.
55,"C.-H. Lin, C. Kaushik, E. L. Dyer, and V. Muthukumar, ""The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective,"" Journal of Machine Learning Research, vol. 25, no. 91, pp. 1–85, 2024. [Online]. Available: https://jmlr.org/papers/volume25/22-1312/22-1312.pdf","Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting the entire spectrum of the data covariance matrix through ridge regression. These effects, when applied to popular augmentations, give rise to a wide variety of phenomena, including discrepancies in generalization between over-parameterized and under-parameterized regimes and differences between regression and classification tasks. Our framework highlights the nuanced and sometimes surprising impacts of DA on generalization, and serves as a testbed for novel augmentation design."
56,"S. Chen, E. Dobriban, and J. H. Lee, ""A Group-Theoretic Framework for Data Augmentation,"" Journal of Machine Learning Research, vol. 21, no. 245, pp. 1–71, 2020. [Online]. Available: https://www.jmlr.org/papers/volume21/20-163/20-163.pdf","Data augmentation is a common technique in training deep neural networks, where transformed versions of original data are added to the training set, yet its theoretical underpinnings have remained unclear. This paper introduces a mathematical framework that models data augmentation as an averaging process over the orbits of a group that approximately preserves the data distribution, thereby leading to variance reduction. The framework is applied to empirical risk minimization and illustrated through examples including exponential families, linear regression, and certain two-layer neural networks. Additionally, the work explores the relevance of data augmentation for problems with inherent symmetry, such as those found in cryo-electron microscopy (cryo-EM)."
57,"L. L. Duan, J. E. Johndrow, and D. B. Dunson, ""Scaling up Data Augmentation MCMC via Calibration,"" Journal of Machine Learning Research, vol. 19, no. 64, pp. 1–34, 2018. [Online]. Available: https://jmlr.org/papers/volume19/17-573/17-573.pdf","There is significant interest in making Bayesian inference more scalable, particularly by reducing both the computation time per iteration and, less commonly, the number of required iterations in Markov chain Monte Carlo (MCMC). This work addresses inefficiencies in data augmentation MCMC (DA-MCMC), a popular method whose samples become highly autocorrelated in large datasets due to conditional posterior distributions being overly concentrateda miscalibration problem that increases the required MCMC path length for low Monte Carlo error. The authors propose a family of calibrated data augmentation algorithms that adjust the variance of conditional posteriors and use a Metropolis-Hastings step to correct bias in the stationary distribution. Their approach is straightforward, broadly applicable, and shownacross probit, logistic, and Poisson log-linear modelsto greatly reduce autocorrelation and Monte Carlo error, thereby increasing the effective sample size per unit of computing time and yielding substantial computational efficiency gains."
58,"H. Yang, J. Li, K. Z. Lim, C. Pan, T. V. Truong, Q. Wang, K. Li, S. Li, X. Xiao, M. Ding, T. Chen, X. Liu, Q. Xie, P. Valdivia y. Alvarado, X. Wang, and P.-Y. Chen, “Automatic strain sensor design via active learning and data augmentation for soft machines,” Nature Machine Intelligence, vol. 4, no. 1, pp. 84–94, Jan. 2022. [Online]. Available: https://www.nature.com/articles/s42256-021-00434-8","Emerging soft machines demand advanced strain sensors for effective closed-loop feedback control, and this work presents a three-stage machine learning framework to automate high-accuracy strain sensor design. Initially, a support-vector machine classifier was trained using a dataset of 351 nanomaterial compositions. Subsequently, 125 strain sensors were fabricated across 12 active learning loops to enrich the dataset. To address dataset limitations, data augmentation generated over 10,000 synthetic datapoints, and a genetic algorithm was used to optimize model prediction accuracy. The study established several data-driven design rules for piezoresistive nanocomposites, confirmed through in situ microscopic analysis. Ultimately, model-driven sensor designs were integrated into various soft machines, enabling real-time strain-sensing functionality."
59,"D. Cao, G. Chen, J. Jiang, J. Yu, R. Zhang, M. Chen, W. Zhang, L. Chen, F. Zhong, Y. Zhang, C. Lu, X. Li, X. Luo, S. Zhang, and M. Zheng, “Generic protein–ligand interaction scoring by integrating physical prior knowledge and data augmentation modelling,” Nature Machine Intelligence, vol. 6, no. 6, pp. 688–700, Jun. 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00849-z","Developing robust methods for evaluating protein-ligand interactions has been a long-standing problem. Here, the authors propose a novel approach called EquiScore, which uses an equivariant heterogeneous graph neural network to integrate physical prior knowledge and characterize protein-ligand interactions within an equivariant geometric space. To enhance generalization, they created a dataset named PDBscreen and designed multiple data augmentation strategies appropriate for training scoring methods. They also examined potential data leakage risks in commonly used data-driven modeling workflows and introduced a more rigorous redundancy removal scheme to address this. On two large external test sets, EquiScore outperformed 21 existing methods across various screening metrics and maintained its performance regardless of binding pose generation approaches. EquiScore further demonstrated strong performance in the activity ranking of structural analogs, suggesting its utility for guiding lead compound optimization. Lastly, they explored several levels of interpretability in EquiScore, potentially offering greater insights for structure-based drug design."
60,"P.-D. Tudosiu, W. H. L. Pinaya, P. Ferreira Da Costa, J. Dafflon, A. Patel, P. Borges, V. Fernandez, M. S. Graham, R. J. Gray, P. Nachev, S. Ourselin, and M. J. Cardoso, “Realistic morphology-preserving generative modelling of the brain,” Nature Machine Intelligence, vol. 6, no. 7, pp. 811–819, Jul. 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00864-0","Medical imaging research faces challenges related to data scarcity, privacy, and limited phenotype coverage, all of which restrict the progress of deep learning in healthcare. This work presents a novel three-dimensional morphology-preserving generative model of the human brain, using a VQ-VAE and transformer-based pipeline trained on large-scale MRI datasets (UK Biobank, ADNI). The approach encodes 3D images into discrete latent spaces using a learned codebook and leverages a transformer autoregressor, conditioned on patient variables, to generate realistic high-resolution brain images while preserving critical biological and disease phenotypes. Quantitative analysis shows that the proposed model achieves state-of-the-art performance, surpassing baselines by up to two orders of magnitude in FID and MMD, and produces images sharply matching real data distributions. Morphological validity is confirmed through voxel-based morphometry, subcortical analyses, and cortical thickness estimation, demonstrating near-perfect alignment with real datasets. Furthermore, models trained on synthetic data produced by the pipeline generalize effectively to downstream tasks such as age regression. The model directly tackles data scarcity, supports fairness in AI by allowing balanced generation across subpopulations, and offers promising generalization to other organs and imaging modalities. Resources and code are made openly available, establishing a foundation for future work in principled medical data synthesis and evaluation."
61,"C. Shorten and T. M. Khoshgoftaar, ""A survey on Image Data Augmentation for Deep Learning,"" Journal of Big Data, vol. 6, no. 1, article no. 60, pp. 1–48, 2019. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0","Deep convolutional neural networks excel across many computer vision tasks but suffer from overfitting in limited-data domains such as medical imaging. This survey reviews a comprehensive suite of Data Augmentation techniquesgeometric transformations (flipping, rotation, cropping), color space manipulation (channel isolation, histogram alteration), kernel filtering (blur, sharpen), mixing (Mixup, SamplePairing), random erasing, feature space augmentation, adversarial training, GAN-based methods (e.g., DCGAN, CycleGAN, conditional and VAE-GANs), neural style transfer, and meta-learning approaches (Neural augmentation, AutoAugment)used to artificially enlarge datasets and improve deep learning model generalization. Reviewed studies demonstrate substantial error rate reductions with augmentation (e.g., SamplePairing lowering CIFAR-10 error from $8.22\%$ to $6.93\%$, random erasing from $5.17\%$ to $4.31\%$), and that augmentations can be additive; combinations often yield better results than single methods as seen throughout the literature. However, there remain challenges: some augmentations risk mislabeling or class imbalance, GAN outputs may lack diversity or quality, and generalizable augmentation policies are hard to design. The discussion highlights the distinction between easy-to-interpret methods and those that are empirically powerful but less explainable. Future work should focus on developing taxonomies, improving GAN outputs, meta-learning augmentation discovery, and extending augmentation techniques to other modalities, all while carefully handling class imbalance and dataset bias, as no augmentation can introduce classes entirely absent in the original data. Overall, Data Augmentation effectively helps deep models emulate the benefits of big data by mitigating overfitting, with the greatest opportunities still emerging in combining and automating augmentation policy search (e.g., AutoAugment), exploring intermediate network layers for augmentation, and balancing efficiency with diversity in synthesized samples."
62,"C. Shorten, T. M. Khoshgoftaar, and B. Furht, ""Text Data Augmentation for Deep Learning,"" Journal of Big Data, vol. 8, article no. 101, 2021. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00492-0","Natural Language Processing (NLP) in the deep learning era benefits substantially from Data Augmentation, a set of strategies aimed at enhancing generalization and robustness by artificially expanding training data. This survey organizes the main themes of text data augmentation, namely strengthening decision boundaries, brute force training, causal and counterfactual reasoning, and distinguishing meaning from form. It systematically reviews symbolic (rule-based, graph-structured, feature-space, MixUp), neural (including back-translation, style transfer, and recent generative models like C-BERT, GPT3Mix, and retrieval-augmented methods), and label-based augmentations (e.g., knowledge distillation, label smoothing). Special attention is given to how augments help evaluate generalizationcovering adversarial test creation, CheckList, and leveraging LLMs for counterfactualsand to contrasting augmentation practices in text and vision domains. The survey presents practical components such as consistency regularization, supervised and self-supervised contrastive learning, and implementation challenges related to semantic invariance, stacking, domain transfer, hardware choice, and curriculum learning. While NLP lags behind Computer Vision in the maturity of augmentation pipelines due to the discrete and semantic sensitivity of text, the paper argues for the promise of using new generative text models, hybrid retrieval-augmented paradigms, and transfer/multitask schemes for future advancement. Concluding, the authors suggest that effective data augmentation acts as a way to encode domain priors and improve resilience to distribution shiftsultimately advocating for further research on augmenting diversified tasks, leveraging vision-derived concepts for language, and integrating self-supervision, with broad opportunities toward robust, generalizable NLP systems."
63,"C. J. Ejiyi, D. Cai, F. O. Eze, M. B. Ejiyi, J. E. Idoko, S. K. Asere, and T. U. Ejiyi, ""Polynomial-SHAP as a SMOTE alternative in conglomerate neural networks for realistic data augmentation in cardiovascular and breast cancer diagnosis,"" Journal of Big Data, vol. 12, article no. 97, 2025. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-025-01152-3","This paper presents two augmentation-free neural network models, Double Conglomerate (D-CongNet) and Triple Conglomerate (T-CongNet), for the early diagnosis of cardiovascular disease (CVD) and breast cancer (BC) using transparent and interpretable machine learning. Unlike approaches reliant on synthetic oversampling (e.g., SMOTE), these models employ polynomial feature transformation and SHAP (Shapley Additive Explanations) for robust feature analysis, with D-CongNet leveraging a CNN+ANN architecture and T-CongNet further incorporating LSTM to capture temporal dependencies. Evaluated on UCI CVD and WDBC BC datasets, T-CongNet achieved an accuracy of 86.96\% for CVD and 97.37\% for BC, with high sensitivity and specificity; D-CongNet delivered comparable results. Key predictive features identifiedsuch as MaxHR, ST_Slope for CVD and concave points_mean, area_worst for BCcorrespond to clinical knowledge. The results show these models not only match state-of-the-art accuracy but deliver interpretable, reliable predictions without introducing augmentation-induced artifacts. Limitations include dataset dependence and challenges with rare subtypes and domain transferability. Planned future work focuses on increasing dataset diversity, enhancing generalizability via domain adaptation, and integrating multimodal data, positioning these models as explainable AI tools for improved clinical decision-making."
64,"Z. Wang, P. Wang, K. Liu, P. Wang, Y. Fu, C.-T. Lu, C. C. Aggarwal, J. Pei, and Y. Zhou, ""A Comprehensive Survey on Data Augmentation,"" arXiv preprint arXiv:2405.09591 [cs.LG], May 2024. [Online]. Available: https://arxiv.org/abs/2405.09591","Data augmentation, a collection of techniques for generating artificial data by manipulating existing datasets, addresses challenges of data scarcity and imbalance and enhances AI models' generalization across various domains. Unlike previous surveys that focus narrowly on specific modalities or operation-centric categorizations, this work introduces a unified, modality-independent taxonomy that spans five prevalent data modalities: image, text, graph, tabular, and time-series. The taxonomy operates on two tiers: Tier 1 categorizes augmentation methods by sample granularitysingle-instance (transforming one sample), multi-instance (combining multiple samples), and dataset-level (generative approaches, such as GANs or LLMs); Tier 2 focuses on what information is leveragedvalue-based transformations (e.g., pixel erasing, token substitution), structure-based transformations (e.g., shape morphing, node/edge dropping), and their combinations. Multi-instance methods include classical algorithms like Mixup and SMOTE, as well as newer approaches employing neural mixing and prompt-based generative modeling. Evaluation of augmentation effectiveness centers on downstream model performance, with affinity (semantic consistency) and diversity (pattern variety) as critical metricsmeasured using strategies and formulas tailored to each modality. The survey highlights a technical evolution from handcrafted augmentations to advanced automated methods (e.g., AutoAugment, GANs, diffusion models) and discusses their application beyond basic dataset expansion, such as in self-supervised or contrastive learning. Despite ongoing progress, notable challenges remain, such as balancing semantic fidelity and diversity, scaling generative methods, and standardizing augmentation evaluation. Future directions include enhancing cross-modal generalization, integrating human-in-the-loop data synthesis, improving the practicality of generative models, and systematically benchmarking evaluation metrics. This work thus offers a comprehensive and systematic framework for understanding and advancing data augmentation across the AI landscape."
65,"B. Li, Y. Hou, and W. Che, ""Data Augmentation Approaches in Natural Language Processing: A Survey,"" arXiv preprint arXiv:2110.01852 [cs.CL], Jun. 2022. [Online]. Available: https://arxiv.org/abs/2110.01852","As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges. Some helpful resources are provided in the appendix."
66,"C. Hou, J. Zhang, and T. Zhou, ""When to Learn What: Model-Adaptive Data Augmentation Curriculum,"" in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), arXiv preprint arXiv:2309.04747 [cs.CV], Sep. 2023. [Online]. Available: https://arxiv.org/abs/2309.04747","Data augmentation (DA) is widely used to improve the generalization of neural networks by enforcing the invariances and symmetries to pre-defined transformations applied to input data. However, a fixed augmentation policy may have different effects on each sample in different training stages but existing approaches cannot adjust the policy to be adaptive to each sample and the training model. In this paper, we propose ""Model-Adaptive Data Augmentation (MADAug)"" that jointly trains an augmentation policy network to teach the model ""when to learn what"". Unlike previous work, MADAug selects augmentation operators for each input image by a model-adaptive policy varying between training stages, producing a data augmentation curriculum optimized for better generalization. In MADAug, we train the policy through a bi-level optimization scheme, which aims to minimize a validation set loss of a model trained using the policy-produced data augmentations. We conduct an extensive evaluation of MADAug on multiple image classification tasks and network architectures with thorough comparisons to existing DA approaches. MADAug outperforms or is on par with other baselines and exhibits better fairness: it brings improvement to all classes and more to the difficult ones. Moreover, MADAug learned policy shows better performance when transferred to fine-grained datasets. In addition, the auto-optimized policy in MADAug gradually introduces increasing perturbations and naturally forms an easy-to-hard curriculum."
67,"L. Lin, H. Mu, Z. Zhai, M. Wang, Y. Wang, R. Wang, J. Gao, Y. Zhang, W. Che, T. Baldwin, X. Han, and H. Li, “Against The Achilles’ Heel: A Survey on Red Teaming for Generative Models,” Journal of Artificial Intelligence Research, vol. 82, pp. 1-84, 2025. [Online]. Available: https://www.jair.org/index.php/jair/article/view/17654","Generative models are increasingly utilized in daily applications, leading to concerns about their safety as vulnerabilities are discovered. This survey, encompassing over 120 papers, offers a comprehensive taxonomy of attack strategies tailored to language models' capabilities and introduces the searcher framework to unify diverse automated red teaming techniques. The survey also explores emerging areas such as multimodal attacks and defenses, risks linked to LLM-based agents, instances of excessive filtering of harmless queries, and the challenge of balancing harmlessness with helpfulness in generative models. The work thus addresses the evolving landscape of red teaming for generative AI, with attention to both technical advances and societal implications."
68,"A. Karev and D. Xu, “ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models,” Journal of Artificial Intelligence Research, vol. 82, pp. 1-32, 2025. [Online]. Available: https://www.jair.org/index.php/jair/article/view/17028","Large Language Models (LLMs) have seen rapid development, making comparative evaluation a significant challenge. This paper introduces the Consistency-focused Similarity Comparison Framework (ConSCompF) designed to compare generative LLMs by evaluating the similarity between their text outputs and assigning a similarity score. Notably, ConSCompF works with a small set of unlabeled prompts and requires no proprietary information from LLM developers. Experimental results demonstrate its ability to discern similarities between various LLMs and show that its similarity scores correlate with differences measured by established benchmarks such as ROUGE-L. Further, ConSCompF's utility is validated in few-shot scenarios and enables analysis of LLM similarity matrices that can be visualized using principal component analysis (PCA). The framework offers practical benefits for uncovering information about LLM training data and potentially identifying fraudulent investment activities linked to LLM development."
69,"G. I. Winata, H. Zhao, A. Das, W. Tang, D. D. Yao, S.-X. Zhang, and S. Sahu, “Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey,” Journal of Artificial Intelligence Research, vol. 82, pp. 1-55, 2025. [Online]. Available: https://www.jair.org/index.php/jair/article/view/17541","Preference tuning is an essential process for aligning deep generative models with human preferences, particularly in modalities such as language, speech, and vision. This survey comprehensively reviews recent progress in preference tuning and the use of human feedback. The paper is structured into three major sections: an introduction to relevant reinforcement learning frameworks, tasks, models, and datasets across modalities; an in-depth analysis of preference tuning methods; and a discussion of applications, evaluation methodologies, and future research directions. The survey aims to present state-of-the-art methodologies for preference tuning and model alignment, enhance researchers' and practitioners' understanding, and foster continued innovation in this field. Further resources, including a GitHub repository, are provided for deeper exploration."
70,"S. Perdikis, R. Leeb, R. Chavarriaga, and J. d. R. Millán, ""Context-Aware Learning for Generative Models,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 8, pp. 3471-3483, Aug. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9163155","This paper investigates algorithms for learning with side-information by augmenting generative models with context-related variables, focusing on finite mixture models (FMM) as a canonical Bayesian network. It demonstrates that maximum-likelihood estimation (MLE) of parameters via expectation-maximization (EM) in this extended framework yields better results than standard unsupervised learning and approaches supervised learning performance, even without explicit ground truth labels. Applying the missing information principle (MIP), the study proves these context-augmented algorithms achieve performance between supervised and unsupervised MLE, proportional to the informativeness of the contextual data. The approach confers advantages such as higher estimation precision, reduced standard errors, faster convergence, and improved classification or regression accuracy across multiple scenarios. Applicability is established using Gaussian Mixture Model-based unsupervised classification in three real-world situations, and extended to deep learning by deriving a context-aware algorithm for variational autoencoders (VAs), with further contrast to a neural-symbolic method leveraging side-information."
71,"C. Du, J. Zhu, and B. Zhang, ""Learning Deep Generative Models With Doubly Stochastic MCMC,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 6, pp. 1371-1383, June 2017. [Online]. Available: https://ieeexplore.ieee.org/document/7961239","We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of the log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learned jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation, and missing data imputation. Our method outperforms many state-of-the-art competitors."
72,"M. Dedeoglu, S. Lin, Z. Zhang, and J. Zhang, ""Continual Learning of Generative Models With Limited Data: From Wasserstein-1 Barycenter to Adaptive Coalescence,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 35, no. 9, pp. 12042-12055, Sept. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10070745","Learning generative models at network edge nodes is hindered by limited local data and computational resources, motivating the need to utilize pre-trained generative models from the cloud or peer nodes. This work leverages optimal transport theory within the framework of Wasserstein-1 generative adversarial networks (WGANs) to systematically optimize continual learning at the edge by viewing knowledge transfer as Wasserstein balls centered on pre-trained models. The continual learning process is formulated as a constrained optimization problem, reducible to finding a Wasserstein-1 barycenter. The proposed two-stage approach first computes adaptive barycenters offline using displacement interpolation in a recursive WGAN setting, establishing a meta-model. This meta-model is then used for initialization at the edge node, where rapid adaptation with local data produces the target generative model. Furthermore, the framework introduces a weight ternarization techniquejointly optimizing weight values and quantization thresholdsto further compress the resulting generative model."
73,"Philipp Hess, Michael Aich, Baoxiang Pan, and Niklas Boers, ""Fast, scale-adaptive and uncertainty-aware downscaling of Earth system model fields with generative machine learning,"" Nature Machine Intelligence, vol. 7, pp. 363–373, 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00980-5","This paper presents a machine learning approach utilizing consistency models (CMs) for the efficient, probabilistic, and scale-adaptive downscaling of Earth system model (ESM) precipitation fields. The problem addressed is the high computational cost and lack of generalization in previous downscaling methods, which required retraining for each ESM and struggled with unseen climates. The proposed CM is trained solely on high-resolution observational ERA5 data and uses a 2D U-Net architecture. Spatial resolution is optimized via power spectral density intersections, and the model denoises noised ESM fields in a single step at inference. Results demonstrate that the CM matches or outperforms state-of-the-art diffusion models in spatial correlation (e.g., Pearson $r=0.95$ for the Potsdam Earth Model), bias correction (notably in climate extremes and zonal means), and computational efficiency, generating samples in 0.1s compared to diffusion's 39.4s. Ensemble downscaling enables empirical quantification of uncertainty, as measured by the continuous ranked probability score. The method reliably preserves trends in future climate scenarios, generalizes to out-of-sample climates, and does not require explicit physical constraints. The approach thus enables fast, uncertainty-aware, and high-fidelity downscaling of ESMscrucial for climate impact assessmentsand suggests further extensions, such as incorporating physical constraints, multivariate prediction, and improved temporal dynamics."
74,"Ziqi Chen, Bo Peng, Xia Ning, Mengxuan Zhou, Jiaqi Zhu, Pengyong Li, Rongrong Jin, and Long Zhang, ""Generating 3D small binding molecules using shape-conditioned diffusion models with guidance,"" Nature Machine Intelligence, vol. 7, pp. 588–591, 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-01009-4","This paper introduces the Geometry-Complete Diffusion Model (GCDM), a novel SE(3)-equivariant generative model for 3D molecule generation using denoising diffusion probabilistic frameworks and geometry-complete message-passing neural networks (GCPNet++). GCDM jointly diffuses both atom coordinates and types in fully-connected 3D molecular graphs and uses chirality-aware layers to ensure robust geometric and chemical validity. GCDM outperforms previous models across unconditional and property-conditional generation on QM9, and generates more energetically-stable, large, and valid molecules on GEOM-Drugs, as measured by negative log-likelihood, validity, and property-matching metrics. Furthermore, GCDM enables downstream tasks such as molecular structure optimization and protein-conditional molecule generation (GCDM-SBDD), achieving superior results in drug-like property metrics (e.g., Vina score, QED, diversity). Despite high computational requirements for large molecule batches, GCDM demonstrates state-of-the-art performance, versatility for drug design applications, and robustness across datasets, with further improvements anticipated through more efficient sampling and advanced geometric representations. The source code and datasets are publicly available."
75,"Y. Deldjoo, Z. He, J. McAuley, A. Korikov, S. Sanner, A. Ramisa, R. Vidal, M. Sathiamoorthy, A. Kasrizadeh, S. Milano, and F. Ricci, ""Recommendation with Generative Models,"" arXiv preprint arXiv:2409.15173, Sep. 2024. [Online]. Available: https://arxiv.org/abs/2409.15173","Generative models are advanced AI systems that learn and sample from data distributions to create new instances, with notable frameworks including GANs, VAEs, and transformer-based models like GPT; these have broad applications such as generating images, text, and music. In recommender systems (Gen-RecSys), such models enhance recommendations by producing diverse and structured content, personalization, and dynamic user interactions, impacting fields like eCommerce and media. Unlike prior works, this book offers a comprehensive overview and introduces a taxonomy for deep generative models (DGMs), categorizing them as ID-driven models, large language models (LLMs), and multimodal models, each reflecting distinct technical advancements. This framework enables streamlined research across Gen-RecSys developments, including conversational and multimodal AI, while also addressing evaluation frameworks and the societal implications and risks associated with generative models."
76,"T. Karras, M. Aittala, T. Aila, and S. Laine, ""Elucidating the Design Space of Diffusion-Based Generative Models,"" arXiv preprint arXiv:2206.00364, Oct. 2022, NeurIPS 2022. [Online]. Available: https://arxiv.org/abs/2206.00364","This paper clarifies and modularizes the design space of diffusion-based generative models, which generate high-quality images, audio, and video by reversing a stochastic noising process parameterized by a score network. The authors use a general forward process defined by the SDE $dx = f(x, t) dt + g(t)\,dw$, flexibly parameterize the score network independent of noise schedule, and treat sampling as plug-and-play via a range of ODE/SDE solvers. Explicitly making input/output scaling (preconditioning) a design choice, they introduce improvements in training and samplingincluding a deterministic ODE sampler with adaptive step size and better conditioning of the learning objective. These modular and clearly separated components enable new state-of-the-art image generation, with FID scores of 1.79 (CIFAR-10 class-conditional), 1.97 (unconditional), and 1.36 (ImageNet-64 retrained), all with much faster sampling ($35$ neural net evaluations per image). The approach boosts sample quality even with pretrained networks and supports systematic exploration and reproducibility, paving the way for further advances in high-fidelity generative modeling."
77,"X. Du, N. Kolkin, G. Shakhnarovich, and A. Bhattad, ""Generative Models: What Do They Know? Do They Know Things? Let's Find Out!,"" arXiv preprint arXiv:2311.17137, Oct. 2024. [Online]. Available: https://arxiv.org/abs/2311.17137","This paper introduces Intrinsic LoRA (I-LoRA), a universal, plug-and-play Low-Rank Adaptation approach that enables any generative modelincluding Diffusion, GAN, and Autoregressive modelsto efficiently extract scene intrinsic maps (such as normals, depth, albedo, and shading) directly from their internal representations, using less than 0.6% of the models' parameters and as few as 250 labeled images. The authors show that generative models implicitly encode high-quality information about physical scene properties, evidenced by I-LoRA's ability to match or surpass leading supervised methods when producing scene intrinsic maps. Their method formulates the learning objective as $\min_\theta \mathbb{E}_z \left[d(G_\theta(z), \Phi(G(z)))\right]$ (or, for diffusion models, $\min_\theta \mathbb{E}_x \left[d(G_\theta(x), \Phi(x))\right]$), where $G$ is the image generator, $\theta$ are LoRA parameters, $\Phi$ is a pseudo-ground truth predictor, and $d$ a distance metric. LoRA modules are injected appropriately into model architectures (e.g., attention or affine layers), and ablation studies confirm diminishing returns after low LoRA rank and small labeled sample thresholds. The paper highlights that extraction quality correlates with generative model strength, and extends findings to self-supervised models like DINOv2. Challenges include alignment and color-shift artifacts in diffusion-based generation, which are partly mitigated by improved objectives and conditioning. The authors propose to incorporate intrinsic extraction into generative model objectives and to develop new evaluation metrics for physical properties. Overall, the study evidences that high-performing image generative models naturally acquire and encode intrinsic scene properties, and that these can be 'unlocked' with minimal adaptation, paving the way for practical advances in image understanding and novel research directions."
78,"V. Claveau, A. Chaffin, and E. Kijak, ""Generating artificial texts as substitution or complement of training data,"" Artificial Intelligence, vol. 298, pp. 103528, 2021. [Online]. Available: https://arxiv.org/pdf/2110.13016","The paper investigates the role of artificially generated texts, produced using transformer models like GPT-2, in supervised learning tasks, focusing on three questions: whether artificial data serve as effective complements, whether they can replace unavailable or confidential real data, and whether they improve model explainability. Through experiments on classification tasks such as sentiment analysis and fake news detection, the study finds that while artificial data can be beneficial to some extent, significant performance improvements require pre-processing. Notably, bag-of-word classifiers derive the greatest advantage from data augmentation with artificial samples."
79,"L. Jing and Y. Tian, ""Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4037–4058, Nov. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9086055","Large-scale labeled data are typically needed to train deep neural networks for computer vision tasks, but collecting and annotating such datasets is costly. Self-supervised learning, a subset of unsupervised methods, enables learning general-purpose visual features from large unlabeled datasets without human annotations. This paper extensively reviews deep learning-based self-supervised visual feature learning from images and videos, covering the motivation, standard methodologies, and terminology of the field. It summarizes the main neural network architectures employed, discusses the principal components and evaluation metrics, reviews commonly used image and video datasets, and categorizes existing self-supervised learning methods. The paper also presents quantitative performance comparisons on benchmark datasets, discusses results for both image and video feature learning, and concludes with promising future research directions in self-supervised visual feature learning."
80,"M. C. Tsakiris, ""Low-Rank Matrix Completion Theory via Plücker Coordinates,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 8, pp. 10084–10099, Aug. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10056236","Despite the popularity of low-rank matrix completion, the majority of its theory has been developed under the assumption of random observation patterns, whereas very little is known about the practically relevant case of non-random patterns. Specifically, a fundamental yet largely open question is to describe patterns that allow for unique or finitely many completions. This paper provides two such families of patterns for any rank. A key to achieving this is a novel formulation of low-rank matrix completion in terms of Plucker coordinates, the latter a traditional tool in computer vision. This connection is of potential significance to a wide family of matrix and subspace learning problems with incomplete data."
81,"C. Gao, B. D. Killeen, Y. Hu, R. B. Grupp, R. H. Taylor, M. Armand, and M. Unberath, ""Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis,"" Nature Machine Intelligence, vol. 5, pp. 294–308, 2023. Available: https://www.nature.com/articles/s42256-023-00629-1","This paper presents SyntheX, a framework leveraging realistic simulation and strong domain randomization to enable machine learning models for X-ray-based clinical tasks without dependence on large-scale real-data acquisition. Instead of labor-intensive and ethically fraught in situ data collection, SyntheX uses advanced physics-based simulation (DeepDRR), complemented by domain generalization/adaptation methods, to synthesize annotated datasets for training. In benchmarking across three clinical applicationship imaging (landmark localization, segmentation), surgical robotic tool detection, and COVID-19 lung lesion segmentationmodels trained solely on simulated data achieved real-world test performance comparable to, and in some cases surpassing, those trained on matched real data. For example, in hip imaging, SyntheX Sim2Real landmark error was $5.95 \pm 3.52 \text{ mm}$ versus $6.46 \pm 8.21 \text{ mm}$ for Real2Real, and segmentation Dice scores were $0.86 \pm 0.21$ and $0.79 \pm 0.25$, respectively. Tool detection exhibited similar parity ($1.10 \pm 0.88\text{mm}$ Sim2Real error vs. $1.19 \pm 2.49\text{mm}$ Real2Real) and notably higher Sim2Real Dice score ($0.92 \pm 0.07$ vs. $0.41 \pm 0.23$). Scaling synthetic data volumes further improved performance, and strong domain randomization matched or exceeded domain adaptation methods like CycleGAN and ADDAimportantly, without requiring real data access during training. The approach enables cost-effective, flexible generation of diverse training data, facilitating the testing of novel instrumentation and clinical workflows unconstrained by human subject data limitations. Limitations include the fidelity of simulation, annotation mismatches across modalities, and computational requirements, with future work aiming to enhance pathological simulation realism. SyntheX is open-source and accompanied by public datasets, accelerating research in medical AI for interventional imaging."
82,"X. Xing, F. Shi, J. Huang, Y. Wu, Y. Nan, S. Zhang, Y. Fang, M. Roberts, C.-B. Schönlieb, J. Del Ser, and G. Yang, ""On the caveats of AI autophagy,"" Nature Machine Intelligence, vol. 7, pp. 172–180, 2025. Available: https://www.nature.com/articles/s42256-025-00984-1","Generative Artificial Intelligence (AI) technologies and large models are advancing rapidly, enabling the production of realistic outputs in domains such as images, text, speech, and music; however, the creation of these models is resource-intensive, especially in terms of acquiring large, high-quality datasets. To reduce costs, developers increasingly rely on model-generated (synthetic) data for training, but not all such data improves performance, highlighting the need for a strategic mix of real and synthetic data. The uncontrolled proliferation and unregulated spread of synthetic datanow contaminating datasets collected via web scrapinghas led to a phenomenon dubbed AI autophagy, in which generative AI systems may unwittingly train increasingly on their own outputs. This raises significant concerns about degrading model performance, reliability, and ethical standards. The paper reviews the literature to explore the consequences and risks of AI autophagy and proposes mitigation strategies to promote the sustainable evolution of generative AI, advocating a deliberate and balanced approach to dataset curation in the era of large models."
83,"Y. Tang, J. Weng, and P. Zhang, ""Neural-network solutions to stochastic reaction networks,"" Nature Machine Intelligence, vol. 5, pp. 376–385, 2023. Available: https://www.nature.com/articles/s42256-023-00632-6","The paper presents a machine-learning approach based on a variational autoregressive network to solve the chemical master equation, which describes the time evolution of the joint probability distribution over species counts in a stochastic reaction network. Traditional methods face computational challenges due to exponential growth in state space with the number of species, but the proposed method leverages the policy gradient algorithm within a reinforcement learning framework and eliminates the need for prior simulation data. Unlike traditional trajectory simulators, this approach directly models the joint probability distribution's time evolution, allowing efficient sampling and computation of normalized probabilities. Applied to cases in physics and biology, the technique demonstrates accuracy, the ability to represent multimodal distributions, compatibility with conservation laws, support for time-dependent rates, and efficiency in high-dimensional settings due to flexible count limits. This suggests a generalizable modern machine learning framework for investigating stochastic reaction networks."
84,"Wijnand van Woerkom, Davide Grossi, Henry Prakken, and Bart Verheij, ""A Fortiori Case-Based Reasoning: From Theory to Data,"" Journal of Artificial Intelligence Research, vol. 81, pp. 2113–2151, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15178","The paper addresses the need for interpretability in machine learning systems, especially for sensitive applications, by leveraging concepts from AI & law, particularly the theory of precedential constraintwhich interprets training data as analogous to legal cases and offers a formalism for understanding how machine learning models use their data. The authors advance this theory by connecting it to order theory and logic, thus enabling a software implementation capable of computational manipulation of the models definitions and providing automatic proofs for its properties. Their implementation is evaluated on multiple datasets, permitting an analysis that identifies what types of datasets are best described by the theory, thus contributing to the broader understanding of which data conditions favor explainable machine learning outcomes."
85,"Ying Wang, Fuyuan Ma, Zhaoqi Yang, Yaodi Zhu, Bo Yang, Pengfei Shen, and Lei Yun, ""Rumor Detection with Adaptive Data Augmentation and Adversarial Training,"" Journal of Artificial Intelligence Research, vol. 82, pp. 1–34, 2025. [Online]. Available: https://doi.org/10.1613/jair.1.16963","Rumors spread rapidly on social media, negatively impacting social stability, and while many detection methods exist, most overlook robustness against noise and adversarial attacks, reducing effectiveness in unknown environments. To address this, the ADAAT framework (Adaptive Data Augmentation and Adversarial Training) adaptively modifies less important features and edges while employing adversarial training to generate hard samples for contrastive learning, thereby bolstering the detection model's robustness. Experimental results indicate that ADAAT improves rumor detection accuracy by an average of 3.6%, 4.5%, and 2.5% over state-of-the-art methods on the Twitter15, Twitter16, and PHEME datasets, respectively. Furthermore, when tested on adversarially attacked data, the framework sustains its effectiveness, with detection accuracy dropping by only 1.3%, 1.4%, and 1.2% across those datasets."
86,"Serban Stan and Mohammad Rostami, ""Preserving Fairness in AI under Domain Shift,"" Journal of Artificial Intelligence Research, vol. 81, pp. 1966–2003, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.16694","Existing algorithms for ensuring fairness in AI use a single-shot training strategy, where an AI model is trained on an annotated training dataset with sensitive attributes and then fielded for utilization. This training strategy is effective in problems with stationary distributions, where both the training and testing data are drawn from the same distribution. However, it is vulnerable with respect to distributional shifts in the input space that may occur after the initial training phase. As a result, the time-dependent nature of data can introduce biases and performance degradation into the model predictions, even if the model is initially fair. Model retraining from scratch using a new annotated dataset is a naive solution that is expensive and time-consuming. We develop an algorithm to adapt a fair model to remain fair and generalizable under domain shift using solely new unannotated data points. We recast this learning setting as an unsupervised domain adaptation (UDA) problem. Our algorithm is based on updating the model such that the internal representation of data remains unbiased despite distributional shifts in the input space. We provide empirical validation on three common fairness datasets to show that the challenge exists in practical setting and to demonstrate the effectiveness of our algorithm."
87,"R. Timpone and Y. Yang, ""Artificial Data, Real Insights: Evaluating Opportunities and Risks of Expanding the Data Ecosystem with Synthetic Data,"" arXiv preprint arXiv:2408.15260, Aug. 2024. [Online]. Available: https://arxiv.org/abs/2408.15260","Synthetic Data is not new, but recent advances in Generative AI have raised interest in expanding the research toolbox, creating new opportunities and risks. This article provides a taxonomy of the full breadth of the Synthetic Data domain. We discuss its place in the research ecosystem by linking the advances in computational social science with the idea of the Fourth Paradigm of scientific discovery that integrates the elements of the evolution from empirical to theoretic to computational models. Further, leveraging the framework of Truth, Beauty, and Justice, we discuss how evaluation criteria vary across use cases as the information is used to add value and draw insights. Building a framework to organize different types of synthetic data, we end by describing the opportunities and challenges with detailed examples of using Generative AI to create synthetic quantitative and qualitative datasets and discuss the broader spectrum including synthetic populations, expert systems, survey data replacement, and personabots."
88,"R. Liu, J. Wei, F. Liu, C. Si, Y. Zhang, J. Rao, S. Zheng, D. Peng, D. Yang, D. Zhou, and A. M. Dai, ""Best Practices and Lessons Learned on Synthetic Data,"" presented at COLM 2024, arXiv preprint arXiv:2404.07503, Aug. 2024. [Online]. Available: https://arxiv.org/abs/2404.07503","The paper surveys the landscape of synthetic data as a key enabler for modern AI development, especially in addressing data scarcity, privacy, and high annotation costs. Synthetic data, generated via algorithms, models, or simulations, plays a crucial role in domains such as mathematical and code reasoning, multimodal learning, multilingual tasks, planning, and tool-use; it also enhances tasks such as instruction-following and model alignment with human preferences. Empirical evidence demonstrates synthetic data's effectivenessimproving model capability, diversity, privacy, and scalabilitywhile emphasizing best practices for ensuring factuality, fidelity, and fairness. The authors note risks like bias, hallucination, misinformation, and benchmark contamination, recommending robust validation, ethical oversight, and careful integration with real data. Future research directions include establishing scaling laws, improving quality and diversity attributes, enabling scalable oversight with synthetic evaluations, and understanding iterative self-improvement. Overall, the work highlights the promise and challenges of synthetic data for building inclusive, trustworthy, and high-performing AI systems, urging the community to develop standards and responsible usage protocols as the field advances."
89,"M. Ibrahim, Y. Al Khalil, S. Amirrajab, C. Sun, M. Breeuwer, J. Pluim, B. Elen, G. Ertaylan, and M. Dumontier, ""Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges,"" arXiv preprint arXiv:2407.00116, Jul. 2024. [Online]. Available: https://arxiv.org/abs/2407.00116","This paper systematically reviews recent developments in generative modelsincluding GANs, VAEs, diffusion models, and LLMsfor synthesizing various forms of medical data such as images (dermoscopic, mammographic, ultrasound, CT, MRI, X-ray), text, time-series, and tabular (EHR) data, with a focus on works published from January 2021 to November 2023. Unlike previous reviews with a narrower emphasis, this study covers a wide spectrum of modalities and evaluates the application purposes, synthesis techniques, and evaluation methodologies of synthetic medical data. It notes that while conditional models using class labels, segmentation masks, and image translations are widespread, there remains a significant underutilization of patient-specific and clinical context information, highlighting the opportunity for more personalized synthetic data generation. The survey identifies a prevailing use of synthetic data for augmentation but reveals a lack of application in validating and evaluating downstream clinical AI models. Furthermore, it emphasizes that the absence of standardized, domain-tailored evaluation protocols for synthetic medical data impedes clinical deployment, underscoring the need for dedicated benchmarking, detailed comparative studies, and collaborative practices to advance the responsible use of generative AI in the medical domain."
90,"G. Wang, J. Zhang, K. Zhang, R. Huang, and L. Fang, ""GiganticNVS: Gigapixel Large-Scale Neural Rendering With Implicit Meta-Deformed Manifold,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 1, pp. 338-353, Jan. 2024. [Online]. Available: https://doi.org/10.1109/TPAMI.2023.3323069","The paper presents GiganticNVS, a novel approach for high-fidelity novel view synthesis (NVS) in gigapixel-level, large-scale imaging scenarios, addressing the challenges posed by large baselines and underutilized high-resolution image details. Traditional NVS methods result in blurred artifacts due to insufficient exploitation of image resolution and difficulties in faithful geometry recovery, relying heavily on dense observations for radiance interpolation. The authors introduce a meta-deformed manifold, where 'meta' denotes a locally defined surface manifold embedded in a high-dimensional latent space, decoded as neural fields via an MLP (implicit representation). This representation allows effective multi-view geometric correspondence through featuremetric deformation and enables reflectance field learning directly on the surface. Experimental evaluations demonstrate that the proposed method surpasses state-of-the-art techniques both quantitatively and qualitatively, particularly excelling on datasets with complex real-world scenes featuring large baseline angles and on ultra-large-scale gigapixel benchmarks."
91,"Y. Luo, Q. Yang, Y. Fan, H. Qi, and M. Xia, ""Measurement Guidance in Diffusion Models: Insight from Medical Image Synthesis,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 7983-7997, Dec. 2024. [Online]. Available: https://doi.org/10.1109/TPAMI.2024.3363309","This paper addresses the challenge of medical image data scarcity by advancing diffusion models for high-quality synthetic sample generation aimed at improving downstream clinical tasks such as disease grading and diagnosis. Traditional diffusion model advancements focus on metrics like Frechet Inception Distance (FID) and Inception Score (IS), with classifier guidance offering only limited control over synthetic data distributionoften resulting in uninformative, in-distribution samples. The authors develop an uncertainty-guided diffusion model (UGDM), which incorporates entropy and margin-based uncertainty at each sampling step to enhance the informativeness and label reliability of generated images, thus reducing overfitting and increasing clinical utility. Evaluation across four benchmark medical datasets using ten classic deep learning networks (including VGG and ResNet) shows consistently higher average diagnostic accuracy, supported by a theoretical foundation for general measurement-guided generation. The work marks a significant step toward controllable, diagnostically-useful synthetic data for healthcare AI, being the first to comprehensively assess generative model efficacy across a wide spectrum of downstream networks."
92,"Z. Wan, J. Zhang, D. Chen, and J. Liao, ""High-Fidelity and Efficient Pluralistic Image Completion With Transformers,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 9612-9629, Dec. 2024. [Online]. Available: https://doi.org/10.1109/TPAMI.2024.3382342","Image completion has made tremendous progress with convolutional neural networks (CNNs), because of their powerful texture modeling capacity. However, due to some inherent properties (eg, local inductive prior, spatial-invariant kernels), CNNs do not perform well in understanding global structures or naturally support pluralistic completion. Recently, transformers demonstrate their power in modeling the long-term relationship and generating diverse results, but their computation complexity is quadratic to input length, thus hampering the application in processing high-resolution images. This paper brings the best of both worlds to pluralistic image completion: appearance prior reconstruction with transformer and texture replenishment with CNN. The former transformer recovers pluralistic coherent structures together with some coarse textures, while the latter CNN enhances the local texture details of coarse priors guided by the high-resolution masked images. The proposed method vastly outperforms state-of-the-art methods in terms of three aspects: 1) large performance boost on image fidelity even compared to deterministic completion methods; 2) better diversity and higher fidelity for pluralistic completion; 3) exceptional generalization ability on large masks and generic dataset, like ImageNet. Code and pre-trained models have been publicly released at https://github.com/raywzy/ICT."
93,"Zichen Yang, Haifeng Liu, and Deng Cai, ""On the Diversity of Conditional Image Synthesis With Semantic Layouts,"" IEEE Transactions on Image Processing, vol. 28, no. 6, pp. 2898-2907, June 2019. [Online]. Available: https://doi.org/10.1109/TIP.2019.2891935","The paper introduces a novel diversity loss for conditional image synthesis, particularly targeting tasks where one input maps to many plausible outputs (e.g., turning semantic layouts into realistic images). The central challenge is that existing models, like Pix2pix and CRN, tend to ignore input noise, leading to minimal variety in their outputs. To address this, the authors' diversity loss objective maximizes the distance between pairs of synthesized images in the output space as their corresponding input noise vectors diverge, explicitly linking each noise dimension to a specific semantic segment (such as windows, sky, or cars). Formally, for $f: L \times N \to I$ (with $L$ semantic layouts, $N$ noise, $I$ images), the loss enforces output diversity responsive to changes in noise, constrained to prevent implausible segment distortions. Extensively tested on the CMP Facades and Cityscapes datasets, the method demonstrably allows independent manipulation of image segments and a dramatically higher combinatorial diversity: each noise channel alters only its target segment, letting users intuitively drive local changes. Quantitative and qualitative metrics (e.g., accuracy and IoU from a pretrained segmenter, and human-assigned realism) indicate that realism is preserved or marginally improved despite greater diversity. The approach generalizes as a drop-in module for various GAN frameworks and sidesteps memory bottlenecks in prior diversity-augmenting solutions. Although induced variations are predominantly in color or illumination, with few structural changes, the method paves the way for user-driven, explainable control of image synthesis. Future work aims to expand this to instance-level segmentation, richer structural variety, and broader image translation tasks by semantically linking noise to deep features."
94,"Hongchen Tan, Xiuping Liu, Meng Liu, Baocai Yin, and Xin Li, ""KT-GAN: Knowledge-Transfer Generative Adversarial Network for Text-to-Image Synthesis,"" IEEE Transactions on Image Processing, vol. 30, pp. 1275-1290, 2021. [Online]. Available: https://doi.org/10.1109/TIP.2021.3049544","This paper introduces the Knowledge-Transfer Generative Adversarial Network (KT-GAN), a novel framework for fine-grained text-to-image generation, incorporating two key mechanisms: the Alternate Attention-Transfer Mechanism (AATM), which alternately updates word attention and image sub-region attention to enhance important textual features and image detail, and the Semantic Distillation Mechanism (SDM), which leverages an image encoder trained on image-to-image tasks to guide the text encoder for better feature extraction and image generation in text-to-image tasks. Experimental results on two public datasets demonstrate that KT-GAN significantly outperforms baseline methods and achieves competitive performance across various evaluation metrics."
95,"Y. X. Tan, C. P. Lee, M. Neo, and K. M. Lim, ""Text-to-image synthesis with self-supervised learning,"" Pattern Recognition, vol. 127, pp. 108633, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0167865522001064","Text-to-image synthesis involves extracting semantic meaning from textual descriptions and generating corresponding images, with applications in fields such as graphic design and image editing. Traditional approaches, primarily based on generative adversarial networks (GANs), face challenges in generating visually realistic images and are prone to overconfidence and training instability. This paper introduces a self-supervised text-to-image synthesis method enhanced by self-supervised learning for greater image variation, feature matching, $L_1$ distance loss to improve visual similarity to real images, and one-sided label smoothing to penalize overconfident discriminator predictions, thereby stabilizing training. Experiments on the Oxford-102 and CUB datasets reveal that the proposed approach produces images with richer diversity, better visual realism, and stronger semantic consistency with the input text, outperforming comparable methods in both inception score and Structural Similarity Index metrics."
96,"V. Talasila, M. R. Narasingarao, and M. V. Murali Mohan, ""Optimized GAN for text-to-image synthesis: Hybrid Whale Optimization Algorithm and Dragonfly Algorithm,"" Pattern Recognition, vol. 128, pp. 108610, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0031320321006535","This research introduces a new text-to-image synthesis method comprising three main stages: feature extraction, text encoding, and optimized image synthesis. Initially, textual features including improved TFIDF, bag of words, and N-gram are extracted and processed using a Bi-LSTM model. Cross-modal feature grouping occurs during the text-to-image encoding phase. The image itself is synthesized via a modified GAN (MGAN) that employs a novel loss function. To further enhance synthesis precision, GAN weights are optimized using the Self-improved Social Ski-Driver (SI-SSD) optimization algorithm. Comparative assessments validate the models superiority over existing approaches."
97,"Qiushi Sun, Jingtao Guo, and Yi Liu, ""Face image synthesis from facial parts,"" EURASIP Journal on Image and Video Processing, vol. 2022, no. 7, pp. 1–22, 2022. [Online]. Available: https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-022-00585-7","This paper introduces a novel deep generative network for synthesizing realistic face images conditioned on small patches of key facial parts, where alignment and semantic coherence are essential despite most pixels being absent. Leveraging the extraction of 68 facial keypoints via dlib, the model incorporates three principal loss functions: a per-pixel reconstruction loss (ensuring content generation in unknown regions), a perceptual loss (enhancing modeling of high-level semantics), and an adversarial loss (promoting visual realism), along with total variation regularization for artifact suppression. The method, built on the TensorFlow platform and evaluated on benchmarks like CelebA, CACD, and LFW, outperforms strong baselines such as Pix2Pix and standard inpainting, achieving visually realistic outputs and superior performance across several quantitative metrics (detailed in Table 3). Experimental results confirm the model's ability to construct coherent faces by fusing features from distinct individuals, with applications spanning portrait drawing, medical preview, and the generation of synthetic training data for face recognition. Future work includes controllable attribute manipulation and style variation for increased versatility."
98,"Li Yao, Qiurui Lu, and Xiaomin Li, ""View synthesis based on spatio-temporal continuity,"" EURASIP Journal on Image and Video Processing, vol. 2019, no. 86, pp. 1–18, 2019. [Online]. Available: https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-019-0485-9","This paper addresses the challenges of generating high-quality free viewpoint videos (FVV) using depth-image-based rendering (DIBR), particularly the presence of holes, ghost artifacts, and temporal discontinuities like flicker. The authors propose a novel spatio-temporal virtual view synthesis method that leverages time domain information from video sequences to extract a static background image of the scene and introduces a weighted-fusion hole-filling method based on this background. The method involves enhanced depth map processing using edge detection, expansion, and Gaussian smoothing, static scene extraction via SSIM-based pixel matching across frames, and artifact elimination using combined color-depth fusion. Experimental evaluation on standard datasets ('BreakDancers', 'Ballet') demonstrates that the proposed approach surpasses previous methods in both subjective and objective measures, achieving higher PSNR and SSIM scores for spatial quality and lower F-scores for improved temporal continuity, as summarized in inline LaTeX tabular format for result comparisons. The method is limited to fixed camera setups and can be computationally intensive, with future work aimed at accelerating processing using parallel computation (e.g., CUDA). In conclusion, the presented method effectively mitigates artifacts, fills holes, and maintains strong spatio-temporal continuity in virtual view synthesis by utilizing both spatial and temporal information."
99,"Dragana Sandić-Stanković, Dragan Kukolj, and Patrick Le Callet, ""DIBR-synthesized image quality assessment based on morphological multi-scale approach,"" EURASIP Journal on Image and Video Processing, vol. 2017, no. 4, pp. 1–23, 2016. [Online]. Available: https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-016-0124-7","This paper addresses the limitations of conventional image quality assessment (IQA) metrics in evaluating depth-image-based rendering (DIBR)-synthesized 3D images, which suffer from unique geometric and edge distortions, especially near disoccluded regions. To better capture these artifacts, the authors introduce two full-reference multi-scale image quality metricsMorphological Pyramid Peak Signal-to-Noise Ratio (MP-PSNR) and Morphological Wavelet Peak Signal-to-Noise Ratio (MW-PSNR)that utilize non-linear morphological filters in the decomposition process to preserve geometric edge information across resolution levels. Edge distortions between the multi-scale representations of a reference and a synthesized image are quantified using mean squared error (MSE), placing emphasis on artifact-prone edges. Both metrics were evaluated on two DIBR image databases (IRCCyN/IVC DIBR and MCL-3D), where they achieved significantly higher correlation with human subjective scores than existing state-of-the-art metrics, with MP-PSNR slightly outperforming MW-PSNR; for example, reduced MP-PSNR achieved Pearsons 0.904 and Spearmans 0.863 on the IRCCyN/IVC DIBR database. The methods are computationally efficient, requiring only basic operations (min, max, sum), and do not require parameter optimization or image registration. Reduced versions that focus on high-level detail bands retain high agreement with subjective assessment while further lowering computational requirements, making these metrics practical for real-world 3D video applications."
100,"R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, ""High-Resolution Image Synthesis with Latent Diffusion Models,"" arXiv preprint arXiv:2112.10752, presented at CVPR 2022, 2022. [Online]. Available: https://arxiv.org/abs/2112.10752","This paper introduces Latent Diffusion Models (LDMs), which decompose the image generation process by training diffusion models in the latent space of powerful pretrained autoencoders, thereby achieving a near-optimal balance between computational efficiency and fidelity. The approach involves first training a perceptual compression model (patch-based adversarial VQGAN) to obtain a lower-dimensional, perceptually equivalent representation, then training a time-conditional UNet diffusion model in this latent space. Cross-attention layers enable flexible conditioning on various modalitiessuch as text, class labels, or layoutsand allow high-resolution synthesis. LDMs achieve state-of-the-art or highly competitive results on unconditional and conditional image generation, inpainting, and super-resolution, substantially reducing computational demands compared to pixel-based diffusion models and previous GANs. Quantitative benchmarks (e.g., FID, LPIPS) and user studies confirm that LDMs deliver superior or equivalent fidelity and versatility with fewer parameters and less computation. Challenges include sequential sampling speed compared to GANs and autoencoding bottlenecks for pixel-perfect tasks, while future work aims to accelerate sampling, enhance conditioning flexibility, and explore ethical considerations. Overall, LDMs provide a simple, versatile, and efficient generative modeling framework, readily scalable to high-resolution synthesis and adaptable to diverse conditioning scenarios."
101,"X. Ouyang, Y. Chen, K. Zhu, and G. Agam, ""Fine-grained Text to Image Synthesis,"" arXiv preprint arXiv:2412.07196, 2024. [Online]. Available: https://arxiv.org/abs/2412.07196","This paper presents FG-RAT GAN, a fine-grained text-to-image synthesis approach that enhances the Recurrent Affine Transformation (RAT) GAN by incorporating an auxiliary classifier and a contrastive learning strategy. The auxiliary classifier is integrated into the discriminator to enable class-wise image classification and guide the generator toward producing fine-grained, class-consistent images. Simultaneously, the contrastive learning mechanism, utilizing a cross-batch memory (XBM), is designed to maximize similarity between images of the same subclass while minimizing it between different subclasses, thereby sharpening intra-class and inter-class distinctions. The total loss functions combine adversarial, categorical cross-entropy, and contrastive components: discriminator loss $L_d^\text{total} = L_d^\text{adv} + L_d^\text{ce} + L_d^\text{cl}$ and generator loss $L_g^\text{total} = L_g^\text{adv} + L_g^\text{ce} + L_g^\text{cl}$. Experiments on the CUB-200-2011 and Oxford-102 datasets show that FG-RAT GAN achieves state-of-the-art Frechet Inception Distance (FID) and competitive Inception Scores (IS), outpacing LAFITE, VQ-Diffusion, RAT GAN, and large diffusion models, while being computationally efficient. Ablation studies confirm the significant impact of both the auxiliary classifier and contrastive learning. Although dependent on labeled data for fine-grained categories, this method advances subclass-aware image generation from text and sets the stage for future development toward label-independent models. Sample results demonstrate higher semantic and visual fidelity, as summarized in evaluation tables (\begin{tabular}{lccc}\hline Model & FID & IS & Params \\\hline FG-RAT GAN & \textbf{lowest} & top & fewest\\ LAFITE & higher & $\sim$ & more\\ VQ-Diffusion & higher & $\sim$ & more\\\hline\end{tabular}), underscoring FG-RAT GANs superior performance with fewer resources."
102,"F. Liu and X. Chang, ""IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis,"" arXiv preprint arXiv:2403.13378, accepted by CVMJ 2024, 2024. [Online]. Available: https://arxiv.org/abs/2403.13378","The paper introduces IIDM, an Image-to-Image Diffusion Model for semantic image synthesis, which generates realistic images given both semantic segmentation masks ($m$) and style reference images ($X_R$). Unlike conventional GAN-based methods that synthesize images in a single step and often struggle to simultaneously satisfy style and segmentation constraints, IIDM re-frames synthesis as an image denoising task using a latent diffusion model. The process starts by encoding the style image to a latent vector $z_0 = E(X_R)$, initializing the diffusion with $z_T = \sqrt{\bar{\alpha}_T} E(X_R) + \sqrt{1-\bar{\alpha}_T}\, \epsilon$ ($\epsilon \sim \mathcal{N}(0, I)$), and performing progressive denoising toward $z_0$ under the guidance of the segmentation mask. The denoised latent is decoded to produce the output image $X_G = D(\hat{z}_0)$. Three inference-stage enhancementsrefinement, color transfer, and model ensemblefurther improve quality and style fidelity without extra training. IIDM significantly outperforms prior GAN and diffusion-based methods in mask accuracy (94.15%), FID (30.75), and total score, effectively balancing semantic content and style similarity, as shown in inline evaluations and ablation studies. The models effective architecture and flexible inference components demonstrate strong results on a large-scale landscape dataset and led to success in a competitive AI challenge."
