# -*- coding: utf-8 -*-
"""Hallucination_result.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a1zgZrJzgffPHNeGbpSCoMvDX9hcUMAW
"""

!pip install habanero
!pip install PyMuPDF

import os
import csv
import fitz  # PyMuPDF
import re
import requests
from habanero import Crossref

cr = Crossref()

# === STEP 1: Extract references from PDF using layout blocks ===
def extract_references_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    references = []
    current_ref = ""
    last_y = None

    for page in doc[-3:]:  # last few pages typically hold references
        for block in page.get_text("dict")["blocks"]:
            for line in block.get("lines", []):
                spans = [span["text"].strip() for span in line["spans"] if span["text"].strip()]
                if not spans:
                    continue
                line_text = " ".join(spans)

                y = line["bbox"][1]  # top coordinate of line
                if last_y is not None and abs(y - last_y) > 10 and len(current_ref) > 30:
                    references.append(current_ref.strip())
                    current_ref = ""
                current_ref += " " + line_text
                last_y = y

    if current_ref.strip():
        references.append(current_ref.strip())

    references = [r.strip() for r in references if len(r.strip()) > 50 and re.search(r"\d{4}", r)]
    return references

def check_crossref(title):
    try:
        title_guess = title.split(".")[0][:100]
        res = cr.works(query=title_guess, limit=1)
        items = res.get("message", {}).get("items", [])
        if items:
            return True, items[0].get("title", ["(no title)"])[0], "CrossRef"
    except Exception as e:
        print("âš ï¸ CrossRef error:", e)
    return False, None, "CrossRef"

def check_semantic_scholar(title):
    try:
        query = title.split(".")[0]
        url = f"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&limit=1&fields=title"
        res = requests.get(url).json()
        data = res.get("data", [])
        if data:
            return True, data[0].get("title", "(no title)"), "SemanticScholar"
    except Exception as e:
        print("âš ï¸ Semantic Scholar error:", e)
    return False, None, "SemanticScholar"

def extract_arxiv_id(text):
    match = re.search(r"arxiv:(\d+\.\d+)", text, re.IGNORECASE)
    return match.group(1) if match else None

def check_arxiv_trace(arxiv_id):
    try:
        url = f"https://export.arxiv.org/api/query?id_list={arxiv_id}"
        res = requests.get(url)
        return "entry" in res.text
    except Exception as e:
        print("âš ï¸ arXiv error:", e)
        return False

def compute_expanded_ctr(pdf_path):
    refs = extract_references_from_pdf(pdf_path)
    print(f"\nðŸ“ [{os.path.basename(pdf_path)}] Found {len(refs)} reference candidates")
    if not refs:
        return 0.0, 0, []
    detailed_results = []
    valid = 0
    for i, ref in enumerate(refs):
        print(f"\nðŸ” Checking [{i+1}]: {ref[:80]}...")
        found, title, source = check_crossref(ref)
        if not found:
            found, title, source = check_semantic_scholar(ref)
        if not found:
            arxiv_id = extract_arxiv_id(ref)
            if arxiv_id and check_arxiv_trace(arxiv_id):
                found, title, source = True, f"arXiv:{arxiv_id}", "arXiv"
        if found:
            valid += 1
            print(f"âœ… Found via {source}: {title}")
        else:
            print("âŒ Not traceable")
        detailed_results.append({
            "ref_text": ref[:100],
            "matched_title": title if title else "(none)",
            "found": found,
            "source": source
        })
    ectr = valid / len(refs)
    hallucination_rate = 1 - ectr
    print(f"\nðŸ“Š eCTR: {ectr:.2f} ({ectr*100:.1f}%), Hallucination: {hallucination_rate:.2f}")
    return ectr, len(refs), detailed_results

# === NEW: Batch mode for a folder ===
def batch_analyze_pdfs(folder_path, output_csv="hallucination_report.csv"):
    results = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(folder_path, filename)
            try:
                ectr, total, _ = compute_expanded_ctr(pdf_path)
                results.append({
                    "file": filename,
                    "total_refs": total,
                    "valid_refs": int(total * ectr),
                    "eCTR": round(ectr, 4)
                })
            except Exception as e:
                print(f"âŒ Failed to process {filename}: {e}")
                results.append({
                    "file": filename,
                    "total_refs": 0,
                    "valid_refs": 0,
                    "eCTR": 0.0
                })

    # Write results to CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["file", "total_refs", "valid_refs", "eCTR"])
        writer.writeheader()
        writer.writerows(results)

    # Compute and print average eCTR
    valid_ectrs = [r["eCTR"] for r in results if r["total_refs"] > 0]
    mean_ectr = sum(valid_ectrs) / len(valid_ectrs) if valid_ectrs else 0.0
    print(f"\nâœ… Batch analysis completed. Mean eCTR: {mean_ectr:.4f}")
    return mean_ectr

# === Example usage ===
batch_analyze_pdfs("citation")

