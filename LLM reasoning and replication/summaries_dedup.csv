Index,Citation,Summary
1,"Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H. Chen, ""Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine,"" npj Digital Medicine, vol. 7, Article 20, 2024. [Online]. Available: https://www.nature.com/articles/s41746-024-01010-1","This study examines whether large language models (LLMs), specifically GPT-3.5 and GPT-4, can imitate human clinical reasoning when making diagnostic decisions in medicine. The researchers developed specialized diagnostic reasoning promptsmodeled on differential diagnosis, intuitive, analytical, and Bayesian inferenceand compared them to traditional chain-of-thought (CoT) prompting using free-response USMLE (MedQA) exam questions and challenging New England Journal of Medicine (NEJM) case reports. GPT-3.5 achieved its best accuracy (48%) with intuitive reasoning, but performed significantly worse with other clinical reasoning prompts compared to traditional CoT. GPT-4 showed improved overall accuracy (7678% across most prompting strategies on MedQA) with little significant performance difference between traditional CoT and diagnostic reasoning prompts. Notably, GPT-4 can convincingly generate rationales that mimic clinical reasoning processes, which supports interpretability, allowing clinicians to assess the trustworthiness of responses based on rationale qualitythe study found incorrect answers often included logical errors (65% of incorrect GPT-4 rationales), whereas correct ones did so less frequently (18%). These insights suggest that while GPT-4 can emulate clinical reasoning formats, it does not necessarily use human-like reasoning processes to improve diagnostic accuracy, but its ability to provide interpretable rationales may help mitigate the black box perception of LLMs in clinical settings. All datasets and code are openly available via figshare."
2,"Marco Guevara, Shan Chen, Spencer Thomas, Tafadzwa L. Chaunzwa, Idalid Franco, Benjamin H. Kann, Shalini Moningi, Jack M. Qian, Madeleine Goldstein, Susan Harper, Hugo J. W. L. Aerts, Paul J. Catalano, Guergana K. Savova, Raymond H. Mak, and Danielle S. Bitterman, ""Large language models to identify social determinants of health in electronic health records,"" npj Digital Medicine, vol. 7, Article 6, 2024. [Online]. Available: https://www.nature.com/articles/s41746-023-00970-0","Social determinants of health (SDoH), critical for understanding health disparities, are underdocumented in structured electronic health records (EHRs) but often appear in unstructured clinical notes. This study evaluated methods for using large language models (LLMs) to extract six SDoH categoriesemployment, housing, transportation, parental status, relationship, and social supportfrom EHR narratives. Utilizing a curated dataset of annotated clinic notes and synthetic data generated via GPT-3.5, fine-tuned Flan-T5 models (especially the XL/XXL variants) achieved robust performance, with $\text{macro-F1} = 0.71$ for detecting any SDoH and $0.70$ for adverse mentions; these outperformed both BERT and zero/few-shot ChatGPT-family models except for GPT-4 with 10-shot prompting in adverse SDoH detection. Data augmentation with LLM-synthesized text improved performance particularly for rare classes and reduced sensitivity to annotation scarcity. Importantly, fine-tuned models demonstrated lower susceptibility to demographic bias: their predictions were less affected by injected race/ethnicity or gender descriptors ($p < 0.05$) compared to ChatGPT. When benchmarked against EHR structured codes (ICD-10 Z-codes), text-extracted models identified 93.8% of patients with adverse SDoH, while codes alone identified only 2.0%. The findings support the utility of specialized LLMs and synthetic data for better SDoH documentation and highlight publicly available resources to advance research and support patient care."
3,"Hanyin Wang, Chufan Gao, Christopher Dantona, Bryan Hull, and Jimeng Sun, ""DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients,"" npj Digital Medicine, vol. 7, Article 16, 2024. [Online]. Available: https://www.nature.com/articles/s41746-023-00989-3","The study introduces DRG-LLaMA, an advanced large language model based on LLaMA and fine-tuned using Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, aimed at improving the accuracy of Diagnosis-Related Group (DRG) assignment in the U.S. inpatient payment system. DRG-LLaMA-7B surpassed prior models, achieving a macro-averaged F1 score of 0.327 (a 40.3% and 35.7% relative improvement over ClinicalBERT and CAML), a top-1 prediction accuracy of 52.0%, and macro-AUC of 0.986. The approach included both single-label and decomposed two-label (base DRG and CC/MCC status) classification, with the former yielding slightly better performance. Model scalability (size and input length) correlated with performance gains, with the 13B model and 1024 tokens achieving a macro-F1 of 0.361. Analysis revealed that DRG-LLaMAs performance particularly excelled with common DRGs and longer, higher-quality summaries, though limitations persist due to the use of only discharge summaries and computational constraints. The models superior results suggest large, context-aware LLMs are effective for clinical coding tasks, and future work should extend to earlier-stage note prediction, larger models, and real-world integration in hospital workflows. The main equation for the two-label loss was $L_\text{total} = L_\text{CE, base DRG} + 0.5 \, L_\text{CE, CC/MCC}$. Error analysis identified key challenges in concept extraction, information completeness, and nuanced DRG mapping."
4,"J. Zhou, W. Zhong, Y. Wang, and J. Wang, ""Adaptive-solver framework for dynamic strategy selection in large language model reasoning,"" Information Processing & Management, vol. 62, no. 3, Article 104052, 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0306457324000468","Large Language Models (LLMs) excel at reasoning tasks but typically use a fixed problem-solving approach for all cases, resulting in either unnecessary computational overhead for easy problems or insufficient capability for difficult ones. This paper introduces the Adaptive-Solver (AS) framework, which dynamically adapts model choice, sample size, prompting method, and problem decomposition based on real-time reliability signals (measured via answer consistency) for each question. AS operates via two modules: an initial evaluation that accepts reliable answers after a few calls, and an adaptation module that escalates resources or changes strategy when confidence is low. Experiments on benchmarks like GSM8K, SVAMP, and AQuA show AS reduces expensive API calls by up to 85% without loss in accuracy, or achieves up to 4.5% better accuracy at a fixed computational budget compared to static methods. Ablation studies demonstrate that each adaptation strategymodel, prompt, sample size, or decompositioncontributes individually, but recursive and multi-strategy adaptations provide the largest gains. Limitations include reliance on answer-consistency signals and a rule-based adaptation controller, suggesting future directions in learned controllers and more challenging multimodal benchmarks. Overall, AS narrows the gap between current LLM strategies and the flexible, adaptive reasoning seen in humans, offering a robust pathway for flexible and resource-efficient AI reasoning at scale."
5,"B. Liu, C. Lyu, Z. Min, Z. Wang, J. Su, and L. Wang, ""Retrieval-augmented multi-modal chain-of-thoughts reasoning for large language models,"" Information Processing & Management, vol. 62, no. 1, Article 103907, 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0306457323004317","This paper introduces a novel method for enhancing multi-modal reasoning in Large Language Models (LLMs) using retrieval-augmented Chain of Thought (CoT) demonstration selection. By dynamically retrieving demonstration examples based on cross-modal and intra-modal similarities, and employing Stratified Sampling to promote diversity, the approach ensures that selected examples are both relevant and varied. The methodology involves converting images to textual information using captioning and OCR, encoding examples with models such as CLIP and SentenceBERT, and calculating cosine similarities across four retrieval types: Text-to-Text, Text-to-Image, Image-to-Text, and Image-to-Image. For a test example $q$ with visual and textual context $\{q^v, q^t\}$, the system retrieves top-$k$ similar demonstrations from a pool $Q$ and concatenates these with the original query to form the input prompt. Experiments on ScienceQA and MathVista show significant performance gains: GPT-4 sees a 6\% improvement on ScienceQA and 12.9\% on MathVista, while GPT-4V improves by 2.7\% over zero-shot baselines. Ablation studies confirm that visual knowledge, retrieval mechanisms, and stratified sampling each contribute to improved accuracy, and the combination yields state-of-the-art resultssuch as GPT-4 outperforming previous methods like Chameleon and PoT by 46\%. Despite being tested only on two datasets and mainly with closed-source LLMs, the approach lays important groundwork for diverse and effective demonstration selection in multi-modal CoT LLM prompting, with potential extensions to broader domains and other multi-modal tasks."
6,"X. Liu, X. Wei, G. Shi, D. Liu, F. Qian, P. Wang, and Y. Zhang, ""End-to-end event factuality prediction using directional labeled graph recurrent network,"" Information Processing & Management, vol. 59, no. 2, Article 102836, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S0306457321003083","This paper introduces the Directional Labeled Graph Recurrent Network (DLGRN), an end-to-end multi-task system for event factuality prediction, which jointly identifies event anchors and determines their factuality values using syntactic dependency graphs. DLGRN encodes both the direction and labels of syntactic relations in a recurrent message passing framework, where each token's hidden state is updated by aggregating information from syntactic neighbors with label- and direction-specific transformation matrices, as in $h^{(t+1)}_i = \mathrm{GRU}(h^{(t)}_i, \mathrm{AGG}^{\rightarrow}_{labels}(\mathrm{msg}(H^{\rightarrow}, L^{\rightarrow}, E^{\rightarrow})), \mathrm{AGG}^{\leftarrow}_{labels}(\mathrm{msg}(H^{\leftarrow}, L^{\leftarrow}, E^{\leftarrow})))$. A novel two-stage graph pruning mechanism filters out irrelevant or redundant edges by (1) removing statistically insignificant label types and (2) keeping only high-attention edges per event. Trained jointly on anchor detection (token-level classification) and factuality assignment (classification/regression), DLGRN achieves new state-of-the-art results on four benchmark datasets (FactBank, Meantime, UW, UDS-IH2), for example: anchor F1 of 94.21 and macro-F1 75.69 on FactBank, and Pearson $r = 0.592$ on UDS-IH2, outperforming all prior baselines. Ablation studies show that modeling edge direction, label, and context, as well as graph pruning and multi-tasking, all contribute significantly to performance. The architecture is robust to syntactic noise and extensible to broader domains, with future work planned for adaptation to other languages and integration with deeper semantic representations."
7,"E. La Malfa, A. Petrov, S. Frieder, C. Weinhuber, R. Burnell, R. Nazar, A. Cohn, N. Shadbolt, and M. Wooldridge, “Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,” Journal of Artificial Intelligence Research, vol. 80, 2024. Accessed: Aug. 26, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15865","Some of the most powerful language models today are proprietary and accessible only through restricted web or API interfaces, a trend described as the Language-Models-as-a-Service (LMaaS) paradigm. Unlike open-source models with full accessibility, these closed systems pose challenges for evaluating, benchmarking, and testing. The paper aims to clarify how these challenges hinder the accessibility, reproducibility, reliability, and trustworthiness of LMaaS by systematically assessing the impact of limited information on these aspects. It provides a detailed analysis of existing solutions, offers recommendations, and suggests future directions, while also summarizing licence types and capabilities of popular LMaaS offerings."
8,"C. Wei, K. Duan, S. Zhuo, H. Wang, S. Huang, and J. Liu, “Enhanced Recommendation Systems with Retrieval-Augmented Large Language Model,” Journal of Artificial Intelligence Research, vol. 82, 2025. Accessed: Feb. 26, 2025. [Online]. Available: https://doi.org/10.1613/jair.1.17809","Recommender systems face persistent issues with cold start and data sparsity, often addressed by integrating side information; however, this can introduce noise, inflexibility for data scaling, and inconsistencies, ultimately degrading user preference inference and recommendation quality. Leveraging the extensive knowledge bases and inferential strengths of large language models (LLMs), the authors introduce ER2ALMa framework that utilizes Retrieval-Augmented Generation (RAG) to flexibly and accurately augment auxiliary information, capturing implicit user preferences with reduced noise through a specific strategy designed to enhance reliability. Experimental validation on two real-world datasets shows that ER2ALM significantly outperforms state-of-the-art methods in both accuracy and robustness, suggesting its efficacy as a new paradigm for preference mining in recommendation systems."
9,"M. Pternea, P. Singh, A. Chakraborty, Y. Oruganti, M. Milletari, S. Bapat, and K. Jiang, “The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,” Journal of Artificial Intelligence Research, vol. 80, 2024. Accessed: Aug. 26, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15960","This work presents a comprehensive review of research combining Reinforcement Learning (RL) and Large Language Models (LLMs), introducing the RL/LLM Taxonomy Tree which classifies the synergy between these fields into three main categories: RL4LLM (where RL improves LLMs, either via fine-tuning or prompt optimization), LLM4RL (where LLMs assist RL by shaping rewards, generating goals, or replacing policy modules), and RL+LLM (where both agents collaborate in planning frameworks without training each other, distinguishable by presence or absence of language feedback). The study systematically surveyed 24 representative publications, mapping them to the taxonomys branches, and compiled comparative tables (`\begin{tabular}{l|c|c} Study & Synergy Class & Core Innovation \\ \hline ... \end{tabular}`) detailing their characteristics and results. The review analyzes motivations for RL/LLM integration, such as RL's sequential decision-making adaptability to NLP tasks and LLMs' reasoning/world knowledge, discusses successes in alignment and Responsible AI, and highlights current challenges including limited real-world applicability, scalability, computational efficiency, and dependency on prompt/fine-tuning sensitivity. The paper situates RLLLM architectures within the broader AI method landscape, acknowledging that while promising, these synergies are not always the optimal solution; it also anticipates significant future progress as the field rapidly evolves, with ongoing updates planned for the taxonomy as new research emerges, thus providing a valuable framework for both understanding current work and guiding future developments in RL/LLM synergy."
10,"J. Huang and K. Chen-Chuan Chang, ""Towards Reasoning in Large Language Models: A Survey,"" Findings of the Association for Computational Linguistics: ACL 2023, pp. 1049–1065, 2023. [Online]. Available: https://arxiv.org/abs/2212.10403","Reasoning is a fundamental element of human intelligence, crucial for tasks such as problem solving, decision making, and critical thinking. This survey provides a comprehensive overview of reasoning abilities in large language models (LLMs), reviewing their architecture, training strategies, benchmarks, and prompting techniques (such as zero-shot, few-shot, chain-of-thought, and instruction-based prompts). The results indicate that large-scale LLMs (e.g., GPT-3, PaLM) can perform well on reasoning benchmarks, with accuracy being highly sensitive to promptingchain-of-thought prompting, in particular, yields substantial improvements. However, despite these advances, LLMs remain brittle and prone to failure on logic puzzles, multi-step inference, and tasks that demand integration of world knowledge; they do not yet match human-level abstraction or causal reasoning. The discussion highlights challenges such as over-reliance on statistical patterns, lack of compositional abstraction, annotation artifacts, and limited interpretability. Current evaluation methods are constrained to static benchmarks and do not fully assess interactive or causal reasoning. The paper suggests future work in developing interactive/causal benchmarks, integrating symbolic reasoning, improving rationale transparency, and exploring scaling laws for emergent reasoning abilities. Although substantial progress has been made, fundamental questions about the nature and limitations of LLM reasoning persist, and further cross-disciplinary research is needed to bridge these gaps and advance towards genuine reasoning in language models."
11,"J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, ""Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"" Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 24824–24837, 2022. [Online]. Available: https://arxiv.org/abs/2201.11903","We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
12,"M. Besta, J. Barth, E. Schreiber, A. Kubicek, A. Catarino, R. Gerstenberger, P. Nyczyk, P. Iff, Y. Li, S. Houliston, T. Sternal, M. Copik, G. Kwaśniewski, J. Müller, Ł. Flis, H. Eberhard, H. Niewiadomski, and T. Hoefler, ""Reasoning Language Models: A Blueprint,"" arXiv preprint arXiv:2501.11223, version 3, Jan. 2025. [Online]. Available: https://arxiv.org/abs/2501.11223","This paper presents a comprehensive and modular blueprint for reasoning language models (RLMs), which integrate large language models (LLMs) with advanced reasoning techniques such as reinforcement learning, search heuristics (e.g., Monte Carlo Tree Search, Beam Search), and diverse reasoning structures (chains, trees, graphs, and their nested forms). Organized through a detailed survey and analysis of all RLM work to date, the blueprint unifies disparate architectures (such as LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts) as special cases and provides detailed algorithmic and mathematical formulationse.g., formalizing RLM training as Markov Decision Processes (MDPs) and introducing the Outcome-Driven Process Reward Model, which mathematically links intermediate process steps to final outcomes for more granular feedback during learning. The authors introduce x1, an open-source modular implementation, to facilitate rapid prototyping and experimentation. Using x1, they empirically analyze multi-phase training (separating policy and value models), the importance of training on familiar data distributions, and process-based evaluation on math and reasoning benchmarks. Key practical challenges in scalability, supervision, and resource allocation are discussed, and the blueprints operator set inspires development of novel hybrid search and nested reasoning strategies. The conclusion emphasizes the blueprint and x1 platform as democratizing agents for RLM design, suggesting future directions including trace-based supervision, operator-inspired modeling, and large-scale cloud deployments to make advanced reasoning more accessible across all tiers of AI research and application."
13,"Anya Belz, ""A Metrological Perspective on Reproducibility in NLP,"" Computational Linguistics, vol. 48, no. 4, pp. 1125–1135, Dec. 2022. [Online]. Available: https://aclanthology.org/2022.cl-4.21.pdf","Reproducibility has become an increasingly debated topic in NLP and ML over recent years, but so far, no commonly accepted definitions of even basic terms or concepts have emerged. The range of different definitions proposed within NLP/ML not only do not agree with each other, they are also not aligned with standard scientific definitions. This article examines the standard definitions of repeatability and reproducibility provided by the meta-science of metrology, and explores what they imply in terms of how to assess reproducibility, and what adopting them would mean for reproducibility assessment in NLP/ML. It turns out the standard definitions lead directly to a method for assessing reproducibility in quantified terms that renders results from reproduction studies comparable across multiple reproductions of the same original study, as well as reproductions of different original studies. The article considers where this method sits in relation to other aspects of NLP work one might wish to assess in the context of reproducibility."
14,"Yonatan Belinkov, ""Probing Classifiers: Promises, Shortcomings, and Advances,"" Computational Linguistics, vol. 48, no. 1, pp. 221–242, Mar. 2022. [Online]. Available: https://aclanthology.org/2022.cl-1.10.pdf","Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simplea classifier is trained to predict some linguistic property from a models representationsand has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This article critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances."
15,"Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada Mihalcea, ""Deep Learning for Text Style Transfer: A Survey,"" Computational Linguistics, vol. 48, no. 1, pp. 159–218, Mar. 2022. [Online]. Available: https://aclanthology.org/2022.cl-1.8.pdf","Text style transfer is a significant task in natural language generation, focusing on controlling specific attributes in generated text, such as politeness, emotion, and humor. This survey article reviews developments in neural text style transfer, examining over 100 pivotal works since 2017. It covers task formulations, available datasets and subtasks, evaluation methods, and advances in methodology for both parallel and non-parallel data scenarios. The article also discusses various challenges and opportunities for the future of this field."
16,"R. Dror, G. Baumer, M. Bogomolov, and R. Reichart, ""Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,"" Transactions of the Association for Computational Linguistics, vol. 5, pp. 471–486, 2017. [Online]. Available: https://aclanthology.org/Q17-1033/","With the ever-growing amounts of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper, we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction."
17,"A. Belz, L. Anastasakos, Y. Zhang, S. Spadine, I. Augenstein, and F. Liu, “A Systematic Review of Reproducibility Research in Natural Language Processing,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 249–266, 2021. [Online]. Available: https://aclanthology.org/2021.eacl-main.29.pdf","Against the backdrop of the so-called reproducibility crisis in science, the NLP field has shown growing interest in and concern for the reproducibility of its research findings. Recent years have witnessed numerous new initiatives, events, and active research efforts focused on this issue. However, there remains a lack of consensus on how to define, measure, and address reproducibility in NLP, with perspectives currently diverging rather than converging. This focused contribution aims to offer a comprehensive snapshot of current work on reproducibility within NLP, highlighting key differences and similarities, and suggesting some common threads across the diverse approaches in the field."
18,"P. Van Eecke, J. Nevens, and K. Beuls, ""Neural heuristics for scaling constructional language processing,"" Journal of Language Modelling, vol. 10, no. 2, pp. 287–314, 2022. [Online]. Available: https://doi.org/10.15398/jlm.v10i2.318","Constructionist approaches to language employ form-meaning pairings, known as constructions, to represent all necessary linguistic knowledge for language comprehension and production; processing language then entails assembling sequences of these constructions to resolve linguistic tasks, but as grammars increase in size, finding suitable sequences becomes a combinatorial search problem that is computationally intractable. This paper introduces a neural methodology that learns heuristics to optimize this search for constructional language processing, validated on the CLEVR benchmark dataset. The results show that this approach surpasses state-of-the-art methods in minimizing search space size and computation time, especially in language production tasks, and it holds promise for enabling scalable learning of large-scale construction grammars by overcoming major efficiency constraints."
19,"P. Boersma, T. Benders, and K. Seinhorst, ""Neural network models for phonology and phonetics,"" Journal of Language Modelling, vol. 8, no. 1, pp. 103–177, 2020. [Online]. Available: https://doi.org/10.15398/jlm.v8i1.224","This paper contends that a unified framework for explaining both phonological and phonetic dataincorporating language and experimental findingsmust be grounded in neural networks. It presents an artificial neural network model equipped with the inoutstar learning algorithm, capable of handling stochastic processing in both production and perception of speech. Uniquely, this model simultaneously addresses gradual phonological category formation and auditory dispersion, bridging two critical aspects of language transmission across generations. This offers a solution to the longstanding problem of how apparently discrete linguistic behaviors emerge in children despite receiving gradient input from their linguistic environment. The authors conclude that neural network models, aside from their biological plausibility, offer substantial theoretical promise in understanding linguistic systems that embody both continuous and discrete representations."
20,"S. Jha, A. Sudhakar, and A. K. Singh, ""Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languages,"" Journal of Language Modelling, vol. 7, no. 2, pp. 101–142, 2019. [Online]. Available: https://doi.org/10.15398/jlm.v7i2.214","Out-of-vocabulary (OOV) words present significant challenges in machine translation (MT), especially for low-resource language (LRL) pairs lacking parallel corpora. This work adapts sequence-to-sequence (seq2seq) models to transduce such words from Hindi to Bhojpuri by learning from cognate pairs derived from a bilingual dictionary. The models operate at the character level to capture phonetic and orthographic similarities relevant to both synchronic and diachronic adaptations, as well as loan words and cognates. Training multiple character-level NMT systems, the study finds that these approaches can substantially improve translation quality in resource-poor settings, achieving a 6.3 BLEU score improvement on Hindi-to-Bhojpuri tasks. The methodology also generalizes well, demonstrated by successful transfer to HindiBangla cognate translation. The work thus contributes solutions for (i) the OOV words problem in MT, (ii) building effective parallel corpora for under-resourced languages, and (iii) leveraging semantic knowledge from word embeddings in character-level models."
21,"M. Kinniment, L. J. K. Sato, H. Du, B. Goodrich, M. Hasin, L. Chan, L. H. Miles, T. R. Lin, H. Wijk, J. Burget, A. Ho, E. Barnes, and P. Christiano, ""Evaluating Language-Model Agents on Realistic Autonomous Tasks,"" arXiv preprint arXiv:2312.11671, 2024. [Online]. Available: https://arxiv.org/abs/2312.11671","In this report, the authors investigate the capacity of language model agents for ""autonomous replication and adaptation"" (ARA), which involves self-replication, resource acquisition, and adaptation to novel challenges. Recognizing the potential broad and unpredictable impacts of systems with ARA capabilities, the report emphasizes the importance of measuring and forecasting these abilities for security and alignment purposes. The authors construct four example agents that combine language models with action-executing tools, evaluating them on 12 ARA-relevant tasks. Results show current agents can only complete the simplest tasks, though some progress is seen on harder ones. Crucially, the report warns that these findings do not preclude the possibility of near-future agentsespecially those utilizing next-generation language models with significantly greater compute or fine-tuningachieving robust ARA, underlining the need for ongoing and intermediate evaluations during model development."
22,"L. Vaugrante, M. Niepert, and T. Hagendorff, ""A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions,"" arXiv preprint arXiv:2409.20303, 2024. [Online]. Available: https://arxiv.org/abs/2409.20303","In an era where large language models (LLMs) are increasingly integrated into a wide range of everyday applications, research into these models' behavior has surged. However, due to the novelty of the field, clear methodological guidelines are lacking. This raises concerns about the replicability and generalizability of insights gained from research on LLM behavior. In this study, we discuss the potential risk of a replication crisis and support our concerns with a series of replication experiments focused on prompt engineering techniques purported to influence reasoning abilities in LLMs. We tested GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on the chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well as Re-Reading prompt engineering techniques, using manually double-checked subsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE, ScienceQA, and StrategyQA. Our findings reveal a general lack of statistically significant differences across nearly all techniques tested, highlighting, among others, several methodological weaknesses in previous research. We propose a forward-looking approach that includes developing robust methodologies for evaluating LLMs, establishing sound benchmarks, and designing rigorous experimental frameworks to ensure accurate and reliable assessments of model outputs."
23,"S. Black*, A. C. Stickland*, J. Pencharz, O. Sourbut, M. Schmatz, J. Bailey, O. Matthews, B. Millwood, A. Remedios, and A. Cooney, ""RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents,"" arXiv preprint arXiv:2504.18565, 2024. [Online]. Available: https://arxiv.org/abs/2504.18565","The paper introduces RepliBench, a comprehensive benchmark suite aimed at evaluating the autonomous replication capabilities of large language model (LLM) agents, a critical safety concern as capabilities improve. RepliBench is based on a detailed decomposition of replication into four domains: resource acquisition, model weight exfiltration, compute deployment, and persistent operation, resulting in 20 novel task families with 86 individual tasks. Evaluation of five frontier models shows they can accomplish many subtaskssuch as obtaining cloud compute, deploying self-replicating code, and exfiltrating weights under simple setupsbut consistently fail at more complex challenges like passing KYC checks, setting up persistent successor agents, and evading advanced security. The highest-performing model, Claude 3.7 Sonnet, achieves over 50% pass@10 for 15/20 task families and for 9/20 hardest variants, indicating substantial proficiency in components of replication but not full autonomy. The methodologies use a systematic breakdown, sandboxed tool-using agents, and automatic scoring with some reliance on LLM-based judgement. However, current evaluations are limited by less robust security settings, simulated environments for financial/money tasks, and potential overestimation by judge models. Discussion highlights that while today's models are not yet autonomously self-replicating threats, rapid capability growth and success on key subtasks suggest these risks could materialize soon, particularly with human assistance or better agent persistence. Recommendations include tougher, more realistic security environments and a focus on end-to-end, persistent evaluations to monitor emerging autonomy risks."
24,"V. Mieskes, K. Goeuriot, L. Büchler, S. Evert, S. Kazet, G. Bel, Y. Dupont, J. Duh, F. François, S. Han, M. Jones, A. Kabadjova, M. Kammas, C. Kobus, J. Leveling, R. Lofi, G. Parent, S. Pateux, L. Pla, L. Romanello, M. L. Ruiz-González, and E. SanJuan, ""Reproducibility in Computational Linguistics: Are We Willing to Share?"" Computational Linguistics, vol. 44, no. 4, pp. 641–649, Dec. 2018. [Online]. Available: https://aclanthology.org/J18-4003/","This study examines the willingness of computational linguistics authors to share source code and data, a crucial factor for reproducibility. Reviewing 395 full papers from the 2011 and 2016 ACL Annual Meetings, the research assessed the presence and functionality of data and code links, and contacted authors when these were missing. Results revealed data was commonly shared but code less so; when not included, code was provided upon request in about one third of cases. Reproduction attempts on ten selected papers found approximate results for six, with exact reproduction achieved in just one case. While sharing practices and reproducibility seem to have improved over five years, the field still largely relies on trust. The study notes a positive trend and highlights that papers with working source code links receive higher median citation counts, underlining benefits for both the field and individual researchers."
25,"C. Thomson, E. Reiter, and A. Belz, ""Common Flaws in Running Human Evaluation Experiments in NLP,"" Computational Linguistics, vol. 50, no. 2, pp. 795–807, Apr. 2024. [Online]. Available: https://aclanthology.org/2024.cl-2.9/","The ReproHum project systematically attempted to replicate human evaluation experiments in NLP, uncovering flaws in every caseeven among papers selected for their apparent reproducibilitysuch as coding errors, inappropriate data exclusion, incorrect reporting, poor interface design, and ethical oversights. These issues stem from inadequate reporting, insufficient peer review processes, and a prevailing research culture that often deprioritizes rigor and transparency. The study underscores that current evaluation practices jeopardize scientific trust and calls for adopting measures like pre-registration, rigorous code development, and systematic piloting. Drawing on best practices from other scientific fields, the authors advocate for specialist-reviewed statistics and ethics, automated data-to-table reporting, and supportive post-publication discourse, asserting that only substantive cultural and procedural shifts can ensure the reproducibility and scientific integrity essential to NLP as a discipline."
26,"M. Crane, ""Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results,"" Transactions of the Association for Computational Linguistics, vol. 6, pp. 241–252, 2018. [Online]. Available: https://transacl.org/ojs/index.php/tacl/article/download/1299/296/3798","Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported."
27,"D. Deutsch, N. Kassner, J. Li, R. Reichart, and D. Roth, ""A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods,"" Transactions of the Association for Computational Linguistics, vol. 9, pp. 1132–1146, 2021. [Online]. Available: https://transacl.org/index.php/tacl/article/view/3125/1031","The quality of a summarization evaluation metric is typically assessed by the correlation between its scores and human annotations across many summaries; however, the precision of these correlation estimates and the significance of differences between metrics remain unclear. This work proposes methods to calculate confidence intervals and conduct hypothesis testing for metric correlations using bootstrapping and permutation resampling techniques. Two simulation experiments determine the most suitable method for summarization tasks, which is then applied to various automatic evaluation metrics across three human-annotated datasets. Results show that confidence intervals are generally wide, indicating considerable uncertainty in metric reliability, and statistical improvements over ROUGE are rarethough QAEval and BERTScore demonstrate significant gains in some settings."
28,"W. Digan, A. Névéol, A. Neuraz, M. Wack, D. Baudoin, C. Rance, A. Burgun, and P. Rosset, ""Can reproducibility be improved in clinical natural language processing? A study of 7 clinical NLP suites,"" Journal of the American Medical Informatics Association, vol. 28, no. 3, pp. 504–515, 2021. [Online]. Available: https://academic.oup.com/jamia/article/28/3/504/6034902","The paper addresses the reproducibility challenges in clinical natural language processing (NLP) pipelines by evaluating whether workflow management systems (WMS) and bioinformatics practices can enhance reproducibility. Through literature review, the authors identified 40 specific reproducibility features spanning traceability, versioning, standardization, usability, and shareability, and used these to assess the compliance of seven NLP frameworks (including cTakes, CLAMP, GATE, ScispaCy, TextFlow, LAPPS Grid, and OpenMinTed). Frameworks based on WMS (notably LAPPS Grid and OpenMinTed) matched over 50% of the recommended features, outperforming traditional clinical NLP tools. Key reproducibility bottlenecks identified include lack of version control at the tool and pipeline level, ad hoc rather than standardized provenance metadata, and inadequate support for containerization and public workflow repositories. Drawing on bioinformaticswhich widely leverages WMS like Snakemake, Galaxy, and Nextflow, container technologies, and standardized data formats such as PROV, the authors recommend clinical NLP adopt similar strategies to boost modularity, provenance, and shareability. They underline the importance of actionable reproducibility for robust clinical secondary data use and suggest that systematic adoption of public workflow repositories, containers, and provenance standards will further promote replicability and reusability in clinical NLP. Notably, technical traceability and versioning, when combined with FAIR principles, are critical for empirically validating NLP tools in new environments, despite ongoing clinical data-sharing challenges."
29,"S. Bakken, ""The journey to transparency, reproducibility, and replicability,"" Journal of the American Medical Informatics Association, vol. 26, no. 3, pp. 185–187, 2019. [Online]. Available: https://academic.oup.com/jamia/article/26/3/185/5301680","Transparency, reproducibility, and replicability are essential principles for scientific rigor and advancement in biomedical and health informatics, as emphasized for publications in JAMIA. To promote these values, researchers should comprehensively document their data analysis workflow, deposit data in repositories like Dryad, and report all code versions and analysis steps, preferably using version control systems (e.g., Git). Connection of data, code, and narrative through approaches such as literate programming (e.g., Jupyter Notebooks, R Markdown) strengthens research integrity. Protocol registration, adherence to publishing guidelines (such as CONSORT, PRISMA, SRQR), and providing public access to data, scripts, and results are advocated for open science. Illustrative papers in this JAMIA issue highlight practical applications: Price et al demonstrate challenges in national information system strategy, Gonul et al develop a transparent and adaptive digital intervention framework, Mercaldo et al describe reproducible sampling for genomic survey research, Afshar et al ensure replicable clinical text analytics, and Lu et al improve spell correction using dual-embedding models with clear reproducibility supports. The editorial calls on the community to further embed transparency and openness in biomedical informatics research practices."
30,"D. J. Schlueter, N. R. Bar, E. Shin, P. Chou, E. Winden, X. Zhou, J. Ramirez, K. Chu, N. Guller, B. Liang, H. E. Armour, J. H. Gilmore, and L. Bastarache, ""Systematic replication of smoking disease associations using survey responses and EHR data in the All of Us Research Program,"" Journal of the American Medical Informatics Association, vol. 31, no. 1, pp. 139–150, 2024. [Online]. Available: https://academic.oup.com/jamia/article/31/1/139/7330649","This study evaluated the concordance between survey and electronic health record (EHR) definitions of smoking exposure for replicating associations with various phenotypes in the All of Us dataset. Researchers identified 165 meta-analyses representing 99 distinct phenotypes matched to EHR phenotypes, replicating most published associations with smoking; specifically, 74 out of the total were found nominally significant at $P < 0.05$. Both survey-based and EHR-based smoking definitions produced similar results. The study concludes that it is feasible to study common exposures using the All of Us data, supporting the reproducibility of known exposure-outcome associations from meta-analyses."
31,"M. Eguchi and K. Kyle, ""Building custom NLP tools to annotate discourse-functional features for second language writing research: A tutorial,"" Research Methods in Applied Linguistics, vol. 3, no. 3, p. 100153, Dec. 2024. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S2772766124000594",The current tutorial paper describes a process of developing a custom natural language processing model with a particular focus on a discourse annotation task.
32,"A. Laurinavichyute, H. Yadav, and S. Vasishth, ""Share the code, not just the data: A case study of the reproducibility of articles published in the Journal of Memory and Language under the open data policy,"" Journal of Memory and Language, vol. 125, p. 104332, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0749596X22000195",The aim of the present work is to evaluate the data sharing policy and to investigate the reproducibility of papers published in the Journal of Memory and Language under the open data policy.
33,"I. Magnusson, N. A. Smith, and J. Dodge, ""Reproducibility in NLP: What Have We Learned from the Checklist?,"" arXiv preprint arXiv:2306.09562 [cs.CL], To be published in ACL 2023 Findings, 2023. [Online]. Available: https://arxiv.org/abs/2306.09562","Scientific progress in NLP hinges on reproducibility, prompting the *CL conferences to introduce the NLP Reproducibility Checklist in 2020, which authors complete at submission. Analyzing 10,405 anonymous Checklist responses, the study found improved reporting on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Submissions with more 'Yes' checklist answers had higher acceptance rates. Notably, 44% of submissions involved new data collection, but these were 5% less likely to be accepted and received 2% lower reproducibility ratings. Only 46% claimed to open-source code, but those that did achieved an 8% higher reproducibility scorethe largest increase of any checklist item. The authors recommend allowing code and appendices to be submitted a week after the deadline and suggest measuring dataset reproducibility via a dedicated checklist."
34,"A. Belz, ""Quantifying Reproducibility in NLP and ML,"" arXiv preprint arXiv:2109.01211 [cs.CL], 2021. [Online]. Available: https://arxiv.org/abs/2109.01211","Reproducibility in NLP and ML has been a contentious issue without a standard approach for assessment or quantification, partly due to the belief that broader scientific reproducibility definitions do not apply to these fields. This paper challenges that assumption by directly applying standard metrology terminology and definitions to NLP/ML. The authors demonstrate that this approach enables the creation of a practical framework that yields a quantifiable degree of reproducibility, facilitating comparisons across different reproduction studies."
35,"Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas W. Mayer, and Padhraic Smyth, ""What large language models know and what people think they know,"" Nature Machine Intelligence, vol. 7, pp. 221–231, Feb. 2025. [Online]. Available: https://www.nature.com/articles/s42256-024-00976-7","As large language models (LLMs) are increasingly used in decision-making, this study examines the discrepancy between their internal confidence and how effectively they communicate uncertainty to users. Using behavioral experiments with multiple-choice and short-answer datasets (MMLU and Trivia QA), and models GPT-3.5, PaLM2, and GPT-4o, the authors measure the calibration gap (difference between LLM and human confidence) and discrimination gap (ability to distinguish correct from incorrect answers). Results show that users, provided with default LLM explanations, consistently overestimate model accuracyparticularly when explanations are longer, regardless of factual accuracydue to a length bias. Metrics such as Expected Calibration Error (ECE) and AUC reveal that model confidence is better calibrated and more discriminative than human perception based solely on explanations; for example, model AUC was around 0.75 while human AUC was near 0.59. Adjusting explanation language to transparently reflect model confidence narrows both gaps, aligning human trust with actual model certainty and improving decision relevance. Limitations include the focus on specific question types and double-prompting to control explanation style. The findings underscore the critical role of communicating uncertainty effectively and suggest future research into scalable, real-world approaches for confidence-modified explanations and understanding underlying drivers of explanation misalignment. Responsible alignment of explanation style with model confidence is essential for building trustworthy AI systems, particularly in high-stakes applications."
36,"Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan, ""Large language models for scientific discovery in molecular property prediction,"" Nature Machine Intelligence, vol. 7, pp. 437–447, Mar. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-00994-z","Large language models (LLMs) represent an advanced form of artificial intelligence capable of performing diverse tasks such as writing, translation, summarization, and code generation, yet their application in scientific discoveryparticularly molecular property predictionremains largely untapped. This work presents LLM4SD, a framework that leverages LLMs to synthesize knowledge from scientific literature (e.g., identifying that molecular weight impacts solubility) and infer new patterns from molecular data such as SMILES-encoded structures (e.g., determining that halogen-containing molecules are more likely to cross the bloodbrain barrier). These insights are represented as interpretable features, converting molecules into vectors that, when input to models like random forests, enable LLM4SD to surpass current benchmarks in predicting molecular properties. By making these features interpretable, LLM4SD also has the potential to provide novel scientific insights and facilitate discovery in molecular property prediction."
37,"Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian Du, and Christopher G. Lucas, ""Embodied large language models enable robots to complete complex tasks in unpredictable environments,"" Nature Machine Intelligence, vol. 7, pp. 592–601, Apr. 2025. [Online]. Available: https://www.nature.com/articles/s42256-025-01005-x","The paper introduces ELLMER, an embodied robotic framework that integrates a large language model (GPT-4) with retrieval-augmented generation (RAG), force, and vision sensing to enable robots to complete complex, long-horizon tasks in unpredictable environments. ELLMER dynamically extracts relevant knowledge to compose adaptable action plans, incorporating real-time force and visual feedback for precise manipulation. The systems effectiveness is demonstrated in tasks such as making coffee and decorating plates, where it responds to high-level natural language prompts, decomposes them into subtasks, and autonomously manages uncertainties like variable object locations or ingredient quantities. Hardware implementation uses a Kinova robotic arm with a Robotiq gripper, Azure Kinect vision, and a multiaxis force sensor, orchestrated through ROS. Evaluation against a baseline (VoxPoser) shows that augmenting LLMs with RAG and sensor feedback significantly improves task faithfulness and adaptability, as measured by the accuracy of generated robot plans. The approach also highlights efficient carbon emissions (approx.~7g CO$_2$ per 4-minute task). Challenges remain in modeling complex force dynamics and expanding tactile capabilities, but ELLMERs design is flexible for future enhancements, supporting integration of soft robotics and advanced sensory modalities. By uniting LLM-driven cognitive reasoning with robotic sensorimotor skill, ELLMER advances toward scalable, intelligent robots capable of robustly completing complex real-world tasks through embodied machine intelligence."
38,"G. Izacard, F. Petroni, L. Hosseini, S. Krone, A. Joulin, S. Khattab, E. Grave, and S. Wang, ""Atlas: Few-shot Learning with Retrieval Augmented Language Models,"" Journal of Machine Learning Research, vol. 24, pp. 1-35, 2023. [Online]. Available: http://www.jmlr.org/papers/volume24/23-0037/23-0037.pdf","Atlas is a retrieval-augmented language model designed for few-shot learning on knowledge-intensive tasks, pairing a dense retriever (Contriever) with an encoder-decoder reader (T5). The retriever, pretrained with contrastive learning, fetches the most relevant Wikipedia passages given an input, which the reader then encodes with the context to generate answers. The reader is pretrained as a denoising autoencoder and with a self-retrieval task. Fine-tuning on only a handful of labeled examples per task enables Atlas to achieve state-of-the-art results in open-domain question answering, fact verification, and related areas. For example, with just 64 Natural Questions examples, Atlas attains 42.4% accuracy, outperforming prior few-shot techniques and even approaching the performance of fully supervised methods. Experiments and ablation studies confirm the benefits of joint retriever-reader pretraining and the critical role of retrieval quality. While Atlas generalizes well across tasks and corpora and is robust to retrieval noise, it currently faces challenges related to computational demands and dependence on the breadth and coverage of the retrieval corpus. Future work aims to improve efficiency, extend to multilingual or cross-domain scenarios, and enhance retrieval for better performance in real-time or resource-constrained environments."
39,"N. Muennighoff, D. Garrette, F. Hernandez, B. Brorsson, H. Buechel, E. Qiu, M. Vania, M. Sporleder, R. Bingel, S. Kanerva, K. Rama, and A. E. G. Blancke, ""Scaling Data-Constrained Language Models,"" Journal of Machine Learning Research, vol. 26, pp. 1-91, 2025. [Online]. Available: https://www.jmlr.org/papers/volume26/24-1000/24-1000.pdf","The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations."
40,"Sublime, J., ""The AI Race: Why Current Neural Network-based Architectures are a Poor Basis for Artificial General Intelligence,"" Journal of Artificial Intelligence Research, vol. 80, pp. 1165-1189, 2024. [Online]. Available: https://jair.org/index.php/jair/article/view/15315/26999","Artificial General Intelligence (AGI) refers to a hypothetical AI agent that would far surpass the intelligence of the brightest human minds. This concept has been discussed since the early days of AI, raising numerous scenarios about its potential behavior toward humans. The paper reviews recent advances in AI, highlighting how rapid developments, impressive new methods, and their effectssuch as deceiving humans, excelling at previously unthinkable tasks, and disrupting job marketshave intensified fears of AGI arriving sooner than expected. By focusing on three specific families of modern AI, the authors argue that deep neural networks, despite being the backbone of current AI, possess fundamental limitations that make them poor candidates for AGI. Consequently, the paper suggests that imminent threats are not from AGI itself, but rather from the limitations, applications, and insufficient regulation of today's AI models and algorithms."
41,"Castagna, F., Pelosi, G., Rago, A., Toni, F., and Wang, C., ""Computational Argumentation-based Chatbots,"" Journal of Artificial Intelligence Research, vol. 79, pp. 129-179, 2024. [Online]. Available: https://www.jair.org/index.php/jair/article/view/15407/27067","Chatbots are conversational applications designed for diverse interactions, but only recently have they integrated computational argumentationmodels that formalize argument exchange in a machine-readable format to better simulate human dialogue. This survey examines the literature on argumentation-based chatbots, assessing their advantages and disadvantages compared to traditional bots. It also considers future directions, such as merging argumentation methodologies with Transformer architectures and advanced large language models, to enhance the sophistication and utility of conversational agents."
42,"Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, ""A Survey of Large Language Models,"" arXiv preprint arXiv:2303.18223 [cs.CL], 2023. Available: https://arxiv.org/abs/2303.18223","Language is a complex system governed by grammatical rules, posing challenges for the development of capable AI algorithms to understand and generate language. Over the past two decades, language modeling has evolved from statistical models to neural approaches, with recent advances focusing on pre-trained language models (PLMs) utilizing large-scale Transformer architectures. Scaling up model parameters has led not only to improved performance but also to emergent abilities absent in smaller models, prompting the community to distinguish these as large language models (LLMs). Notably, both academia and industry have accelerated LLM research, exemplified by the launch of ChatGPT, which has had a significant societal impact. This survey reviews LLM progress, focusing on four key aspects: pre-training, adaptation tuning, utilization, and capacity evaluation, while also summarizing resources and future challenges in the field."
43,"Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian, ""A Comprehensive Overview of Large Language Models,"" arXiv preprint arXiv:2307.06435 [cs.CL], 2023. Available: https://arxiv.org/abs/2307.06435","This article presents a comprehensive literature review of Large Language Models (LLMs), summarizing key innovations in architectures (e.g., transformers, encoder/decoder/encoder-decoder variants), training strategies (such as bfloat precision, learning rate scheduling, and parallelism), the impact of data scale and fine-tuning, efficiency advances (including PEFT, quantization, and pruning), and evaluation against benchmarks like MMLU, BIG-bench, SuperGLUE, and HumanEval. It tabulates comparative performance and analyzes both pre-trained and fine-tuned/instruction-following LLMs like GPT-3, T5, PaLM, LLaMA, BLOOM, Flan, Alpaca, Vicuna, and LLaMA-2-Chat, among others. The article provides critical discussion of LLM challenges, such as computational scaling, bias/fairness, hallucinations, security/privacy, interpretability, and societal impacts, along with outlining future directions including resource-efficient architectures, mitigations for bias and toxicity, robust privacy, multimodality, continual learning, and stronger regulatory and ethical frameworks. By synthesizing extensive research, the review serves as a self-contained guide and a quick reference for both foundational concepts and state-of-the-art developments in LLM research, offering detailed insights for advancing the field."
44,"Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy, ""Challenges and Applications of Large Language Models,"" arXiv preprint arXiv:2307.10169 [cs.CL], 2023. Available: https://arxiv.org/abs/2307.10169","Large Language Models (LLMs) have rapidly transitioned from being non-existent to becoming a pervasive topic in machine learning, presenting challenges in identifying both persistent problems and successful application domains due to the field's swift evolution. This paper aims to systematically outline open problems and highlight areas of application success, enabling ML researchers to quickly grasp the current landscape and accelerate their productivity."
45,"T. Gauthier, M. Olšák, J. Urban, ""Alien coding,"" Artificial Intelligence, vol. 323, pp. 104036, October 2023. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S000437022300142X","We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments."
46,"W. van Woerkom, D. Grossi, H. Prakken, and B. Verheij, ""A Fortiori Case-Based Reasoning: From Theory to Data,"" Journal of Artificial Intelligence Research, vol. 81, Art. no. 15178, pp. 1–38, 2024. [Online]. Available: https://doi.org/10.1613/jair.1.15178","This paper advances the theory of precedential constrainta formal framework inspired by legal case-based reasoningfor explaining machine learning decisions by relating training data to legal cases using order theory and logic. Each data point is represented as a tuple $(x, y)$, and the model constructs partial orders and geometric structures (cones) to formalize notions such as 'precedent', 'distinguishing', and a fortiori reasoning. The authors provide a new order-theoretic formalization, implement this as an open-source Python package that computes cones, anti-chains, forced/unforced cases, and supports automated proofs, and empirically evaluate its explanatory power on various real and synthetic datasets. Their results show the model fits well for datasets with monotonic or hierarchical structurefully determining many casesbut exposes its limitations in the presence of high feature overlap and non-monotonic labels, where many cases remain undecided. Discussion highlights the approach's utility for transparency and auditability in AI, while acknowledging challenges such as scaling to high-dimensional data, handling non-monotonicity, and bridging to black-box models. Future work will extend the approach to richer domains and hybridize logical with statistical reasoning, aiming for interpretable and robust AI in law and societally sensitive contexts."
47,"P. Totis, J. Davis, L. de Raedt, and A. Kimmig, ""Lifted Reasoning for Combinatorial Counting,"" Journal of Artificial Intelligence Research, vol. 76, Art. no. 14062, pp. 1–58, 2023. [Online]. Available: https://doi.org/10.1613/jair.1.14062","Combinatorics math problems, which require counting solutions to scenarios described in natural language, are used to evaluate human cognitive and problem-solving abilities. While humans excel at identifying recurring structures and applying closed-form formulas exploiting symmetry and exchangeability, current AI approaches fall short of such efficient reasoning. This paper addresses this shortcoming by first identifying combinatoric problem classes beyond the scope of traditional lifted counting methods, then introducing a novel declarative language tailored to these problems. The authors present innovative lifted solving algorithms that integrate probabilistic inference with constraint programming, and implement these methods in a solver capable of efficiently addressing the targeted problem class. Their approach is validated on both real-world combinatorial math problems and synthetic datasets, demonstrating clear advancements over prior methodologies."
48,"C. Cornelio, J. Goldsmith, U. Grandi, N. Mattei, F. Rossi, and K. B. Venable, ""Reasoning with PCP-Nets,"" Journal of Artificial Intelligence Research, vol. 72, Art. no. 13009, pp. 1103–1161, 2021. [Online]. Available: https://doi.org/10.1613/jair.1.13009","We introduce PCP-nets, a formalism that extends CP-nets to model qualitative conditional preferences incorporating probabilistic uncertainty over preference orderings. The work defines and analyzes optimality and dominance queries within PCP-nets, proposing a tractable approximation for dominance that proves highly accurate experimentally. PCP-nets are also explored as aggregated representations in multi-agent settings, where individual agents contribute CP-nets that are combined using different aggregation techniques, assessed via score concepts from voting theory. Experimental results highlight which aggregation methods most accurately reflect the underlying set of CP-nets and reveal the most computationally efficient dominance procedures in such multi-agent scenarios."
49,"Wenguan Wang, Yi Yang, and Fei Wu, ""Towards Data-And Knowledge-Driven AI: A Survey on Neuro-Symbolic Computing,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, Early Access, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10721277","Neural-symbolic computing (NeSy) seeks to unify the strengths of symbolic AIreasoning and interpretabilitywith the robust learning capabilities of neural networks, aiming to address limitations of current AI systems in trustworthiness, safety, and explainability. The paper provides a comprehensive survey of NeSy, tracing its historical evolution from the early ambitions to merge symbolism and connectionism, to present-day advances fueled by deep learning. Key drivers include both theoretical aspirations to model cognition and pragmatic needs in applications. The survey categorizes NeSy methods by integration type, methods for embedding symbolic knowledge in neural architectures, and their focus on learning versus reasoning, detailing six integration types and a variety of knowledge representations (e.g., knowledge graphs, logic, symbolic expressions). Application results span numerous domains such as scientific discovery, robotics, vision-language analysis, and mathematical reasoning, with NeSy models consistently outperforming neural-only baselinese.g., in retrosynthesis prediction and math word problem solvingaccording to benchmarks (\textit{e.g.}, Top-k match accuracy and mIoU metrics). Despite advancements, challenges persist in scalably reasoning over large knowledge bases, achieving broad compositional generalization, automated knowledge acquisition, recursive integration of neural and symbolic modules, and synthesizing NeSy with large language models. The paper concludes that while robust and general NeSy systems remain aspirational, the rapid progress in the field positions NeSy as a promising path toward the next generation of data- and knowledge-driven AI."
50,"Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwen Wang, Wen Zhang, Junwei Wang, Xiang Zhao, Xiaoyan Zhu, and Enhong Chen, ""A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 14061-14083, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10577554","This paper presents the first comprehensive survey of knowledge graph reasoning (KGR), which aims to deduce new facts in knowledge graphs (KGs) using logic rules for applications such as question answering and recommendation systems. KGR models are systematically categorized by graph typestatic, temporal, and multimodaland reviewed within a bi-level taxonomy: (1) top-level by KG type and (2) base-level by technique (embedding-based, path-based, rule-based, RNN-based, transformer-based, etc.) and reasoning scenarios (transductive, inductive, interpolation, extrapolation). Methodological comparisons and quantitative benchmarks, including inline tables (e.g., `\begin{tabular}{l|l|l} Model & Dataset & Result \\ \hline GNN & Static & SOTA \\ RNN & Temporal & SOTA \end{tabular}`), reveal that GNN-based methods excel for static KGs and RNN/transformer models for temporal/multimodal scenarios. The paper details datasets and benchmarks, evaluates state-of-the-art performance, compares technique strengths and weaknesses, and discusses future challenges, such as out-of-distribution reasoning, scalability, advanced fusion for multi-modal data, and explainable KGR. Significant opportunities remain in integrating large language models with KGs and broadening real-world applications. The authors share an open-source repository cataloguing 180 KGR models and 67 datasets for further research: https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning."
51,"Chaoqi Chen, Jiongcheng Li, Hong-Yu Zhou, Xiaoguang Han, Yue Huang, Xinghao Ding, and Yizhou Yu, ""Relation Matters: Foreground-Aware Graph-Based Relational Reasoning for Domain Adaptive Object Detection,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 3677-3694, Mar. 2023, doi: 10.1109/TPAMI.2022.3179445. [Online]. Available: https://ieeexplore.ieee.org/document/9786681","Domain Adaptive Object Detection (DAOD) seeks to improve object detector generalization via knowledge transfer; however, both global and local alignment methods struggle to capture topological relations among foreground objects and often neglect dependencies between and within domains, which can lead to issues such as overfitting to less transferable regions like backgrounds. This work reformulates DAOD as an open-set domain adaptation problem, treating foregrounds as known classes and backgrounds as an unknown class, and introduces the Foreground-aware Graph-based Relational Reasoning (FGRR) framework. FGRR uses graph structures to model intra- and inter-domain object relations at pixel and semantic levels, leveraging bipartite graphs for visual and semantic correlations and graph attention mechanisms for intra-domain relations. By utilizing message-passing to aggregate semantic and contextual information across domains, FGRR significantly improves expressive capability and, according to empirical results, surpasses state-of-the-art performance on four DAOD benchmarks."
52,"Andrea Passerini, Aryo Gema, Pasquale Minervini, Burcu Sayin, and Katya Tentori, ""Fostering effective hybrid human-LLM reasoning and decision making,"" Frontiers in Artificial Intelligence, vol. 7, 1464690, 2025. [Online]. Available: https://www.frontiersin.org/articles/10.3389/frai.2024.1464690/full","This perspective article calls for a shift in large language model (LLM) research toward enhancing human-LLM interaction, arguing that focusing on collaboration is critical for effective reasoning and decision-making. While modern LLMs, such as ChatGPT, exhibit impressive capabilities across tasks like translation and summarization, they are prone to biases including hallucination, inconsistency, and sycophancy; similarly, humans exhibit persistent cognitive biases that aren't easily mitigated. The interaction of these systems can amplify weaknesses or create new forms of error, such as automation bias or confirmation bias, and the paradox that only people with expertise can reliably detect LLM errors but this expertise does not fully immunize against interaction-induced biases. The article proposes two research priorities: mutual understanding (such as integrating theory of mind in AI) and complementary team performance (evaluating outcomes of human-AI teams beyond individual performance). It advocates for deeper involvement of cognitive scientists, the development of evaluation metrics inspired by cognitive science, and robust, explainable, and trust-calibrated human-LLM interaction strategies. Ultimately, a human-centered approach, supported by policy and interdisciplinary research, is needed to achieve transparent, effective, and fair partnerships between humans and LLMs."
53,"Khalil El Gharib, Bakr Jundi, David Furfaro, and Raja-Elie E. Abdulnour, ""AI-assisted human clinical reasoning in the ICU: beyond 'to err is human',"" Frontiers in Artificial Intelligence, vol. 7, 1506676, 2024. [Online]. Available: https://www.frontiersin.org/articles/10.3389/frai.2024.1506676/full","Diagnostic errors are a major public health issue, contributing to significant morbidity and mortality, especially in ICU settings where error rates are high due to factors like cognitive load, patient complexity, and clinician burnout increasing the risk of cognitive biases. Traditional methods such as checklists and reflective strategies have limited effectiveness, and the advent of large language models (LLMs) like GPT-4 offers a new avenue for enhancing clinical reasoning. These models, trained on expansive medical datasets, have demonstrated utility in synthesizing patient information, generating relevant differential diagnoses, and recommending timely diagnostic and management steps. In a case study of a critically ill patient ultimately diagnosed with Listeria monocytogenes infection, GPT-4 recommended earlier lumbar puncture and broad-spectrum antimicrobials than occurred in real clinical practice, underscoring potential improvements in diagnostic timeliness and accuracy. LLMs can also help clinicians by summarizing complex histories, highlighting diagnostic anchoring biases, and serving as an educational resource, though limitations remain, including risks of biases, hallucinations, and ethical concerns. Acknowledging these caveats, the integration of LLMs into ICU workflows, with proper physician education and robust regulatory oversight, could transform diagnostic processes and reduce errors, as shown in this proof-of-concept case; further research is required to rigorously assess the safety and efficacy of such AI tools in clinical care."
54,"Pavel Prudkov, ""On the construction of artificial general intelligence based on the correspondence between goals and means,"" Frontiers in Artificial Intelligence, vol. 8, 1588726, 2025. [Online]. Available: https://www.frontiersin.org/articles/10.3389/frai.2025.1588726/full","The paper argues that achieving artificial general intelligence (AGI) depends on the principle of goals-means correspondence, which requires an intelligent agent's goals and the means to achieve them to be aligned. Conventional AI architectures either fix both goals and means at an agent's creation or define them independently, but both approaches fail to adequately model human goal-directed behavior. Through formal analysis, the author introduces a novel architecture where arbitrary goals and means are constructed jointly, guided by the criterion of minimal construction costs. This flexible approach, considered to mirror human goal-directed processes, results in agents whose structure can evolve similarly to human development. A simple agent model using this architecture is described: the agent, navigating a quadrangular field with distracting stimuli, escapes traps by restructuring its goals and means, thereby demonstrating the model's adaptive potential for AGI beyond traditional rigid systems."
55,"Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah, ""From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review,"" arXiv preprint arXiv:2504.19678, 2025. [Online]. Available: https://arxiv.org/abs/2504.19678","Large language models and autonomous AI agents have proliferated alongside a host of new evaluation benchmarks, frameworks, and collaboration protocols, but this ecosystem remains fragmented without a unified taxonomy or comprehensive review. This work presents a comparative analysis of benchmarks from 2019 to 2025 that assess large language models and agents across domains including general and academic knowledge reasoning, mathematical problem solving, code generation, factual retrieval, domain-specific applications, multimodal and embodied tasks, orchestration, and interactive evaluations. The authors introduce a taxonomy comprising roughly 60 benchmarks and review agent frameworks developed from 2023 to 2025, which empower autonomous decision-making and multi-step reasoning by combining language models with modular toolkits. They also highlight real-world applications of autonomous agents in fields such as materials science, biomedicine, software engineering, synthetic data generation, chemistry, mathematics, GIS, multimedia, healthcare, and finance. The survey covers major agent collaboration protocolsACP, MCP, and A2Aand provides guidance for future research, stressing advanced reasoning strategies, failure modes in multi-agent systems, automated scientific discovery, dynamic tool integration via reinforcement learning, improved search capabilities, and security concerns in agent protocols."
56,"Dibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal, ""Thinking Machines: A Survey of LLM based Reasoning Strategies,"" arXiv preprint arXiv:2503.10814, 2025. [Online]. Available: https://arxiv.org/abs/2503.10814","Large Language Models (LLMs) have achieved remarkable proficiency in language tasks, prompting their consideration as potential steps toward Artificial General Intelligence, but there remains a notable gap between language proficiency and genuine reasoning ability. This survey paper provides a systematic overview of techniques for endowing LLMs with enhanced reasoning, covering approaches based on reinforcement learning (e.g., verbal and reward-based reinforcement, planning with Monte Carlo Tree Search), test-time compute strategies (such as chain-of-thought, graph-of-thought, and feedback-guided refinement), and self-training methods (including fine-tuning on curated data, rejection sampling, and preference optimization). The authors classify and discuss each strategy, noting that reinforcement learning methods can be powerful but demand strong reward modeling and significant computation; test-time compute scaling can yield improvement without retraining but depends on robust pre-training; and self-training methods provide flexible pathways to improved reasoning but can be resource intensive. Challenges include automating process supervision, managing computational demands (notably overthinking and vast search spaces in methods like MCTS), the high cost of stepwise annotation, and the reliance of inference-time strategies on well-pretrained models. The surveys taxonomy and discussion serve as an accessible entry-point to this rapidly evolving field, emphasizing conceptual clarity and top-down organization while pointing to comprehensive references for deeper exploration."
57,"DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, et al., ""DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,"" arXiv preprint arXiv:2501.12948, 2025. [Online]. Available: https://arxiv.org/abs/2501.12948","The paper introduces DeepSeek-R1-Zero and DeepSeek-R1, two large language models (LLMs) focused on advancing reasoning capabilities using large-scale reinforcement learning (RL) rather than traditional supervised fine-tuning (SFT). DeepSeek-R1-Zero is trained solely with RL, directly incentivizing reasoning but facing challenges such as poor readability and language mixing. DeepSeek-R1 improves upon this by incorporating thousands of curated Chain-of-Thought (CoT) examples as a cold start prior to RL, enhancing both performance and output clarity. The models employ Group Relative Policy Optimization (GRPO), a variant of RL, and use structured, rule-based rewards (accuracy and format) instead of neural reward models to avoid pitfalls such as reward hacking. Distillation from DeepSeek-R1 to smaller dense models (1.5B70B parameters) yields state-of-the-art (SOTA) performance for open models at respective sizes. On benchmarks, DeepSeek-R1 achieves 97.3% Pass@1 on MATH-500, 79.8% on AIME 2024 (outperforming OpenAI-o1-1217), and demonstrates superior code reasoning with an Elo of 2029 on Codeforces, surpassing 96.3% of human competitors. Distilled models like Qwen-32B retain strong performance (AIME 72.6%, MATH-500 94.3%). Attempts with Process Reward Models (PRM) or Monte Carlo Tree Search (MCTS) were less successful at scale, and direct RL on small models underperformed compared to distillation. Challenges include language mixing, prompt sensitivity, and coverage gaps in software engineering benchmarks. The authors open-source DeepSeek-R1-Zero, DeepSeek-R1, and all distilled models, and future work will address broader language coverage, more efficient RL for software tasks, and general capabilities such as structured output, multi-turn dialogue, and improved prompting strategies."
58,"Tobias Hille, Maximilian Stubbemann, Tom Hanika, ""Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research,"" Transactions on Machine Learning Research, vol. 2024, Oct. 2024. Available: https://openreview.net/forum?id=vCb_76qX4S","Difficulties in replication and reproducibility of empirical evidence have become prominent concerns in machine learning research, motivating the need for reliable and open scientific practices. This paper introduces an ontology of reproducibility aimed specifically at evaluating how well graph neural network research supports reproducibility, considering factors like experimental workflow and accessibility of resources. The study applies this ontology to analyze sources that may affect model performance, with a focus on controlling for hidden effects. In addition, it addresses the challenge posed by the curse of dimensionalitywhich complicates data collection, representation, and analysisand explores the influence of a dataset's geometric intrinsic dimension on machine learning models. This dual focus aims to promote more robust, generalizable results and advance best practices in the field."
59,"Georgios Sidiropoulos, Samarth Bhargav, Panagiotis Eustratiadis, Evangelos Kanoulas, ""Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup,"" Transactions on Machine Learning Research, vol. 2025, Jan. 2025. Available: https://openreview.net/forum?id=rHmc5Y6ICg","This study investigates the effectiveness and efficiency trade-offs of multiview representation learning (MRLR) for dense retrieval under memory-limited academic conditions, revisiting the online MRLR method by Ma et al. (2023). While dense retrieval traditionally uses neural language model encodings $\theta_q(\cdot)$ and $\theta_d(\cdot)$ to map queries and documents into vectors (e.g., via the [CLS] token of a transformer with ranking based on $\theta_q(q)^\top \theta_d(d)$), MRLR enriches retrieval by incorporating additional contextual views at the cost of greater memory consumption. By reproducing MRLR with varying memory budgets, retriever architectures (BERT-base, BERT-large, ELECTRA), and evaluating on MS MARCO and ANTIQUE datasets (using recall@100 and NDCG@100), the study finds substantial effectiveness gains with MRLR: for instance, recall@100 improves from 95.6 (baseline DPR) to 97.3 (MRLR full-memory) and NDCG@100 from 77.7 to 79.5. These gains persist but diminish as memory is restricted (e.g., at 10M-vector budget, recall@100 is 97.1 and NDCG@100 is 79.2). The results underscore that multiview learning offers measurable improvement over single-view baselines, even with limited resources, although its advantage fades under extreme constraints. The work highlights the need for further research on memory-efficient multiview approaches and provides openly available code to facilitate reproducibility."
60,"Jose L. Garcia, Karolina Hajkova, Maria Marchenko, Carlos Miguel Patiño, ""Reproducibility Study of 'Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation',"" Transactions on Machine Learning Research, vol. 2025, Apr. 2025. Available: https://openreview.net/forum?id=yYb8lvT0KJ","This paper reproduces and extends findings from a negotiation benchmark for Large Language Models (LLMs), evaluating both open-weight (1.5B70B parameters) and proprietary models like GPT-4o Mini. The study shows that smaller models (<10B) struggle with format adherence and coherence, while larger models match proprietary model performance. The benchmark, built on a multi-agent, multi-issue negotiation game inspired by HarborCo, ultimately tests an agent's capability to choose acceptable deals from context rather than genuine negotiation or inter-agent communication. The introduction of a communication-free baseline demonstrates that single-agent strategies can match or exceed multi-agent approaches, challenging the core assumption that agent communication is necessary for optimal performance. Key new metricsincluding structural information leakage (failure rates increase with smaller models) and fairness (using a Gini coefficient-based inequality metric)highlight exploitation of prompt structures and flaws disadvantaging small models. Across all scenarios, acceptable negotiation outcomes are consistently located on the Pareto front, but task optimization effectively ends once these are reached. Prompt design (notably Chain-of-Thought) significantly influences results. The paper advocates for future benchmarks to decouple negotiation quality from prompt sensitivity, properly require multi-agent cooperation, and utilize more robust, less format-dependent evaluations, with implications for fairness, accessibility, and privacy in LLM-based systems."
61,"Nick McGreivy, Ammar Hakim, ""Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations,"" Nature Machine Intelligence, vol. 6, no. 10, pp. 1256–1269, 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00897-5","This paper systematically reviews the literature on machine learning (ML) methods for solving partial differential equations (PDEs), particularly in fluid mechanics, and finds significant reproducibility issues akin to those observed in other scientific fields. The authors report that 79% of articles they surveyed compare ML-based PDE solvers against weak numerical baselines, leading to overoptimistic assessments of ML performance. They identify widespread reporting biases, such as underreporting negative results, and attribute these problems to researcher degrees of freedom and incentive structures that favor positive findings. Despite the promise of ML-based acceleration for PDEs in applications like optimization and uncertainty quantification, the current literature is not a reliable indicator of actual progress due to poor benchmarking and lack of transparency. The study recommends both bottom-up cultural changes (e.g., publishing negative results, transparent benchmarking) and top-down institutional reforms to accurately measure future advances of ML for PDEs. Limitations noted include the scope (focusing on forward problems in fluid mechanics) and lack of absolute proof for the pervasiveness of reporting bias, though evidence is strong. Improvements in research practice are suggested to ensure that ML's real contributions to PDE solving are fairly assessed."
62,"Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas, AIX-COVNET, James H. F. Rudd, Evis Sala, Carola-Bibiane Schönlieb, ""Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans,"" Nature Machine Intelligence, vol. 3, no. 3, pp. 199–217, 2021. [Online]. Available: https://www.nature.com/articles/s42256-021-00307-0","This systematic review rigorously examines all published and preprint machine learning models for COVID-19 diagnosis and prognosis using chest radiographs (CXR) and CT images from 1 January to 3 October 2020. Out of 2,212 initially identified studies, only 62 met stringent methodological and reporting quality criteria. The review highlights that, despite significant international efforts, none of the proposed models are ready for clinical use due to methodological flaws, insufficient dataset quality, poor reproducibility, and high or unclear risk of biasparticularly in papers using public datasets, which often suffered from duplication, lack of demographic information, substandard image formats, and 'Frankenstein' data assembly. Frequent issues included inadequate documentation of model selection, image pre-processing, and training setup (noted in 4961% of excluded deep learning papers), as well as lack of external validation, robustness analysis, and reporting of statistical confidence in accepted studies. The authors emphasize that higher-quality, better-documented, and externally validated datasets, along with robust methodological standards and clinical collaboration, are critical for producing clinically useful AI tools. They provide targeted recommendations for data curators, algorithm developers, authors, and reviewers to address prevalent biases and reproducibility gaps, stressing that no current models are suitable for translation into clinical practice without these improvements."
63,"Shushan Toneyan, Ziqi Tang, Peter K. Koo, ""Evaluating deep learning for predicting epigenomic profiles,"" Nature Machine Intelligence, vol. 4, no. 12, pp. 1088–1100, 2022. [Online]. Available: https://www.nature.com/articles/s42256-022-00570-9","Deep learning models have shown promise in predicting epigenomic profiles from DNA sequences, but varying modeling choices and evaluation strategies have complicated fair comparison of new architectural innovations, particularly for regulatory genomics tasks. This work introduces GOPHER, a unified Python-based evaluation framework designed to systematically benchmark deep learning models trained on chromatin accessibility data, irrespective of whether the prediction task is framed as binary classification (using peak callers) or quantitative regression (directly predicting read coverage). The study compares a range of architectures (including Basenji, BPNet, and novel CNN variants) and highlights critical design choices: quantitative regression models typically generalize and interpret biological data better than binary models, especially across whole chromosomes and for out-of-distribution (variant effect) predictions. Optimal performance is influenced by loss function (Poisson NLL preferred), data augmentations (random shift and reverse-complement), architectural improvements (dilated residual blocks and exponential activations), training on broad coverage-thresholded datasets rather than peak-centered ones, and robust averaging over input translations. Beyond prediction accuracy, the framework introduces a robustness metric quantifying model consistency under input sequence perturbations, which correlates with reliable variant-effect prediction. Model interpretability is enhanced in quantitative settings, with attribution and motif analyses uncovering how the models learn sequence features and motif interactions. While binary models can approach quantitative performance on restricted test sets, they are less robust and generalizable genome-wide. The study recommends broad evaluation protocols, robust and interpretable modeling choices, and extensibility for other omics data or architectures in future work. Overall, GOPHER enables unbiased, comprehensive assessment of new regulatory genomics models, promoting both methodological and biological insights."
64,"Wei Li, Yu Liu, Yuhong Guo, L. P. Chau, Zhanyu Ma, ""LibFewShot: A Comprehensive Library for Few-Shot Learning,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 3, pp. 2959-2976, 2024. [https://ieeexplore.ieee.org/document/10239698/](https://ieeexplore.ieee.org/document/10239698/)","Few-shot image classification has advanced rapidly, but fair comparison is hampered by varying methodologies and the use of different architectures and augmentation tricks across studies. This paper introduces LibFewShot, a unified PyTorch codebase re-implementing eighteen state-of-the-art few-shot learning methods for fair and reproducible evaluation. By systematically benchmarking with diverse backbones and training strategies, the study reveals that techniques such as data augmentation, pre-training, knowledge distillation, and self-supervision significantly impact few-shot learning performance. Importantly, the results affirm that the meta- or episodic-training mechanism remains necessary, particularly when integrated with pre-training. LibFewShot aims to lower the entry barrier for newcomers and clarify the contribution of learning tricks, providing a valuable foundation for future research; the full library is openly available."
65,"N. Ravi, A. Goel, J. C. Davis, and G. K. Thiruvathukal, ""Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis,"" arXiv preprint arXiv:2505.03165, 2025. [Online]. Available: https://arxiv.org/abs/2505.03165","The field of deep learning has advanced rapidly, transforming software capabilities across numerous applications, but these developments are accompanied by growing issues with reproducibility, a cornerstone of reliability and validity in software development. Factors that impede reproducibility include discrepancies in execution environments, incompatible libraries, proprietary or unavailable data and source code, opaque methodologies, and inherent stochasticity. A Nature journal study found that over 70% of researchers could not reproduce others' experiments, while more than 50% failed to reproduce their own. This paper addresses these concerns by presenting systematic guidelines to assess and enhance the reproducibility of deep learning models through a case study. It outlines reproducibility patterns and anti-patterns, advocating for replicating original software environments, implementing comprehensive training and testing processes, transparently disclosing model architecture, and clarifying data pipelines. Additionally, the authors conduct sensitivity analyses to evaluate model performance under varying conditions, ultimately aiming to bridge the gap between deep learning research and practical deployment to ensure reliable, reproducible innovations in software."
66,"H. Semmelrock, T. Ross-Hellauer, S. Kopeinik, D. Theiler, A. Haberl, S. Thalmann, and D. Kowald, ""Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers,"" arXiv preprint arXiv:2406.14325, 2025, Accepted for publication in AI Magazine. [Online]. Available: https://arxiv.org/abs/2406.14325","Many research fields, including those involving Machine Learning (ML), are facing significant reproducibility issues, with concerns labeled by some as a ""crisis."" Problems such as lack of transparency, missing data or code, poor adherence to scientific standards, and the sensitivity of ML training conditions hinder reproducibilityoften making it unachievable even in principle. When attempts at reproduction are possible, they often yield results that differ markedly from the originals. Despite past calls for improved practices and the introduction of initiatives (like reproducibility tracks at conferences and the ACM's Emerging Interest Group), the authors argue that the ML community continues to underestimate the seriousness of the situation, endangering both trust and research integrity. This article presents a new perspective on the major procedural and technical barriers and drivers influencing reproducibility across methods, code, data, and experimental levels. By mapping drivers to barriers, the authors offer concrete strategies for researchers, highlight urgent areas for further investigation, and aim to stimulate deeper discussion on this critical threat to scientific progress."
67,"E. Raff, M. Benaroch, S. Samtani, and A. L. Farris, ""What Do Machine Learning Researchers Mean by 'Reproducible'?"", arXiv preprint arXiv:2412.03854, 2024, To appear in AAAI 2025, Senior Member Presentation Track. [Online]. Available: https://arxiv.org/abs/2412.03854","The paper addresses the growing concern over a ""reproducibility crisis"" in Artificial Intelligence (AI) and Machine Learning (ML), noting that the term ""reproducibility"" is inconsistently defined and often conflated with other aspects of scientific rigor. By surveying 101 papers published since 2017, the authors categorize research into eight rigor types: repeatability, reproducibility, replicability, adaptability, model selection, label/data quality, meta & incentive, and maintainability. These categories, detailed in Table 1, summarize the focus and proportion of the literature, for example: repeatability (12.9%), reproducibility (16.8%), replicability (15.8%), adaptability (4.0%), model selection (19.8%), label/data quality (4.0%), meta & incentive (13.9%), and maintainability (12.9%)with the proportions given by:

\[
\begin{tabular}{l|c}
Aspect & \% of Literature \\
\hline
Repeatability & 12.9 \\
Reproducibility & 16.8 \\
Replicability & 15.8 \\
Adaptability & 4.0 \\
Model Selection & 19.8 \\
Label/Data Quality & 4.0 \\
Meta \& Incentive & 13.9 \\
Maintainability & 12.9 \\
\end{tabular}
\]

The paper emphasizes distinct challenges for each rigor type, such as inconsistent terminology, limited incentives for rigorous research, and underexplored areas like adaptability and maintainability. The authors advocate for clearer definitions, dedicated conference tracks for these topics, and greater engagement with historical perspectives, concluding that understanding and improving these eight rigor aspects will enhance the credibility and durability of AI/ML research."
68,"L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang, ""Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 1–19, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.2.pdf","Large language models (LLMs) excel in many NLP tasks but often produce undesirable behaviors such as hallucination, unfaithful reasoning, and toxic content; to address these, self-correction techniques leveraging automated feedbackgenerated by the LLM itself, external models, or toolshave emerged as an efficient alternative to costly human feedback. This comprehensive survey develops a taxonomy based on what is corrected (e.g., factual errors, toxicity, code bugs), the source and format of feedback, and the stage of correction (training-time, generation-time, post-hoc), and reviews correction strategies such as self-training, generate-then-rank, feedback-guided decoding, iterative self-refinement, and multi-agent debate. Empirical analyses of recent studies demonstrate consistent gains in factual accuracy, faithfulness, and error reduction across all correction stages, with effectiveness dependent on feedback quality and model refinement capacity. The survey highlights challenges in theoretical understanding, quantitative measurement of self-correction, robustness of continued self-improvement, and integration with parameter editing, as well as the need to adapt these strategies for multi-modal LLMs. Future directions include deepening theoretical foundations, expanding comparative evaluations, enabling lifelong learning, integrating parametric changes, and applying techniques to multi-modal models. For further reading, a curated resource list is provided (https://github.com/teacherpeterpan/self-correction-llm-papers)."
69,"T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, ""Benchmarking Large Language Models for News Summarization,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 45–62, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.4.pdf","Large language models (LLMs) have demonstrated strong performance in automatic summarization; however, the mechanisms behind this success remain unclear. Through human evaluations on ten LLMs differing in pretraining, prompting, and scale, the study finds that instruction tuning, rather than size, primarily drives effective zero-shot summarization. Previous benchmarks may have underestimated LLM and human capabilities due to reliance on lower-quality reference summaries. By collecting and evaluating high-quality human summaries, the authors show that LLM-generated summariesdespite stylistic differences like paraphrasingare rated as comparably effective to those written by humans."
70,"N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ""Lost in the Middle: How Language Models Use Long Contexts,"" Transactions of the Association for Computational Linguistics, vol. 12, pp. 103–117, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.10.pdf","While modern language models can process lengthy input contexts, their effectiveness at utilizing information within these long contexts is limited. Performance on tasks such as multi-document question answering and key-value retrieval drops significantly when relevant information shifts position, particularly when it appears in the middle rather than at the beginning or end of the input. This finding holds even for models designed for long contexts, highlighting weaknesses in how current models make use of extended inputs and motivating the need for improved evaluation protocols for future developments."
71,"J. Tsujii, ""Natural Language Processing and Computational Linguistics,"" Computational Linguistics, vol. 47, no. 4, pp. 707–727, Dec. 2021. [Online]. Available: https://aclanthology.org/2021.cl-4.24/","Research in natural language processing (NLP) is significantly limited by the resources and technologies currently available, especially when compared to the possibilities envisioned in theoretical work. This restriction impacts the progress and scope of NLP applications and methodologies."
72,"M. Garcia, ""Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning,"" Computational Linguistics, vol. 47, no. 3, pp. 699–701, Nov. 2021. [Online]. Available: https://aclanthology.org/2021.cl-3.22/","Embeddings are a transformative development in Natural Language Processing (NLP), enabling information to be encoded into low-dimensional vector representations, which has powered a wide range of advancements and applications throughout the field."
73,"G. Chrysostomou, ""Explainable Natural Language Processing,"" Computational Linguistics, vol. 48, no. 4, pp. 1137–1139, Dec. 2022. [Online]. Available: https://aclanthology.org/2022.cl-4.22/","This is a book review of ""Explainable Natural Language Processing"" by Anders Sgaard, reviewed by George Chrysostomou (University of Sheffield). The book, published by Morgan & Claypool (Synthesis Lectures on Human Language Technologies, volume 51), 2021, explores definition and categorization of explanations in NLP."
74,"Haijie Ding and Xiaolong Xu, ""SAN-T2T: An automated table-to-text generator based on selective attention network,"" Natural Language Engineering, vol. 30, no. 3, pp. 429-453, May 2024. [Online]. Available: https://www.cambridge.org/core/journals/natural-language-engineering/article/sant2t-an-automated-tabletotext-generator-based-on-selective-attention-network/20AA8938239332A0E6C8884DA8329D82","Table-to-text generation converts structured data into descriptive natural language and is crucial for applications like question-answering and search engines. Traditional neural language model methods relying on attention often struggle with long texts and insufficient utilization of table structure. This paper introduces SAN-T2T, a new generative model featuring a field-content selective encoder, a descriptive decoder, and a selective attention network, which incorporates table structure directly into field representations. A content selector with self-aligned gates allows mutual importance determination among records, thus improving encoding. In decoding, enhanced semantic alignment and a featured copy mechanism address rare word generation. Experimental results on WikiBio and WeatherGov datasets demonstrate that SAN-T2T achieves significant performance gains over strong baselines, validating the effectiveness of its content selector."
75,"Xin Shen, Wai Lam, Shumin Ma, and Huadong Wang, ""Joint learning of text alignment and abstractive summarization for long documents via unbalanced optimal transport,"" Natural Language Engineering, vol. 30, no. 3, pp. 525-553, May 2024. [Online]. Available: https://www.cambridge.org/core/journals/natural-language-engineering/article/joint-learning-of-text-alignment-and-abstractive-summarization-for-long-documents-via-unbalanced-optimal-transport/46EF85C92B3E4158D89DC2C43E55D621","This paper introduces UOTSum, a novel framework for abstractive summarization of long documents that jointly learns to align salient document text spans to summary sentences and generates the summary. The key innovation is using unbalanced optimal transport (UOT) to compute a soft alignment matrix $T \in \mathbb{R}^{n \times m}$ between document and summary representations, addressing the common issue where large portions of the document are not covered in the summary (and vice versa). The alignment cost is $C_{ij} = \text{dist}(f(x_i), g(y_j))$, and the solution minimizes
\[
\min_{T \geq 0} \sum_{i,j} T_{ij} C_{ij} + \epsilon \cdot \text{KL}(T1 \mid p) + \epsilon \cdot \text{KL}(T^\top 1 \mid q)
\]
where $p$ and $q$ are marginal distributions and $\epsilon$ is a relaxation parameter. The model combines this alignment with a summarization network in a joint learning objective that encourages faithfulness and informativeness. Experiments on benchmarks (CNN/Daily Mail, Reddit TIFU, PubMed) show UOTSum substantially outperforms prior state-of-the-art methods, as summarized in the table:
\[
\begin{tabular}{lccc}
\hline
Model & R-1 & R-2 & R-L \\
\hline
Lead-3 & 40.3 & 17.7 & 36.7 \\
Pointer-Generator & 41.2 & 18.0 & 37.8 \\
BERTSUMEXTABS & 42.1 & 19.2 & 38.6 \\
Longformer-Encoder & 43.6 & 20.0 & 39.9 \\
UOTSum (Ours) & \textbf{44.7} & \textbf{21.3} & \textbf{41.0} \\
\hline
\end{tabular}
\]
Human evaluations also confirm improvements in faithfulness and coverage. The approach is computationally costlier, and careful hyperparameter tuning is required, but UOTSum offers a new, scalable direction for faithful, informative, and controllable long-document summarization."
76,"Figen Beken Fikri, Kemal Oflazer, and Berrin Yanıkoğlu, ""Abstractive summarization with deep reinforcement learning using semantic similarity rewards,"" Natural Language Engineering, vol. 30, no. 3, pp. 554-576, Oct. 2023. [Online]. Available: https://www.cambridge.org/core/journals/natural-language-engineering/article/abstractive-summarization-with-deep-reinforcement-learning-using-semantic-similarity-rewards/5A2F74A2BF5FE5AB80206C772E6B7B5B","Abstractive summarization generates new sentences rather than extracting them directly from source documents. This paper tackles two central challenges: effective evaluation of summarization models and defining an optimal training objective. The authors propose novel evaluation metrics based on semantic similarity, leveraging a fine-tuned BERTurk model with either cross-encoder or bi-encoder setups and fine-tuned on Turkish Natural Language Inference and Semantic Textual Similarity datasets. Their metrics exhibit stronger correlations with human judgments compared to traditional ROUGE and BERTScore measures. Additionally, they introduce a deep reinforcement learning approach using these semantic metrics as part of a mixed training objective, demonstrating that optimizing for their combined objective yields summaries with improved semantic similarity and human readability relative to training with a pure maximum likelihood objective."
77,"Bin Wang and C.-C. Jay Kuo, ""SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2146-2157, 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9140343","This paper introduces SBERT-WK, a novel, training-free approach to generating high-quality sentence embeddings from BERT-based word models by leveraging the geometric structure and layer-wise evolution of contextualized word representations. Instead of relying on simple pooling or additional supervised training, SBERT-WK computes word representations across all BERT layers, then applies geometric projections to measure a word's novelty and alignment within its context. The method integrates these novelty measures to form weighted combinations across layers for each word and uses principal component analysis to assign greater importance to unique word contributions when forming the sentence embedding. Experimental results show that SBERT-WK achieves state-of-the-art performance on multiple semantic textual similarity datasets, outperforms or matches leading baselines across supervised GLUE tasks, and demonstrates strong generalization in ten probing tasks probing diverse linguistic properties. Computationally, SBERT-WK requires only a standard BERT forward pass and lightweight geometric calculations, resulting in efficiency comparable to pooling-based methods. While choices of window size and layers may require tuning, and geometric operations are slightly more complex than pooling alone, these factors are insignificant relative to BERT's inference cost. This geometric, unsupervised framework can be further generalized to other transformer models and potentially benefit from deeper geometric analysis or integration with transfer learning. The code is made publicly available, supporting reproducibility and future research. Key equations in the method include the novelty score of each word, computed as the norm of its orthogonal projection relative to contextual neighbors: $n_{l,t} = \|h^l_t - P_{\mathcal{C}_{l,t}}(h^l_t)\|$, where $P_{\mathcal{C}_{l,t}}$ denotes the projection onto the neighborhood subspace, and the final embedding is a weighted average of contextually informed word vectors."
78,"Yinhe Zheng, Guanyi Chen, and Minlie Huang, ""Out-of-Domain Detection for Natural Language Understanding in Dialog Systems,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 1198–1207, 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9052492","This paper addresses Out-of-Domain (OOD) detection for Natural Language Understanding (NLU) in dialogue systems by proposing a novel approach that generates high-quality pseudo OOD utterances without the need for manually labeled OOD data. The core method leverages a GAN-regularized autoencoder that maps in-domain utterances to a latent space, employing a generator to produce latent codes for pseudo OOD samples which a discriminator tries to distinguish from true in-domain codes; an auxiliary classifier regularizes these samples so they lack clear intent assignability. The models loss integrates cross-entropy and entropy regularization: $L = L_\mathrm{CE} + \lambda \cdot H(p(y|x))$, where $H$ denotes entropy and $\lambda$ regulates its influence. Experiments conducted on ATIS, SNIPS, and CLINC150 datasets show that augmenting training with these pseudo OOD utterances leads to consistent improvements in OOD detection accuracy, AUROC, and FPR95 compared to baseline approaches. Further gains are realized by incorporating unlabeled data (even from the web), enhancing robustness especially for hard OOD cases. Visualization confirms that generated pseudo OODs cluster near decision boundaries in latent space, challenging classifiers. The work highlights the methods scalability, generalizability, and significant performance gains, while noting that the diversity of generated OODs depends on the generators power and that careful management of unlabeled data is critical. Future directions include automating OOD data extraction from large corpora, improving generator architectures, and enabling online adaptation."
79,"Xuenan Xu, Ziliang Xie, Mengyue Wu, and Kai Yu, ""Beyond the Status Quo: A Contemporary Survey of Multi-View Learning in Speech and Language Processing,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 95–112, 2023. [Online]. Available: https://ieeexplore.ieee.org/document/10285526","This paper provides a comprehensive survey of multi-view learning in speech and language processing, highlighting how leveraging multiple data sources or diverse representations enhances model performance and generalization compared to traditional single-view methods. The survey systematically covers core concepts such as co-training, canonical correlation analysis, autoencoder-based methods, and contrastive representation learning, relating them to applications like automatic speech recognition, speaker identification, and emotion recognition. The authors present foundational theories, introduce a taxonomy of methods, and critically discuss the motivations, methodologies, applications, and ongoing challenges in the field, with the goal of inspiring further research and broader adoption of multi-view learning across speech and language tasks."
80,"Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou, ""Hierarchical Document Refinement for Long-context Retrieval-augmented Generation,"" arXiv preprint arXiv:2505.10413, 2025. [Online]. Available: https://arxiv.org/abs/2505.10413","LongRefiner is an efficient, plug-and-play document refinement system designed to enhance Retrieval-Augmented Generation (RAG) applications confronting long, noisy input contexts. Addressing challenges of low signal-to-noise ratio and high computational overhead in lengthy documents, LongRefiner employs a hierarchical document representation (doc tree using XML-inspired tags), dual-level query analysis distinguishing between local and global needs, and adaptive node selection through multi-task LoRA learning, all within a single foundation model. Joint training tasks (query classification, document structuring, global selection) enable adaptive compression, outputting concise, information-dense representations while respecting a strict token budget. Across seven QA datasetsincluding NQ, TriviaQA, HotpotQA, and ELI5LongRefiner outperforms baselines under a 2k token constraint, achieving up to 10x lower computational costs and 4x lower latency without sacrificing accuracy (performance increases with document length). Ablation reveals hierarchical structuring as most critical, and leveraging strong rerankers for local scoring optimizes accuracy. While currently tailored for plain text and the Wikipedia corpus (with some generalization to PopQA), future directions include extending support for structured/non-textual data and domain adaptation. Limitations involve handling tables/images and potential generalizability. The proposed framework, supported by open-sourced code, demonstrates that integrating structural and semantic refinement mechanisms can unlock scalable, high-precision RAG in real-world settings."
81,"Yue Guo, Jae Ho Sohn, Gondy Leroy, and Trevor Cohen, ""Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation,"" arXiv preprint arXiv:2505.10409, 2025. [Online]. Available: https://arxiv.org/abs/2505.10409","This study evaluates the understandability of large language model (LLM)-generated plain language summaries (PLSs) in medical contexts using a large-scale, crowdsourced approach. While previous assessments have relied mainly on automated scores or limited, subjective human ratings, the authors constructed a dataset pairing human-written and various GPT-4-generated PLSs (optimized for factors like informativeness, coherence, simplification, etc.) with robust human evaluationsincluding Likert-scale ratings and objective comprehension measures (e.g., multiple-choice questions, recall tasks), gathered from 150 lay participants via Amazon Mechanical Turk. Results show that LLM-generated summaries appear similar to human ones on subjective metrics (simplicity, informativeness, coherence, faithfulness), but actually support significantly lower comprehension as measured by objective tests. Automated metrics such as ROUGE, BLEU, and METEOR failed to correlate with human comprehension, with only the QA-based QAEval showing significant alignment. Faithfulness in subjective ratings was the strongest predictor of comprehension, and inclusion of background information improved outcomes. Additionally, participant engagement and demographic factors affected comprehension, with excessive self-identification as ""expert"" correlating with lower MCQ scores. The findings highlight a substantial gap between the apparent and actual utility of LLM-generated PLSs for lay readers, indicating that evaluation must prioritize objective, comprehension-focused metrics. The authors recommend future development of both generation techniques and evaluation tools that directly optimize for and assess layperson understanding, rather than relying on surface-level or purely stylistic criteria."
82,"Michael Fire, Yitzhak Elbazis, Adi Wasenstein, and Lior Rokach, ""Dark LLMs: The Growing Threat of Unaligned AI Models,"" arXiv preprint arXiv:2505.10066, 2025. [Online]. Available: https://arxiv.org/abs/2505.10066","Large Language Models (LLMs) have rapidly become integral to various sectors, but their susceptibility to jailbreakinga technique where adversarial prompts bypass safety mechanismsposes significant risks, especially as LLMs sometimes learn harmful patterns from unfiltered training data. The study reveals that both commercial and open-source LLMs, including state-of-the-art systems, remain vulnerable to these universal jailbreak attacks, even when such exploits have been public for months, enabling them to produce detailed responses to illicit and harmful requests. Despite responsible disclosure, major LLM providers were unresponsive or inadequately addressed the vulnerabilities, signaling gaps in current industry safety practices. The proliferation of unaligned dark LLMs like WormGPT and FraudGPT, which are openly marketed as lacking ethical guardrails and assist in cybercrime, exacerbates the problem as training becomes cheaper and models more accessible. The authors advocate for decisive interventions including proactive training data curation, LLM firewalls, machine unlearning, continuous adversarial testing, and greater public awareness, warning that without immediate action, the same AI tools that empower innovation will also democratize dangerous knowledge, presenting a clear and present threat."
83,"J. Kauffmann, M. Esders, L. Ruff, G. Montavon, K.-R. Müller, W. Samek, ""From Clustering to Cluster Explanations via Neural Networks,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 35, no. 2, pp. 1926-1940, Feb. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/9817459/","A novel framework is introduced to address the lack of interpretability in clustering models within the realm of Explainable AI (XAI), extending XAI beyond supervised learning to unsupervised settings where label information is absent. The key insight is that clustering models can be reformulated as neural networks, or 'neuralized', enabling efficient and accurate attribution of cluster assignments to input features. This approach facilitates explanations for why data points are assigned to specific clusters and allows for the assessment of cluster quality and extraction of new insights from the analyzed data and learned representations."
84,"Y. Ge, Y. Xiao, Z. Xu, M. Zheng, S. Karanam, T. Chen, L. Itti, Z. Wu, ""A Peek Into the Reasoning of Neural Networks: Interpreting With Structural Visual Concepts,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 1, pp. 121-135, Jan. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9146584/","Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While recent developments in explainable artificial intelligence attempt to bridge this gap (e.g., by visualizing the correlation between input pixels and final outputs), these approaches are limited to explaining low-level relationships, and crucially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classification NNs with intuitive structural visual concepts. Given a trained classification model, the proposed VRX extracts relevant class-specific visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions. With extensive experiments, we empirically show VRX can meaningfully answer ""why"" and ""why not"" questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN's performance."
85,"W. Li, S. Zhang, L. Lei, H. Liu, Z. Liu, J. Li, ""Learning Deep Generative Clustering via Mutual Information Maximization,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 9, pp. 6263-6277, Sep. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/9669125/","Deep clustering involves simultaneously learning feature representations and performing clustering via deep neural networks, but current approaches either lack cluster-specific representations (discriminative models) or suffer from optimization difficulties (generative models). This paper provides a theoretical proof that maximizing mutual information effectively separates clusters in data space. Building on this insight, the authors propose a model that combines a hierarchical generative adversarial network with mutual information maximization and adds an auxiliary discriminative regularization term to enhance representation compactness and facilitate training. Experimental results across several public datasets show that this method yields state-of-the-art clustering performance."
86,"P. Veličković and C. Blundell, ""Neural algorithmic reasoning,"" Patterns, vol. 2, no. 7, p. 100273, 2021. doi:10.1016/j.patter.2021.100273. Available: https://doi.org/10.1016/j.patter.2021.100273","Neural network architectures have achieved remarkable success across a wide range of problem domains. However, in domains with deeply structured combinatorial rules (such as arithmetic, logic, or symbolic reasoning) the gap with algorithmic solutions remains significant. Recent works have suggested that such rules can be captured and executed through the careful design of neural network architectures, often drawing inspiration from experience with traditional algorithms, especially graph algorithms. This article reviews the emerging line of research at the intersection of neural net architectures and algorithmic reasoning, with particular focus on the combined use of neural networks and classical algorithmic elements (such as data structures). We discuss motivations, representative models, empirical results, and open challenges as well as the potential impact of this rapidly developing field."
87,"M. Yang, Y. Wang, and Y. Gu, ""Language-based reasoning graph neural network for commonsense question answering,"" Neural Networks, vol. 181, p. 106816, Jan. 2025. doi:10.1016/j.neunet.2024.106816. Available: https://doi.org/10.1016/j.neunet.2024.106816","Language models (LMs) are central to common-sense question answering (CSQA), but simply scaling training data offers limited gains due to model parameter constraints. Introducing external knowledge via graph neural networks (GNNs) improves performance, yet integrating multiple knowledge sources and contextualizing text-knowledge interactions remains difficult. This paper introduces LBR-GNN, a Language-Based Reasoning Graph Neural Network, which represents questions, answers, and external knowledge in uniform textual form and encodes them with an LM to capture contextual information. It constructs a GNN using these language-level embeddings, employing a novel edge aggregation mechanism for selective information propagation and language-guided reasoning. Evaluated on CommonsenseQA, CommonsenseQA-IH, and OpenBookQA datasets, LBR-GNN achieves over 5% performance gains on CSQA compared to state-of-the-art models, without increasing parameter count."
88,"W. Liu, Z. Ren, and L. Chen, ""Knowledge reasoning based on graph neural networks with multi-layer top-p message passing and sparse negative sampling,"" Knowledge-Based Systems, vol. 311, p. 113063, Feb. 2025. doi:10.1016/j.knosys.2025.113063. Available: https://www.sciencedirect.com/science/article/pii/S0950705125001108",A knowledge reasoning model is proposed that utilizes graph neural networks with multi-layer top-p message passing and sparse negative sampling.
89,"Di Chen, Yiwei Bai, Sebastian Ament, Wenting Zhao, Dan Guevarra, Lan Zhou, Bart Selman, R. Bruce van Dover, John M. Gregoire, and Carla P. Gomes, ""Automating crystal-structure phase mapping by combining deep learning with constraint reasoning,"" Nature Machine Intelligence, vol. 3, no. 9, pp. 812–822, 2021. [Online]. Available: https://www.nature.com/articles/s42256-021-00384-1","Crystal-structure phase mapping is a central challenge in materials science, involving the identification of crystal phases or mixtures in X-ray diffraction data. Existing algorithms perform well with a few unique, distinguishable phase mixtures but struggle with complexities such as many mixtures, variations in diffraction patterns due to alloy compositions, and high compositional dimensionalityissues that restrict high-throughput materials discovery. This paper reframes phase mapping as an unsupervised pattern demixing problem, introducing deep reasoning networks (DRNets) that integrate deep learning with constraint reasoning to encode prior scientific knowledge. DRNets require only modest, unlabeled data by leveraging thermodynamic rules that govern crystal mixtures, and feature an interpretable latent space tying domain constraints directly into neural network optimization. The approach outperforms previous methods, successfully unraveling the BiCuV oxide phase diagram and supporting the discovery of solar fuel materials. Data and code are publicly available for further research."
90,"Laura E. Suárez, Blake A. Richards, Guillaume Lajoie, and Bratislav Misic, ""Learning function from structure in neuromorphic networks,"" Nature Machine Intelligence, vol. 3, no. 9, pp. 771–786, 2021. [Online]. Available: https://www.nature.com/articles/s42256-021-00376-1","The study explores how the structural organization of the human brain's neural circuits, known as connectomes, influences cognitive function by reconstructing brain networks from in vivo diffusion-weighted imaging and modeling them as artificial neural networks via reservoir computing. The researchers train these neuromorphic networks to perform a cognitive task and find that biologically inspired architectures achieve optimal performance when exhibiting critical dynamics. Performance is closely tied to network topology, with modular arrangements of large-scale functional systems shown to be especially computationally significant. The interaction between network structure and dynamic state reveals that the same architecture can support varied learning capacities, demonstrating how brain network organization underlies and optimizes cognitive abilities, thus bridging neuroscience and artificial intelligence."
91,"Michael A. Skinnider, R. Greg Stacey, David S. Wishart, and Leonard J. Foster, ""Chemical language models enable navigation in sparsely populated chemical space,"" Nature Machine Intelligence, vol. 3, no. 9, pp. 759–770, 2021. [Online]. Available: https://www.nature.com/articles/s42256-021-00368-1","Deep generative models are powerful tools for the exploration of chemical space, enabling the on-demand generation of molecules with desired physical, chemical, or biological properties. However, these models are typically thought to require training datasets comprising hundreds of thousands, or even millions, of molecules. This perception limits the application of deep generative models in regions of chemical space populated by only a small number of examples. Here, we systematically evaluate and optimize generative models of molecules for low-data settings. We carry out a series of systematic benchmarks, training more than 5,000 deep generative models and evaluating over 2.6 billion generated molecules. We find that robust models can be learned from far fewer examples than has been widely assumed. We further identify strategies that dramatically reduce the number of molecules required to learn a model of equivalent quality, and demonstrate the application of these principles by learning models of chemical structures found in bacterial, plant, and fungal metabolomes. The structure of our experiments also allows us to benchmark the metrics used to evaluate generative models themselves. We find that many of the most widely used metrics in the field fail to capture model quality, but identify a subset of well-behaved metrics that provide a sound basis for model development. Collectively, our work provides a foundation for directly learning generative models in sparsely populated regions of chemical space."
92,"M. Bober-Irizar and S. Banerjee, ""Neural networks for abstraction and reasoning: Towards broad generalization in machines,"" arXiv preprint arXiv:2402.03507 [cs.AI], 32 pages main text, 17 pages supplementary, Feb. 2024. [Online]. Available: https://arxiv.org/abs/2402.03507","This paper investigates progress on the Abstraction & Reasoning Corpus (ARC), a challenging benchmark for broad generalization in artificial intelligence, by examining both neurosymbolic and large language model (LLM) based methods. Building on the DreamCoder framework, the authors introduce the Perceptual Abstraction and Reasoning Language (PeARL), a bespoke domain-specific language tailored for ARC tasks, and design a new recognition model that markedly outperforms previous approaches. They demonstrate that large language models, when provided with novel encoding and data augmentation strategies, can solve a distinct subset of ARC problems that are complementary to those addressed by neurosymbolic systems. Quantitative results show that while neither DreamCoder-based nor LLM-based approaches individually solve a majority of ARC tasks, their combination in an ensemble yields improved accuracy, surpassing any single system and previous state-of-the-art baselines such as Icecuber. The study analyzes error classes and identifies both the successes and limitations of each paradigm, emphasizing the persistent challenges of achieving broad generalization. The authors also release the open-source arckit library, enabling reproducible research and further progress. These findings illustrate that while modern advances provide incremental progress, ARC remains largely unsolved, motivating future work on neural program synthesis, tuning of LLMs, and more expressive perception modules, with ARC continuing to serve as a critical benchmark for human-like AI reasoning."
93,"S. Carrow, K. H. Erwin, O. Vilenskaia, P. Ram, T. Klinger, N. A. Khan, N. Makondo, and A. Gray, ""Neural Reasoning Networks: Efficient Interpretable Neural Networks With Automatic Textual Explanations,"" arXiv preprint arXiv:2410.07966 [cs.LG], Oct. 2024. [Online]. Available: https://arxiv.org/abs/2410.07966","Neural Reasoning Networks (NRN) are a novel neuro-symbolic architecture designed for tabular classification tasks, which address the interpretability limitations of traditional neural networks by utilizing modified Weighted Lukasiewicz Logic in their designeach node functioning as a differentiable logical 'And' or 'Or' operation within a PyTorch-accelerated framework. The associated R-NRN training algorithm integrates gradient descent for weight learning and a bandit-based architecture search to optimize structure, with automatic generation of concise, logically sound textual explanations for predictions. Empirical evaluation across 22 tabular datasets demonstrates that R-NRN achieves ROC AUC scores on par with leading methods (Random Forest, XGBoost, Gradient Boosted Trees), while offering 43% faster training times, approximately 100-fold reduction in model parameters, and significantly more compact and accurate explanations (mean size 31% less than NAM or RF+SHAP). Explanations generated take the form of human-interpretable rules, such as house_latitude $\geq$ 37.5 AND rooms $\geq$ 5, and a single deletion metric confirms higher correspondence between identified feature importance and predictive influence. The network is scalable thanks to efficient GPU-batching, and the entire implementation is available as an open-source PyTorch extension. Limitations currently include its focus on classification with tabular data, leaving extensions to regression and other domains as future work, alongside proposed user studies to assess explanation utility. Overall, NRN and R-NRN represent a significant step towards interpretable, high-performance neural models which can generate automatic, accurate, and compact explanations for their predictions."
94,"Q. Chen, Y. Hu, X. Peng, Q. Xie, Q. Jin, A. Gilson, M. B. Singer, X. Ai, P.-T. Lai, Z. Wang, V. K. Keloth, K. Raja, J. Huang, H. He, F. Lin, J. Du, R. Zhang, W. J. Zheng, R. A. Adelman, Z. Lu, and H. Xu, ""Benchmarking large language models for biomedical natural language processing applications and recommendations,"" Nature Communications, vol. 16, no. 1, Art. no. 3280, 2025. Available: https://www.nature.com/articles/s41467-025-56989-2","The paper systematically benchmarks four large language models (LLMs)representing GPT and LLaMA familieson 12 biomedical NLP (BioNLP) benchmarks spanning six tasks, including entity recognition, relation extraction, document classification, question answering, summarization, and text simplification. Results show that state-of-the-art (SOTA) fine-tuned models (BioBERT, PubMedBERT, BART) outperform LLMs in extraction and classification tasks, with fine-tuned models achieving macro-average score 0.65 versus best LLM ~0.51, and, for example, F1 = 0.909 on NCBI Disease versus ~0.6 for LLMs. In reasoning-focused tasks like medical QA, closed LLMs (GPT-4) outperform SOTA (e.g., MedQA accuracy: SOTA 0.42 vs. GPT-4 0.72), though at much higher computational cost (60100 that of GPT-3.5). Open LLMs (LLaMA, PMC LLaMA) require fine-tuning to approach SOTA performance, and biomedical-specific pretraining offers no substantial gains over general LLaMA. Dynamic few-shot prompting helped some tasks but inconsistently, and qualitative analysis reveals that LLMs (especially zero-shot, open-source) exhibit high rates of hallucination, missing, and inconsistent outputsone-shot/fine-tuning partly mitigates this. For text generation (summarization, simplification), GPT-4/GPT-3.5 yield more readable but less complete output than BART. The study highlights crucial challenges: LLM inconsistencies, lack of robust benchmarks for LLMs, hallucination/missingness, cost, and limited impact from domain-specific pretraining. The authors call for new BioNLP datasets and evaluation protocols, better hallucination mitigation, broader benchmarking (including new LLMs), and more real-world validation, and provide open access to data and code (https://doi.org/10.5281/zenodo.14025500)."
95,"C. Wu, P. Qiu, J. Liu, H. Gu, N. Li, Y. Zhang, Y. Wang, and W. Xie, ""Towards evaluating and building versatile large language models for medicine,"" npj Digital Medicine, vol. 8, Art. no. 58, 2025. Available: https://www.nature.com/articles/s41746-024-01390-4","This paper introduces MedS-Bench, a comprehensive benchmark for evaluating large language models (LLMs) in clinical contexts across 11 clinical tasks, and MedS-Ins, a large-scale, instruction-tuning dataset comprising 5 million instances and 19,000 instructions across 122 tasks sourced from 58 medical language corpora. The study found that leading LLMs like GPT-4 and Claude-3.5 excel at multiple-choice question-answering but often underperform in real clinical tasks, highlighting the limitations of existing benchmarks. To address this, the authors developed the open-source MMedIns-Llama 3 model using instruction tuning on MedS-Ins, significantly outperforming both closed- and open-source LLMs on numerous clinical tasks such as information extraction, concept explanation, text summarization, and diagnosis prediction. Key methods included filtering medical tasks from general instruction datasets, converting BioNLP datasets into instructive formats, and fine-tuning using cross-entropy loss: $Loss = -\sum_t \log P(o_t | o_1,...,o_{t-1}, C, I; \theta)$. Results show that MMedIns-Llama 3 achieves top scores in BLEU/ROUGE for text summarization (46.82/48.38), macro-F1 for text classification (86.66), and F1 for NER (79.29), often surpassing GPT-4 and Claude-3.5. The work emphasizes the importance of broader clinical scenario coverage, multilingual expansion, and real-world validation, while making all datasets, code, and models openly available to foster community-driven research and progress toward effective clinical LLMs."
96,"N. Riccardi, X. Yang, and R. H. Desai, ""The Two Word Test as a semantic benchmark for large language models,"" Scientific Reports, vol. 14, Art. no. 21593, 2024. Available: https://www.nature.com/articles/s41598-024-72528-3","The paper introduces the Two Word Test (TWT), a new open-source benchmark designed to evaluate semantic understanding in large language models (LLMs) using 1,768 noun-noun combinations, each previously rated for meaningfulness by humans. Unlike benchmarks reliant on logic or domain knowledge, the TWT involves simple but fundamental linguistic judgmentsdistinguishing sensible phrases like baby boy from nonsensical ones like goat skywhich humans perform effortlessly. Four top LLMs (GPT-4-turbo, GPT-3.5-turbo, Gemini-1-Pro-001, and Claude-3-Opus) were tested on both a 04 Likert scale and a binary makes sense/nonsense task. Results showed a substantial gap between LLMs and humans: models like GPT-3.5-turbo and Gemini-1.0-Pro-001 were unable to reliably separate meaningful from nonsense phrases, frequently rating both highly, while Claude-3-Opus showed improvement in binary discrimination but still lagged behind humans. Statistical analyses (including modified $t$-tests and signal detection theory metrics) revealed these shortcomings are not merely a result of prompt design or difficulty with numerical scaling but are rooted in deeper issues with semantic composition and core language understanding. Both humans and LLMs correlated meaningfulness with phrase frequency, but LLMs over-relied on cosine similarity between word vectors, often mistaking semantically related words for genuinely meaningful combinations. The paper highlights that current LLMs high performance on complex reasoning tasks does not translate to genuine understanding of simple compositional semantics, as exposed by the TWT, cautioning against attributing human-like language comprehension to these models and emphasizing the need for improved benchmarks and model architectures. The complete dataset and materials are accessible at https://github.com/NickRiccardi/two-word-test."
97,"A. Waldis, Y. Perlitz, L. Choshen, Y. Hou, and I. Gurevych, “Holmes ⌕ A Benchmark to Assess the Linguistic Competence of Language Models,” Transactions of the Association for Computational Linguistics, vol. 12, pp. 1616–1647, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.88","We introduce Holmes, a new benchmark designed to assess language models (LMs) linguistic competencetheir unconscious understanding of linguistic phenomena. Specifically, we use classifier-based probing to examine LMs internal representations regarding distinct linguistic phenomena (e.g., part-of-speech tagging). As a result, we meet recent calls to disentangle LMs linguistic competence from other cognitive abilities, such as following instructions in prompting-based evaluations. Composing Holmes, we review over 270 probing studies and include more than 200 datasets to assess syntax, morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size. However, surprisingly, model architecture and instruction tuning also significantly influence performance, particularly in morphology and syntax. Finally, we propose FlashHolmes, a streamlined version that reduces the computation load while maintaining high-ranking precision."
98,"Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. ""How Can We Know What Language Models Know?"" Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020. [Online]. Available: https://aclanthology.org/2020.tacl-1.28/","This paper addresses the challenge of accurately estimating the knowledge contained in language models (LMs) by recognizing that the prompts typically used to query LMs, such as Obama is a __ by profession, may be sub-optimal and only provide a lower bound on LM knowledge. The authors propose automatic methods, including mining-based and paraphrasing-based techniques, to generate higher-quality, diverse prompts for querying LMs, as well as ensemble approaches to combine information from multiple prompts. Experimental results on the LAMA benchmark show that these methods improve accuracy from 31.1% to 39.6%, yielding a tighter lower bound on LM knowledge extraction. The paper also introduces the LM Prompt And Query Archive (LPAQA), with resources publicly available at https://github.com/jzbjyb/LPAQA."
99,"Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. ""BLiMP: The Benchmark of Linguistic Minimal Pairs for English."" Transactions of the Association for Computational Linguistics, vol. 8, pp. 377–392, 2020. [Online]. Available: https://aclanthology.org/2020.tacl-1.25/","We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP), a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairsthat is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands."
100,"Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. ""Leveraging Pre-trained Checkpoints for Sequence Generation Tasks."" Transactions of the Association for Computational Linguistics, vol. 8, pp. 264–280, 2020. [Online]. Available: https://aclanthology.org/2020.tacl-1.18/","Unsupervised pre-training of large neural models has revolutionized NLP by enabling practitioners to leverage publicly released checkpoints, thereby achieving state-of-the-art results across benchmarks while reducing compute costs. This paper demonstrates that such pre-trained checkpoints are also effective for sequence generation tasks. The authors present a Transformer-based sequence-to-sequence model compatible with BERT, GPT-2, and RoBERTa pre-trained checkpoints, empirically verifying that initializing both encoder and decoder with these checkpoints leads to new state-of-the-art performance on tasks such as Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion."
101,"S. W. Yang, H. J. Chang, Z. Huang, A. T. Liu, P. Su, W. Cheng, Y. Li, M. Wu, J. Lee, O. Hussein, M. Maciejewski, X. Zeng, C. H. Chen, Y. Tsao, D. Su, P. Beh, P. Zhang, Y. Shinohara, F. Weninger, F. Ni, S. Watanabe, T. Hori, A. Subramanian, K. K. Chin, P. Garcia-Perera, M. L. Seltzer, and H. Y. Lee, ""A Large-Scale Evaluation of Speech Foundation Models,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 2884–2899, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10502279","The paper introduces the Speech processing Universal PERformance Benchmark (SUPERB), establishing a standardized and extensible platform to evaluate speech self-supervised learning (SSL) foundation models across 15 diverse speech tasks such as phoneme recognition, keyword spotting, speaker identification, emotion recognition, and automatic speech recognition, among others. SUPERB employs a unified multi-tasking evaluation protocol, typically using a frozen foundation model with lightweight, task-specific prediction heads, and adopts a learnable weighted-sum approach for aggregating information across model layers (contrast: last-layer vs. weighted-sum showed the latter generally boosts performance except for some tasks like voice conversion). Thirty-three models, including HuBERT, wav2vec 2.0, WavLM, and others, were evaluated, with SSL models demonstrating strong generalizability and competitive or superior performance to traditional and non-SSL baselinesthough generative tasks like speech enhancement (SE) and source separation (SS) remain challenging. Statistical analyses reveal that observed leaderboard differences are not always significant, and single-layer benchmarking may be preferable for certain tasks like VC. The platform emphasizes reproducibility, robustness (including under data distortion and low-resource conditions), and community-driven expansion, with open-source resources, deterministic benchmarking, and an online leaderboard. Recommendations include best practices for benchmarking and statistical testing, with future work aimed at designing more robust generative SSL models, improved benchmarks, and few-shot learning tasks."
102,"L. Della Libera, P. Mousavi, S. Zaiem, C. Subakan, and M. Ravanelli, ""CL-MASR: A Continual Learning Benchmark for Multilingual ASR,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 4486–4500, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10737390/","This paper introduces CL-MASR, the first benchmark specifically designed for continual learning (CL) in multilingual automatic speech recognition (ASR). Addressing the challenge where current state-of-the-art ASR models (e.g., Whisper, WavLM) struggle to incorporate new languages without catastrophic forgetting of previously learned ones, CL-MASR offers standardized task sequences, dataset splits (language-, domain-, and task-incremental), and a suite of established and novel CL methodsincluding regularization, replay, expert, and parameter-isolation approachestested on large-scale multilingual data from Common Voice and FLEURS. Experimental results reveal that all approaches experience varying degrees of catastrophic forgetting, particularly on low-resource and typologically distinct languages; replay-based and expert-based methods offer some mitigation but require tradeoffs in memory and compute. Metrics such as Word Error Rate (WER), forgetting, backward transfer, and intransigence are employed for comprehensive evaluation. The authors find that language ordering, resource imbalance, and cross-lingual interference greatly affect CL performance, emphasizing the need for more robust, scalable, and efficient CL strategies tailored to speech tasks. The open-source codebase made available (https://github.com/speechbrain/benchmarks) is intended to standardize and accelerate research in this underexplored domain."
103,"Y. Wang, Y. Zhang, P. Li, and Y. Liu, ""Gradual Syntactic Label Replacement for Language Model Pre-Training,"" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 961–972, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10315017","In this article, the authors propose a multi-stage training strategy that incrementally expands the training vocabulary by modifying the training data, demonstrating that this approach enhances the performance of both discriminative and generative pre-trained language models on benchmark tasks."
104,"Y. Perlitz, E. Bandel, A. Gera, O. Arviv, L. Ein-Dor, E. Shnarch, N. Slonim, M. Shmueli-Scheuer, and L. Choshen, ""Efficient Benchmarking of Language Models,"" arXiv preprint arXiv:2308.11696, Computation and Language (cs.CL), accepted to NAACL main track, v5, pp. 1–19, 2024. [Online]. Available: https://arxiv.org/abs/2308.11696","This paper addresses the substantial computational costs incurred by the comprehensive benchmarking of language models (LMs) and introduces the concept of Efficient Benchmarkingstrategically reducing evaluation costs without compromising reliability. Focusing on the HELM benchmark, the authors propose the Decision Impact on Reliability (DIoR) metric to quantitatively assess how benchmark design choices (such as number of scenarios, datasets, examples, prompts, and aggregation strategies) affect evaluation reliability. The study reveals that removing entire datasets significantly reduces reliability, whereas reducing the number of examples is less detrimental due to the stability of model rankings with fewer examples. It is shown that certain aggregation strategies, especially grouping diverse datasets into scenarios, can undermine reliability, and that Mean Win Rate (MWR), HELMs primary ranking metric, is sensitive to the addition or removal of modelspotentially leading to arbitrary changes in rankings. The findings culminate in concrete guidelines for efficient yet robust benchmark design, and the introduction of Flash-HELMan evaluation algorithm that reduces computational requirements by up to 100-fold with minimal loss in reliability. The work underscores the need for systematic, quantitative methods (such as DIoR) in benchmark development and advocates for transparency and efficiency, moving beyond the notion that more evaluation examples alone increase reliability. The broader impact includes improved accessibility, reproducibility, and environmental responsibility in language model evaluation."
105,"J. Chen, H. Lin, X. Han, and L. Sun, ""Benchmarking Large Language Models in Retrieval-Augmented Generation,"" arXiv preprint arXiv:2309.01431, Computation and Language (cs.CL), accepted to AAAI 2024, v2, pp. 1–14, 2023. [Online]. Available: https://arxiv.org/abs/2309.01431","This paper systematically investigates the effectiveness of Retrieval-Augmented Generation (RAG) on large language models (LLMs) by introducing the Retrieval-Augmented Generation Benchmark (RGB), which tests four core abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. RGB is constructed using QA pairs derived from recent news articles and external documents obtained via search engines, and it divides tasks into four testbeds reflecting the required abilities. Six representative LLMssuch as ChatGPT-3.5, ChatGLM variants, Vicuna-7b, Qwen-7B-Chat, and BELLE-7B-2Mare evaluated across these tasks. Results show that LLMs perform reasonably well under moderate noise ($\approx$ high accuracy in mild settings), but accuracy drops sharply with higher noise, detailed in Table 1. Negative rejection remains a significant weakness, with the best LLMs refusing to answer unsupported questions less than 50% of the time (Table 3), mainly due to evidence uncertainty or concept confusion. LLMs also struggle to integrate information from multiple documentsmaximum accuracy for complex integration tasks is around 60% (Table 5), with common errors including merging, ignoring, and misalignment (Table 6). Furthermore, on counterfactual robustness, LLMs typically defer to erroneous retrieved content even when they possess correct internal knowledge (Table 7). The discussion highlights persistent bottlenecks: difficulty with noise/fake information, low refusal rates, challenges in cross-document reasoning, and susceptibility to misleading retrievals. The authors conclude that substantial advances in document modeling, reasoning strategies, and error detection are needed for reliable RAG deployment in LLMs, cautioning that careful design remains vital to prevent hallucinations and errors."
106,"T. Kew, A. Chi, L. Vásquez-Rodríguez, S. Agrawal, D. Aumiller, F. Alva-Manchego, and M. Shardlow, ""BLESS: Benchmarking Large Language Models on Sentence Simplification,"" arXiv preprint arXiv:2310.15773, Computation and Language (cs.CL), accepted to EMNLP 2023 as a main long paper, v1, pp. 1–9, 2023. [Online]. Available: https://arxiv.org/abs/2310.15773","We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics."
107,"M. N. Nityasya, K. Christodoulopoulos, F. B. Bastani, and J. Kwiatkowski, “A Case for More Rigour in Language Model Pre-Training: Replicability, Reporting, and Evaluations,” Transactions of the Association for Computational Linguistics, vol. 11, pp. 1343–1358, 2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.75/","This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand: How exactly does each factor contribute to the progress that we have today? We provide a case in point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and demonstrate how  under comparable conditions where the baselines are tuned to a similar extent  these baselines (and even-simpler variants thereof) can, in fact, achieve competitive or better performance than BERT. These findings demonstrate how disentangling different factors of model improvements can lead to valuable new insights. We conclude with recommendations for how to encourage and incentivize this line of work, and accelerate progress towards a better and more systematic understanding of what factors drive the progress of our foundation models today."
108,"Y. In'nami, A. Mizumoto, L. Plonsky, and R. Koizumi, ""Promoting computationally reproducible research in applied linguistics: Recommended practices and considerations,"" Research Methods in Applied Linguistics, vol. 1, no. 3, p. 100030, 2022. [Online]. Available: https://doi.org/10.1016/j.rmal.2022.100030","Reproducible research in applied linguistics enables verification of findings by re-creating reported statistical results, aiding assessment of analysis code accuracy, correct reporting of statistical values, and the potential impact of errors. Building on open science practices, this work advances reproducibility from three methodological perspectives: (1) sharing supplementary materials in online repositories such as IRIS and OSF; (2) presenting information reproducibly using R, R Markdown, containers, and online platforms; and (3) employing simulated data derived from original datasets when data cannot be directly shared. These recommendations help researchers adopt reproducible methods in their own work and contribute to the advancement of applied linguistics as a whole."
