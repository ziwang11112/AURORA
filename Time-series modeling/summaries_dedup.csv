Index,Citation,Summary
1,"A. Aknouche and M. G. Scotto, ""A multiplicative thinning‐based integer‐valued GARCH model,"" J. Time Ser. Anal., vol. 45, no. 1, pp. 4–26, 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12690","In this paper, the authors propose a novel multiplicative integer-valued time series model constructed as the product of a unit-mean integer-valued iid sequence and an integer-valued dependent process, the latter built via binomial thinning of its own and the observed process's past values. This new model elegantly merges properties of INGARCH (integer-valued GARCH), ACD, and INAR processes, enabling it to efficiently generate time series with high overdispersion, persistence, and heavy-tailed distributions while remaining semi-parametric and parsimonious. The dynamic probabilistic structure is investigated, and parameters are estimated with a two-stage weighted least squares estimator (2SWLSE), whose consistency and asymptotic normality are established. Applications to both simulated and real-world count time series data are provided, demonstrating the models practical utility."
2,"H. Yuan, K. Lu, G. Li, and J. Wang, ""High-Frequency-Based Volatility Model with Network Structure,"" J. Time Ser. Anal., vol. 45, no. 4, pp. 533–557, 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12751","This paper introduces the network HEAVY model, an extension of the multivariate HEAVY framework that incorporates a time-varying, factor-driven network structure to model dependencies among financial asset volatilities using both high-frequency (realized) and low-frequency (integrated) volatility measures. The model is specified as $V_t = \Gamma_0 + \Gamma_1 x_{t-1} + \Gamma_2 V_{t-1}$, where the realized volatility $x_{it}$ for each asset $i$ is linked across assets via time-varying weights $w_{ij,t}$ defined by latent factors: $w_{ij,t} = \sum_{k=1}^K \beta_{ik} \beta_{jk} f_{kt}$. Estimation employs quasi-maximum likelihood and iterative network reconstruction via rolling windows. Simulation results with $N=10,20,50$ assets demonstrate the models ability to recover dynamic network structures accurately, with superior volatility prediction compared to existing benchmarks, especially under network evolution. Empirical application to 50 Chinese stocks reveals that the asset network evolves notably during financial events, forming economically meaningful clusters and yielding improved forecast and risk measures (e.g., RMSE, VaR). Noted challenges include computational complexity and the need for high-frequency data, with future work suggested on real-time estimation and broader applications. Overall, the network HEAVY model provides a theoretically sound and practically effective tool for dynamic financial volatility analysis and risk management."
3,"A. Aknouche and S. Dimitrakopoulos, ""Autoregressive conditional proportion: a multiplicative‐error model for (0, 1)‐valued time series,"" J. Time Ser. Anal., vol. 44, no. 4, pp. 393–417, 2023. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12663","The paper introduces the autoregressive conditional proportion (ARCP) model, a novel multiplicative time series framework for handling (0, 1)-valued datasuch as proportions in finance and insuranceby generalizing GARCH-type dynamics to the conditional mean via the formulation $Y_t = \mu_t \cdot \epsilon_t$, where $\mu_t$ evolves multiplicatively based on past observations and $\epsilon_t$ are i.i.d. innovations with unit mean. This structure enforces the proper proportion range, captures time-varying conditional means and higher-order moments, and admits a Markov chain representation. Stationarity and ergodicity conditions are rigorously derived by analyzing the process as a Markov chain on $(0,1)^p \times (0,1)^q$. Key properties like infinite divisibility, moment and autocorrelation formulas, and a flexible innovation law (e.g., beta) enable thorough modeling of skewed, heavy-tailed data. Maximum likelihood methods allow for efficient parameter estimation, with diagnostics based on both theoretical and empirical residual analyses. Through simulations and real applications to financial durations and insurance claim frequencies, the ARCP model outperforms traditional beta and Bernoulli autoregressive approaches in terms of likelihood, prediction, and goodness-of-fit, with diagnostic tests confirming its adequacy. Challenges include ensuring strict stationarity, handling parameter identifiability in higher order models, and specifying the innovation distribution. Future directions include multivariate extensions, incorporating exogenous covariates, and robust model selection. The ARCP model thus provides a theoretically robust and practically effective solution for dynamic modeling of proportional time series data."
4,"S. Makridakis, E. Spiliotis, and V. Assimakopoulos, ""The M4 Competition: 100,000 time series and 61 forecasting methods,"" International Journal of Forecasting, vol. 36, no. 1, pp. 54–74, Jan.–Mar. 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207019301128","The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field."
5,"H. Hewamalage, C. Bergmeir, and K. Bandara, ""Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions,"" International Journal of Forecasting, vol. 37, no. 1, pp. 388–427, Jan.–Mar. 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207020300996","Recurrent Neural Networks (RNNs) have shown competitive forecasting performance, as evidenced by their success in the M4 competition, yet established statistical models like ETS and ARIMA remain popular due to their accuracy, robustness, efficiency, and ease of use for non-experts. This paper offers a comprehensive empirical study and open-source framework evaluating current RNN architectures for forecasting, leading to practical guidelines: RNNs can directly model seasonality in datasets with homogeneous seasonal patterns, but deseasonalization is advisable otherwise. Comparisons reveal that while (semi-)automatic RNN models are not universally superior, they can serve as competitive alternatives to classical statistical approaches in numerous situations."
6,"Y. Kang, R. J. Hyndman, and K. Smith-Miles, ""Visualising forecasting algorithm performance using time series instance spaces,"" International Journal of Forecasting, vol. 33, no. 2, pp. 345–358, Apr.–Jun. 2017. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207016301030","It is common practice to evaluate forecasting methods using established time series datasets like the M3 data, but questions remain about the diversity and challenge these datasets offer for distinguishing the strengths and weaknesses of different methods. This paper introduces a visualization approach that represents each time series as a point within a two-dimensional instance space, thereby enabling straightforward visualization of forecasting method performance and diversity assessment across datasets. Addressing concerns about the M3 datasets variability, the authors further propose a method to synthesize new time series with adjustable properties to better populate the instance space, thus enhancing the efficacy and generalizability of benchmarking studies for forecasting methodologies."
7,"H. L. Shang, J. Cao, and P. Sang, ""Stopping time detection of wood panel compression: A functional time-series approach,"" Journal of the Royal Statistical Society: Series C (Applied Statistics), vol. 71, no. 5, pp. 1205–1224, 2022. Available: https://academic.oup.com/jrsssc/article/71/5/1205/7073314","We consider determining the optimal stopping time for the glue curing of wood panels in an automatic process environment. Using near-infrared spectroscopy to monitor the manufacturing process allows substantial savings in energy and time. A time series of 72 spectra from a near-infrared probe is collected to detect the optimal stopping time. The proposed estimation procedure divides the data into training and testing samples, iteratively computes integrated squared forecast errors on the testing sample, and applies a structural break detection method with one breakpoint to estimate the optimal stopping time from the univariate time series of errors. The method's finite-sample performance is evaluated through simulation studies."
8,"V. Cardiff, M. Ritchie, D. Hoy, and K. Esmail, ""Wavelet shrinkage for trend estimation and forecasting in time series,"" Journal of the Royal Statistical Society: Series C (Applied Statistics), vol. 68, no. 2, pp. 295-316, 2019. Available: https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12296","This paper introduces generalized wavelet shrinkage techniques for nonparametric trend estimation and forecasting in time series with both seasonal and nonseasonal autocorrelated noise. Using discrete wavelet transforms (DWT) and adaptive soft or hard thresholding of coefficientswhere the threshold adapts to the underlying noise structure, such as AR(1), ARMA, or fractionally integrated (long-memory) processesthe methods simultaneously estimate smooth or abrupt trends and provide accurate forecasts. Extensive simulations and benchmark datasets, including ENSO indices, electricity demand, and temperature series, show that wavelet-based estimators and forecasts often yield lower mean squared error and superior detection of level shifts and complex nonstationarity than classical parametric (ARIMA, state space) and nonparametric (local polynomial, spline) approaches. Notably, wavelet methods require no fixed basis or windowing, little parameter tuning, and deliver significant computational speedups for large data. Main limitations include possible underperformance around sharp changes overlapping coarse wavelet supports and the sensitivity of thresholding rules to bias-variance tradeoffs. The study highlights the potential of wavelet shrinkage as an adaptive, computationally efficient framework for trend analysis and forecasting in complex, noisy time series and suggests future research into handling nonstationary noise, online updates, and multivariate extensions."
9,"A. Anastasiou, P. Hatzopoulos, A. Karagrigoriou, and G. Tzagkarakis, ""Causality Distance Measures for Multivariate Time Series with Applications,"" Mathematics, vol. 9, no. 21, Art. no. 2708, 2021. [Online]. Available: https://www.mdpi.com/2227-7390/9/21/2708","In this study, the authors introduce new distance measure algorithmsCausality Within Groups (CAWG), Generalized Causality Within Groups (GCAWG), and Causality Between Groups (CABG)all rooted in Granger causality, tailored for multivariate statistical analysis and clustering of time series data where causal dynamics are prominent, especially in financial and economic domains. The utility of these causal-based distance measures is demonstrated by applying them to hierarchical clustering for the classification of 19 EU countries using seven health resource variables, thus showcasing their potential for uncovering causal structures in complex multivariate datasets."
10,"R. R. Jha, T. S. Teh, and A. R. Hassan, ""Statistical Modeling to Improve Time Series Forecasting Using Machine Learning, Time Series, and Hybrid Models: A Case Study of Bitcoin Price Forecasting,"" Mathematics, vol. 12, no. 23, Art. no. 3666, 2024. [Online]. Available: https://www.mdpi.com/2227-7390/12/23/3666","Bitcoin (BTC-USD), a decentralized digital currency, has seen significant adoption since 2008. This study focuses on forecasting BTC-USD closing prices using data from September 2023 to September 2024, with 390 observations. It employs four machine learning modelsMulti-layer Perceptron, Extreme Learning Machine, Neural Network AutoRegression, and Extreme-Gradient Boostalongside four time series models: Auto-Regressive Integrated Moving Average (ARIMA), Auto-Regressive (AR), Non-Parametric Auto-Regressive, and Simple Exponential Smoothing. Hybrid models, created by simply averaging these individual models, are proposed. The evaluation uses multiple splits of training data (30%, 20%, 10%) for comparative testing, and error metrics such as MAE, RMSE, MAPE, SMAPE, and directional accuracy (correlation, MDA) are reported. Results indicate that the hybrid model consistently outperforms individual models, a finding further supported by the Diebold-Mariano (DM) test and graphical analyses. This hybrid approach thus equips financial analysts with an effective tool for BTC investment forecasting."
11,"H. Zhu, Y. Chen, X. Wei, and W. Wu, ""Deep Time Series Forecasting Models: A Comprehensive Survey,"" Mathematics, vol. 12, no. 10, Art. no. 1504, 2024. [Online]. Available: https://www.mdpi.com/2227-7390/12/10/1504","This comprehensive review surveys the rapid advancements in deep learning for time series forecasting (TSF), highlighting the transition from statistical methods to the dominance of architectures such as Transformers, MLPs, CNNs, RNNs, GNNs, foundation models, and novel approaches like Mamba and diffusion models. TSF applications span vital areasenergy, finance, healthcare, and beyondwhere deep learning models have enabled substantial performance gains but still face challenges like handling massive, multivariate data, accommodating distribution shifts, and achieving broad generalization. The paper introduces a modern taxonomy for TSF deep neural network models, rigorously compares state-of-the-art techniques (e.g., patching, channel dependency strategies, cross-dimension attention, frequency domain learning), and summarizes ubiquitous evaluation metrics and datasets. Results from the literature reveal that while Transformers revolutionized TSF, simple linear or MLP-based models can outperform them in long-term forecasting, and innovations like PatchTST, iTransformer, and Mamba offer new efficiency and performance trade-offs. Probabilistic and generative TSF has been invigorated by diffusion models. The discussion addresses enduring challenges: understanding when to model channel dependence, managing non-stationarity via techniques like RevIN or SAN, infusing causality, extracting features (trends, periodicities, noise) through self-supervised and automated methods, and the interpretability/explainability dichotomy using intrinsic or post-hoc approaches (e.g., TFT, SHAP, LIME). The survey concludes by mapping out future researchadvancing theoretical foundations, expanding cross-domain adaptability, and pursuing interpretable, scalable, and robust foundation modelsthus providing a pivotal reference for both current researchers and newcomers to deep learning-based TSF."
12,"Yuan Gao and Han Lin Shang, ""Forecasting high-dimensional functional time series: Application to sub-national age-specific mortality,"" Journal of Time Series Analysis, vol. 39, no. 4, pp. 528–546, 2018. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsa.12312","We address the problem of forecasting high-dimensional functional time series through a two-fold dimension reduction procedure. The difficulty of forecasting high-dimensional functional time series lies in the curse of dimensionality. In this paper, we propose a novel method to solve this problem. Dynamic functional principal component analysis is first applied to reduce each functional time series to a vector. We then use the factor model as a further dimension reduction technique so that only a small number of latent factors are preserved. Classic time series models can be used to forecast the factors and conditional forecasts of the functions can be constructed. Asymptotic properties of the approximated functions are established, including both estimation error and forecast error. The proposed method is easy to implement especially when the dimension of the functional time series is large. We show the superiority of our approach by both simulation studies and an application to Japanese age-specific mortality rates."
13,"Thoman Liboschik, Nadja Fokianos, and Roland Fried, ""tscount: An R Package for Analysis of Count Time Series Following Generalized Linear Models,"" Journal of Time Series Analysis, vol. 38, no. 3, pp. 451–478, 2017. [Online]. Available: https://onlinelibrary.wiley.com/doi/full/10.1111/jtsa.12214","The R package tscount implements likelihood-based estimation methods for modeling count time series via generalized linear models, providing a flexible framework that models serial correlation parsimoniously by linking the conditional mean to past values, observations, and covariates. Supported models include those with identity and logarithmic link functions, and conditional distributions such as Poisson and negative binomial, notably covering INGARCH models and their log-linear extensions. The package features model fitting, assessment, prediction, and intervention analysis tools, with implementation details and simulation results presented for previously unstudied models. Usage is illustrated through data examples, and the paper includes a comparative review of R packages for count time series analysis, highlighting the distinctive features of tscount."
14,"J. Shang, T. Nakamura, and S. Hamori, ""Does the Sentiment Index Help Predict Crude Oil Prices?"" IETI Transactions on Data Analysis and Forecasting (iTDAF), vol. 1, no. 1, pp. 54–65, 2023. [Online]. Available: https://online-journals.org/index.php/iTDAF/article/view/32683","The study explores the impact of integrating sentiment analysis with machine learning techniquesspecifically random forest, support vector machine, and long short-term memory modelsfor forecasting one-period-ahead West Texas Intermediate (WTI) spot prices. Employing both dynamic expanding and fixed moving windows, the research benchmarks the forecasting performance using root mean squared error and the DieboldMariano test. The empirical findings reveal that sentiment indicators derived from social media outperform traditional technical indicators and lagged price values in predicting crude oil prices, thus demonstrating the significant predictive value of sentiment analysis in the crude oil market context."
15,"M. Housni, ""Multidimensional Forecasting and Analysis: Exploring Moroccan Learning Data Analytics in Business, Environment, and Sustainability,"" IETI Transactions on Data Analysis and Forecasting (iTDAF), vol. 1, no. 3, pp. 75–82, 2023. [Online]. Available: https://online-journals.org/index.php/iTDAF/article/view/40353","As humanmachine interactions intensify within hybrid physical and digital spaces, this paper explores the deployment of data analytics to enhance learning capacities and human development in Moroccoa nation of moderate technological advancement. The work presents the findings of data analytics applied within Moroccos hybrid educational environment and introduces a multidimensional forecasting and analysis model that integrates business, environment, and sustainability perspectives. Aimed at countries experiencing technological growth, the study proposes that comprehensive analytics frameworks can extend the application of learning data analysis beyond advanced nations, fostering innovation and progress across diverse global sectors."
16,"E. Rusu and O. Manta, ""Forecasting and Directions Regarding Sustainable Public Procurement,"" IETI Transactions on Data Analysis and Forecasting (iTDAF), vol. 1, no. 2, pp. 98–107, 2023. [Online]. Available: https://online-journals.org/index.php/iTDAF/citationstylelanguage/get/apa?submissionId=39987&publicationId=77089","In this research, the author highlights the public procurement system as a crucial mechanism for governments to advance sustainable consumption, production, and broader sustainable development objectives. Sustainable Public Procurement (SPP) is characterized as a process wherein public entities acquire goods, services, works, and utilities under optimal price and quality conditions, intentionally generating benefits while minimizing environmental harm. The paper offers methodological considerations for establishing an SPP system, drawing upon models implemented within EU countries, and also addresses forecasting and evaluating the current landscape of public procurement as a foundational element in achieving sustainability goals."
17,"Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen, ""Time-LLM: Time Series Forecasting by Reprogramming Large Language Models,"" in Proceedings of the 12th International Conference on Learning Representations (ICLR 2024), 2024. [Online]. Available: https://arxiv.org/abs/2310.01728","Time series forecasting, crucial in various dynamic systems, typically relies on specialized models tailored for distinct tasks, contrasting with the unified approaches seen in NLP and CV. The emergence of large language models (LLMs) with strong sequence reasoning abilities presents an opportunity, but integrating time series modalities with natural language models remains a challenge due to data sparsity. This work introduces Time-LLM, a framework that reprograms LLMs for time series forecasting by aligning time series inputs with textual prototypes, enabling their compatibility with frozen LLMs. The proposed Prompt-as-Prefix (PaP) technique further enhances the context for these models, guiding the transformation of input patches and facilitating accurate forecasting through subsequent projections. Evaluations show that Time-LLM surpasses current specialized models, excelling in both few-shot and zero-shot contexts."
18,"Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam, ""A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,"" in Proceedings of the International Conference on Learning Representations (ICLR 2023), 2023. [Online]. Available: https://arxiv.org/abs/2211.14730","The paper introduces PatchTST, a Transformer-based architecture optimized for multivariate time series forecasting and self-supervised representation learning. PatchTST is built on two innovations: patching, wherein each time series is segmented into subseries-level patches that serve as Transformer tokens, and channel-independence, which processes each univariate channel separately while sharing Transformer parameters, allowing better attention learning with reduced risk of overfitting. This design enables PatchTST to retain local semantic information, substantially lower the quadratic computational and memory costs of attention (especially for longer look-back windows), and model longer-range dependencies. On eight benchmark datasets, PatchTST consistently surpasses both state-of-the-art Transformer models and recent linear baselines in terms of MSE and MAE across various prediction horizons. For self-supervised tasks, the model employs a masked autoencoder loss (mean squared error), demonstrating strong performance and robust transfer learning: pre-trained representations transfer effectively across domains and outperform supervised-only training. Detailed experiments and ablation studies indicate both patching and channel-independence individually boost performance, with their combination delivering the best results; PatchTST is robust to changes in patch length, random seeds, and model architecture. The work highlights that, while channel-independence forgoes modeling spatial/channel correlationspotentially a limitationthis approach adapts well and is less prone to overfitting for existing benchmark tasks. The authors suggest future work should explore better modeling of cross-channel dependencies (using methods such as graph neural networks) to further improve results, positioning PatchTST as a promising foundation for next-generation time series models."
19,"Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister, ""TSMixer: An All-MLP Architecture for Time Series Forecasting,"" Transactions on Machine Learning Research (TMLR), Sep. 2023. [Online]. Available: https://arxiv.org/abs/2303.06053","Real-world multivariate time-series datasets present complex dynamics often addressed with high-capacity sequential models such as recurrent or attention-based deep learning architectures, yet simple univariate linear models have recently been shown to outperform them on common benchmarks. Building upon this, the paper introduces Time-Series Mixer (TSMixer), a novel stack of multi-layer perceptrons (MLPs) utilizing mixing operations along both time and feature dimensions to efficiently extract information. TSMixer, while simple to implement, matches specialized state-of-the-art models on standard academic benchmarks and surpasses them on the large-scale, real-world M5 retail dataset. These findings highlight the crucial role of effectively leveraging cross-variate and auxiliary data in time series forecasting and suggest that the design paradigms behind TSMixer may inspire future advances in deep learning-based time series forecasting."
20,"S. Bandyopadhyay, C. Jentsch, and S. Subba Rao, ""A Spectral Domain Test for Stationarity of Spatio‐Temporal Data,"" Journal of Time Series Analysis, vol. 38, no. 2, pp. 326-351, 2017. [Online]. Available: https://doi.org/10.1111/jtsa.12212","Many environmental and geophysical processes are functions of both space and time, known as spatio-temporal processes, and are often observed at equidistant times but irregular spatial locations; assuming such processes are stationary can lead to modeling errors if the assumption fails. This article introduces a test for spatio-temporal stationarity that leverages the property that the Fourier transforms of second-order stationary stochastic processes are nearly uncorrelated, while those of nonstationary processes are correlated. A discrete Fourier transform suitable for irregular spatial sampling is constructed, and two statistics quantifying correlation in these transforms are proposed for testing stationarity. The methodology also enables tests for one-way (spatial or temporal) stationarity, and its utility is demonstrated with a simulation study."
21,"Z. Gao and R. S. Tsay, ""A Structural-Factor Approach to Modeling High-Dimensional Time Series and Space-Time Data,"" Journal of Time Series Analysis, vol. 40, no. 3, pp. 343-362, 2019. [Online]. Available: https://doi.org/10.1111/jtsa.12481","This paper presents a structural-factor approach for modeling high-dimensional time series and space-time data by decomposing each series into a deterministic dynamic component (the linear structure) and an idiosyncratic component modeled as a linear process. The unique linear structures for each series can capture diverse serial and cross-sectional dynamics, while the idiosyncratic components are influenced by dynamic factors and random errors. In large-dimensional settings, dynamic factors are consistently estimable via eigenanalysis of a matrix constructed from the sample autocovariance matrices of the data, enabling dimension reduction and simplifying inference. Theoretical properties are analyzed, and the method is validated using simulations and real-world datasets."
22,"P. Nystrup, J. K. Møller, H. Madsen, ""Dimensionality reduction in forecasting with temporal hierarchies,"" International Journal of Forecasting, vol. 37, no. 3, pp. 1127–1146, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207020301898","Combining forecasts from multiple temporal aggregation levels leverages distinct information and addresses model uncertainty, with reconciliation ensuring predictions are unified for coherent decision-making across different time horizons. However, estimating the full cross-covariance matrix inherent in temporal hierarchies poses significant challenges due to its potentially large dimensionality and the difficulty in discerning the most crucial elements of the error structure. To overcome this, the authors introduce an eigendecomposition-based dimensionality reduction approach during forecast reconciliation, aiming to maximize the extraction of information from available error structures. Simulation studies and real-world applicationsspecifically in short-term electricity load and financial volatility forecastingdemonstrate that this estimator not only achieves state-of-the-art accuracy but also uniformly improves performance across all levels of temporal aggregation, making it broadly applicable regardless of hierarchy size."
23,"J. K. Møller, P. Nystrup, H. Madsen, ""Likelihood-based inference in temporal hierarchies,"" International Journal of Forecasting, vol. 40, no. 2, pp. 515–531, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207022001704","We consider the importance of correctly specifying the variancecovariance matrix to allow information to be shared between aggregation levels when reconciling forecasts in a temporal hierarchy. We propose a novel framework for parametric modelling of the variancecovariance matrix, along with an iterative algorithm for maximum likelihood estimation. The covariance between aggregation levels can be modelled by aggregating the lower-level errors and disaggregating information from the higher levels. Using the likelihood approach, statistical inference can be applied to identify a parsimonious parametric structure for the variancecovariance matrix. We test and discuss different structures for how forecast errors are connected across aggregation levels and present a framework for simplifying these structures using Wald and likelihood-ratio tests. We evaluate the proposed method in a simulation study and through an application to day-ahead electricity load forecasting and find that it performs well compared to optimal shrinkage estimation."
24,"J. Cordes and M. C. Castro, ""Spatial analysis of COVID-19 clusters and contextual factors in New York City,"" Spatial and Spatio-temporal Epidemiology, vol. 34, Article 100355, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S1877584520300332","Identifying areas with low access to testing and high case burden is necessary to understand risk and allocate resources in the COVID-19 pandemic. Using zip code level data for New York City, we analyzed testing rates, positivity rates, and proportion positive. A spatial scan statistic identified clusters of high and low testing rates, high positivity rates, and high proportion positive. Boxplots and Pearson correlations determined associations between outcomes, clusters, and contextual factors. Clusters with less testing and low proportion positive tests had higher income, education, and white population, whereas clusters with high testing rates and high proportion positive tests were disproportionately black and without health insurance. Correlations showed inverse associations of white race, education, and income with proportion positive tests, and positive associations with black race, Hispanic ethnicity, and poverty. We recommend testing and health care resources be directed to eastern Brooklyn, which has low testing and high proportion positives."
25,"V. van Zoest, K. Lindberg, G. Varotsis, F. B. Osei, and T. Fall, ""Differentiated mental health patterns in pregnancy during COVID-19 first two waves in Sweden: a mixed methods study using digital phenotyping,"" Spatial and Spatio-temporal Epidemiology, vol. 48, Article 100636, 2024.","This study used the Mom2B mobile application to examine depressive and anxiety symptoms, wellbeing, and life conditions among 1,577 pregnant women in Sweden during the first two waves of the COVID-19 pandemic (Jan 2020Feb 2021). Depression (EPDS  13) peaked at 25% and 24% during the two waves (vs. 8.0% pre-pandemic, $p<0.001$), while anxiety rose only during the first wave ($p=0.023$), and low wellbeing (WHO-5 < 50) affected nearly half of participants in both waves. Google search volumes for Corona and national COVID-19 deaths strongly correlated with poor mental health outcomes only during the first wave (interaction $p$-values: $0.001$$0.003$), with associations attenuated or reversed in the second wave, suggesting population adaptation. Qualitative data highlighted distress over restricted maternity care access, partner exclusion, and appointment cancellations. No significant relationship was observed between mobility data and mood outcomes. The findings support the implementation of digital phenotyping for real-time mental health monitoring, acknowledging limitations such as selection bias and self-reporting. The authors conclude that digital tools are vital for identifying and supporting high-risk groups during health crises, and mental health trends may shift across pandemic phases due to societal adaptation, emphasizing the need for ongoing monitoring."
26,"A. García-Pérez, ""On Robustness for Spatio-Temporal Data,"" Mathematics, vol. 10, no. 10, article 1785, pp. 1–17, 2022. Available: https://www.mdpi.com/2227-7390/10/10/1785","The paper addresses the critical role of the spatio-temporal variogram in kriging-based spatio-temporal predictions, which are fundamental in environmental and climate data analysis. It highlights the major limitation of the classical spatio-temporal variogram estimatorits extreme sensitivity to outliersand proposes two remedies: first, the introduction of robust spatio-temporal variogram estimators framed as M-estimators over transformed data; and second, a comparative approach that leverages differences between the classical and robust estimates to identify spatio-temporal outliers. The robust estimators' distribution is investigated using a multivariate scale-contaminated normal model, providing reliable sample approximations. Furthermore, the study presents and analyzes a new class of M-estimators, applying them to real data to assess differences in spatio-temporal variograms across temporal lags, with the additional aim of potentially reducing the number of temporal lags needed in analysis."
27,"J. Zhou, R. Gao, J. Yang, P. Zhang, ""Spatial–Temporal Dynamic Graph Differential Equation Network for Traffic Prediction,"" Mathematics, vol. 11, no. 13, article 2867, 2023. Available: https://www.mdpi.com/2227-7390/11/13/2867","Traffic flow forecasting is essential for intelligent transportation systems, yet is hindered by complex and dynamic spatialtemporal dependencies in road networks. Existing graph convolutional network models that use static adjacency matrices fail to capture these evolving dynamics. The proposed ST-DGDE model addresses this by learning dynamic connectivity between spatial nodes via a dynamic graph learning network, leveraging dynamic graph differential equations (DGDE) to model continuously changing spatialtemporal relationships. Static adjacency matrices are also constructed using static node embeddings. By fusing both dynamic and static graphs and inputting them into a gated temporal causal convolutional network, the model captures both fixed long-term associations and global spatialtemporal dynamics, leading to enhanced long-term traffic prediction. Experiments on two real-world traffic datasets demonstrate that ST-DGDE achieves superior performance compared to baseline methods."
28,"L. Yuan, M. Yin, C. Chen, ""Temporal Community Detection and Analysis with Network Representation Learning,"" Mathematics, vol. 13, no. 5, article 698, 2025. Available: https://www.mdpi.com/2227-7390/13/5/698","We propose TCDA-NE, a novel TCD algorithm that combines evolutionary clustering with convex non-negative matrix factorization (Convex-NMF)."
29,"M. Kim and S.-Y. Shin, ""Temporal Dynamic Embedding for Irregularly Sampled Time Series,"" arXiv preprint arXiv:2504.05768, 2025. [Online]. Available: https://arxiv.org/abs/2504.05768","In practical settings like healthcare, patient data are typically recorded at irregular intervals, leading to sparse and irregularly sampled time series that challenge conventional neural network models using fixed structured representations. To address this, the authors propose temporal dynamic embedding (TDE), a method where each time series variable is modeled as an embedding vector that evolves over time, allowing neural networks to flexibly handle varying numbers of observed variables at each timestep. TDE accommodates selective adoption and aggregation of only the available variable subsets at any moment, capturing patient status based on observed data and alleviating issues from missingness. Experiments on three clinical datasetsPhysioNet 2012, MIMIC-III, and PhysioNet 2019demonstrate that TDE matches or surpasses imputation-based and recent state-of-the-art methods, while also reducing training runtime."
30,"Y. Wang, H. Wu, J. Dong, Y. Liu, M. Long, and J. Wang, ""Deep Time Series Models: A Comprehensive Survey and Benchmark,"" arXiv preprint arXiv:2407.13278, 2024. [Online]. Available: https://arxiv.org/abs/2407.13278","This paper presents a comprehensive survey of deep learning models tailored for time series analysis, emphasizing the distinct challenges posed by nonlinear patterns and temporal variability inherent to time series data. The authors systematically review both foundational modules (e.g., normalization, decomposition, Fourier analysis) and principal architectures (MLPs, RNNs, CNNs, GNNs, Transformers), categorizing their capabilities across five prevalent tasks: classification, forecasting (short/long-term), imputation, and anomaly detection. Leveraging the newly developed open-source Time Series Library (TSLib), which standardizes benchmarking for 24 models over 30 datasets, empirical evaluations reveal that Transformer-based methods (such as iTransformer, PatchTST) exhibit consistently strong performance across all tasks, while MLP- and CNN-based models remain competitive for specific applications. The study not only highlights the reliability of Transformers but also identifies persistent open challenges including handling very long sequences, integrating exogenous data, and achieving scalability and interpretability in industrial deployments. The authors advocate for large foundation models and advanced pre-training methods as promising future directions and position TSLib as an essential platform for fair and comprehensive evaluation in the field."
31,"X. Yu, Z. Chen, Y. Ling, S. Dong, Z. Liu, and Y. Lu, ""Temporal Data Meets LLM – Explainable Financial Time Series Forecasting,"" arXiv preprint arXiv:2306.11025, 2023. [Online]. Available: https://arxiv.org/abs/2306.11025","This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4."
32,"B. Shi, X. Bai, and C. Yao, ""An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 11, pp. 2298–2304, Nov. 2017. doi: 10.1109/TPAMI.2016.2646371. URL: https://ieeexplore.ieee.org/document/7801919/","The paper addresses the problem of scene text recognition within image-based sequence recognition by proposing a novel neural network architecture that unifies feature extraction, sequence modeling, and transcription in an end-to-end trainable framework. This architecture stands out due to its ability to handle arbitrary sequence lengths without character segmentation or scale normalization, its performance in both lexicon-free and lexicon-based recognition, and its compact model size suitable for practical applications. Extensive experiments on IIIT-5K, Street View Text, and ICDAR datasets demonstrate superior performance compared to previous methods; additionally, the approach generalizes well to tasks such as image-based music score recognition, highlighting the models versatility."
33,"Y. Wang, H. Wu, J. Zhang, Z. Gao, J. Wang, P. S. Yu, and M. Long, ""PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 2, pp. 2208–2225, Feb. 2023. doi: 10.1109/TPAMI.2022.3165153. URL: https://ieeexplore.ieee.org/document/9749915","The paper introduces PredRNN, a predictive recurrent neural network designed for spatiotemporal sequence prediction, focusing on generating future images from past frames by jointly capturing spatial appearances and temporal variations. Unlike traditional LSTMs, PredRNN enables memory states to traverse both vertically across stacked RNN layers and horizontally through the sequence of RNN states, facilitated by a novel Spatiotemporal LSTM (ST-LSTM) unit that learns spatial and temporal representations simultaneously. This unified memory structure allows the model to achieve state-of-the-art performance on several video prediction datasets and offers flexibility for extension to other predictive learning tasks by integrating with additional architectures."
34,"J.-T. Chien and C.-W. Wang, ""Hierarchical and Self-Attended Sequence Autoencoder,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 4975–4986, Sept. 2022. doi: 10.1109/TPAMI.2021.3068187. URL: https://ieeexplore.ieee.org/document/9384306","It is important and challenging to infer stochastic latent semantics for natural language applications. The difficulty in stochastic sequential learning is caused by the posterior collapse in variational inference. The input sequence is disregarded in the estimated latent variables. This paper proposes three components to tackle this difficulty and build the variational sequence autoencoder (VSAE) where sufficient latent information is learned for sophisticated sequence representation. First, the complementary encoders based on a long short-term memory (LSTM) and a pyramid bidirectional LSTM are merged to characterize global and structural dependencies of an input sequence, respectively. Second, a stochastic self attention mechanism is incorporated in a recurrent decoder. The latent information is attended to encourage the interaction between inference and generation in an encoder-decoder training procedure. Third, an autoregressive Gaussian prior of latent variable is used to preserve the information bound. Different variants of VSAE are proposed to mitigate the posterior collapse in sequence modeling. A series of experiments are conducted to demonstrate that the proposed individual and hybrid sequence autoencoders substantially improve the performance for variational sequential learning in language modeling and semantic understanding for document classification and summarization."
35,"Y. Keneshloo, T. Shi, N. Ramakrishnan, and C. K. Reddy, “Deep reinforcement learning for sequence-to-sequence models,” IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 7, pp. 2469–2489, Jul. 2020. doi: 10.1109/TNNLS.2019.2929141. [Online]. Available: https://ieeexplore.ieee.org/document/8801910","This survey examines the integration of reinforcement learning (RL) with sequence-to-sequence (seq2seq) models, which are prevalent in tasks like machine translation, text summarization, and image captioning. While standard encoderdecoder architectures achieve strong results, enhancements such as attention mechanisms and pointer-generation models have advanced performance. However, two persistent challenges remain: exposure bias and inconsistency between training and testing measurements. The paper discusses novel RL-based approaches that address these issues by combining decision-making capabilities of RL with the sequential memory strengths of seq2seq models. It overviews recent frameworks blending RL and deep neural networks, identifies inherent difficulties in existing approaches, proposes improvements via RL, and provides source code and experimental results, especially for abstractive summarization tasks, including insights into both model performance and training efficiency."
36,"X. Guan, Y. Yang, J. Li, X. Xu, and H. T. Shen, “Mind the remainder: Taylor’s theorem view on recurrent neural networks,” IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 4, pp. 1507–1519, Apr. 2022. doi: 10.1109/TNNLS.2021.3057294. [Online]. Available: https://ieeexplore.ieee.org/document/9323070","This paper proposes a novel approach that connects the Taylor series expansion with the training of recurrent neural networks (RNNs), positing that RNN training can be viewed as parameter estimation for the Taylor expansion, and introduces a new training algorithm inspired by the Taylor series remainder. The authors demonstrate that, by explicitly incorporating the Taylor remainder into the training process, their method achieves state-of-the-art performance on action recognition and cross-modal retrieval tasks."
37,"Z. Liu, Y. Liu, X. Xie, Y. Zhang, and X. Lu, “A novel sequence-to-sequence-based deep learning model for satellite cloud image time series prediction,” IEEE Transactions on Neural Networks and Learning Systems, vol. 36, no. 1, pp. 638–651, Jan. 2025. doi: 10.1109/TNNLS.2023.3322146. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/10409277/","The paper presents a novel model validated on a real-world satellite cloud image dataset, demonstrating superior performance through extensive experiments. Satellite cloud imagery is critical for meteorological tasks, including the analysis of weather patterns, climate anomaly detection, and precipitation forecasting. The proposed methods include data-driven approaches that leverage measured data and spectral transformations to effectively simulate clouds. The model is specifically validated using data from the Himawari meteorological satellite, with experimental results supporting its effectiveness. The conclusion emphasizes that deep learning, particularly when applied to time series data, can significantly enhance prediction capabilities in meteorological image analysis."
38,"J. Born and M. Manica, ""Regression Transformer enables concurrent sequence regression and generation for molecular language modelling,"" Nature Machine Intelligence, vol. 5, pp. 432-444, 2023. [Online]. Available: https://www.nature.com/articles/s42256-023-00639-z","The Regression Transformer (RT) is introduced as a novel multitask language model that unifies regression (property prediction) and conditional sequence generation by reformulating regression as a sequence modeling task, rather than through separate task-specific heads. Built on an XLNet backbone, RT tokenizes numerical properties into ordered token sequences and employs specialized numerical encodings to capture the proximity of values, alongside an alternating training scheme with permutation language modeling and a novel self-consistency (SC) loss. Extensive evaluations show RT achieves competitive or superior results versus standard regression models (e.g., kNN, SMILES-BERT, XGBoost, MPNN) and generative models (e.g., JT-VAE, GCPN, MoFlow) in benchmarks spanning small molecule, protein, and chemical reaction domains. It enables high-fidelity property prediction ($\mathrm{RMSE} < 0.06$ on QED), robust generation of novel valid molecules based on property 'primers' (with over 99% novelty and high rank correlation with true properties), and matches or surpasses specialized approaches in property-driven molecule and reaction generation tasks. In protein and reaction settings, RT displays nearly perfect regression and strong generative controls, and demonstrates unique multitask flexibility not possible for other models, such as conditional modification of protein properties or reaction yield optimization. The approach is robust to learning continuous properties through numerical tokenization, mitigating defective outputs, and can be extended to multi-property, evolutionary-scale, and unseeded tasks. RT is publicly available via the Generative Toolkit for Scientific Discovery, with datasets drawn from recognized molecule, protein, and reaction benchmarks, representing a significant step toward foundation models in scientific material design."
39,"X. Fang, F. Wang, L. Liu, J. He, D. Lin, Y. Xiang, K. Zhu, X. Zhang, H. Wu, H. Li, and L. Song, ""A method for multiple-sequence-alignment-free protein structure prediction using a protein language model,"" Nature Machine Intelligence, vol. 5, pp. 1087-1096, 2023. [Online]. Available: https://www.nature.com/articles/s42256-023-00721-6","HelixFold-Single addresses the limitations of mainstream protein structure prediction pipelines, such as AlphaFold2, which depend on time-consuming multiple sequence alignments (MSAs) to achieve high accuracy. By integrating a large-scale protein language model (PLM)pre-trained on hundreds of millions of unlabelled sequences using masked predictionwith key geometric learning components from AlphaFold2, HelixFold-Single develops an end-to-end differentiable model that predicts three-dimensional protein structures directly from primary sequences, bypassing the MSA step. The model achieves accuracy competitive with MSA-based methods for proteins that have many homologs, significantly reducing computational time and demonstrating strong performance particularly with targets such as peptides, antibodies, and nanobodies where MSA-based approaches are less effective. Ablation studies confirm that increasing the size of the PLM improves contact prediction and overall accuracy: for example, a 1B-parameter PLM outperforms a 100M-parameter version in both contact prediction and perplexity. While the method excels for high-homology targets, challenges remain in accurately predicting structures for ""long-tail"" or ""orphan"" proteins with few homologs, suggesting that future work should scale up PLM size and diversify training data. Overall, HelixFold-Single significantly accelerates protein structure prediction and holds promise for high-throughput and design-oriented tasks in structural biology."
40,"N. Wang, J. Bian, Y. Li, X. Li, S. Mumtaz, L. Kong, and H. Xiong, ""Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning,"" Nature Machine Intelligence, vol. 6, pp. 548-557, 2024. [Online]. Available: https://www.nature.com/articles/s42256-024-00836-4","RNAErnie is an RNA-focused pretrained language model based on a transformer architecture, designed to address the need for a unified approach to diverse RNA analysis tasks. Utilizing a motif-aware pretraining strategy, it integrates biological priors by incorporating RNA motifs and applies masking at the base, subsequence, and motif levels, while also tokenizing RNA types (such as miRNA, lncRNA) as stop words appended during pretraining. For out-of-distribution tasks involving novel RNA sequences, RNAErnie introduces a type-guided fine-tuning approach, predicting RNA types and refining embeddings via sequence augmentation. The model, trained on 23 million ncRNA sequences and employing a 12-layer, 105-million parameter transformer, demonstrates significant advances over comparable models, yielding up to 1.8% higher accuracy in classification, 2.2% in interaction prediction, and a 3.3% increase in F1 score for structure prediction. Extensive evaluations show RNAErnie's superiority in clustering, classification, interaction, and secondary structure prediction, generalizing robustly across tasks and datasets. Despite some challengessuch as sequence length limitations, lack of RNA-protein or 3D motif modeling, and increased inference complexitythe model represents a robust, generalizable step towards comprehensive RNA sequence analysis, with future work aimed at overcoming these gaps and broadening its scope."
41,"A. Gu and T. Dao, ""Mamba: Linear-Time Sequence Modeling with Selective State Spaces,"" arXiv preprint arXiv:2312.00752, vol. abs/2312.00752, 2024. [Online]. Available: https://arxiv.org/abs/2312.00752","This paper introduces Mamba, a novel sequence modeling architecture based on selective structured state space models (SSMs) that overcomes limitations of both the Transformer and previous SSM variants. By making the SSM parameters functions of the inputi.e., introducing selection mechanismsthe model can perform powerful content-based reasoning and context-dependent processing, which dramatically improves performance on discrete, densely informative modalities like language. Selective SSMs abandon linear time invariance, sacrificing efficient convolution but regaining efficiency through a hardware-aware parallel scan that maintains high throughput and low memory use. The resulting Mamba block integrates the selective SSM and a minimalist MLP, for a stackable architecture that scales linearly with sequence length, yields up to $5\times$ higher inference throughput than Transformers, and demonstrates improved performance up to sequences of length $10^6$. Empirical results show Mamba outperforms both Transformer and state-of-the-art attention-free models on language, DNA, and audio tasks; for example, Mamba-3B matches or exceeds the quality of much larger Transformer models (often those twice its size). Ablations pinpoint selectionespecially in the $\Delta$ parameteras a key driver of improvement. While selectivity brings tradeoffs (reduced continuous data performance), Mamba establishes itself as a strong, general-purpose backbone for foundation models across domains, with open questions about scaling to larger models and the breadth of downstream fine-tuning capabilities. The key innovations are summarized by equations showing input-dependent SSM parameterization and efficient recurrence, with the architecture and experimental benchmarks suggesting linear complexity and strong empirical scaling benefits."
42,"T. Soydan, N. Zubić, N. Messikommer, S. Mishra, and D. Scaramuzza, ""S7: Selective and Simplified State Space Layers for Sequence Modeling,"" arXiv preprint arXiv:2410.03464, vol. abs/2410.03464, 2024. [Online]. Available: https://arxiv.org/abs/2410.03464","The paper introduces S7, a novel state-space model (SSM) designed for efficient and adaptive sequence modeling over extended contexts. Addressing key limitations in previous SSMssuch as the lack of input-dependent filtering in S4 and the complexity increases in MambaS7 incorporates dynamically adjustable state transitions responsive to input content while maintaining a simple and stable reparameterization to ensure robust, scalable long-sequence modeling. S7s formulation guarantees controlled gradient norms, preventing exploding or vanishing gradients and facilitating training stability. Empirical results demonstrate that S7 outperforms state-of-the-art models on a wide range of benchmarks, achieving accuracy rates such as $99.2\%$ on DVS-Gesture and setting new records on Long Range Arena tasks (e.g., $63.77\%$ ListOps, $91.8\%$ Retrieval, $87.2\%$ Text) as well as excelling in dynamical system prediction and real-world tasks like human activity recognition and genomics. Extensive ablation studies and theoretical analysis verify the importance of stable parameterization and the capacity for content-based reasoning. The paper concludes that S7 establishes a new standard for SSMs, offering both computational efficiency and adaptability, with broad applicability across neuromorphic, biological, and physical time-series domains, and suggests avenues for future research in expanding domain coverage, real-world scalability, and theory for input-dependent SSM training."
43,"A. Gu, K. Goel, and C. Ré, ""Efficiently Modeling Long Sequences with Structured State Spaces,"" arXiv preprint arXiv:2111.00396, vol. abs/2111.00396, 2022. [Online]. Available: https://arxiv.org/abs/2111.00396","The paper introduces the Structured State Space sequence model (S4), a principled neural architecture for sequence modeling that efficiently handles very long-range dependencies (LRDs) by building upon state space models (SSMs) and the HiPPO framework. S4 uses a novel parameterization of the SSM state matrix as a sum of a diagonalizable (normal) and low-rank component, enabling stable diagonalization and efficient computation, and ultimately reducing computations to a fast, memory-efficient Cauchy kernel problem. This design allows S4 to achieve theoretical advantages of SSMs while offering $\widetilde{O}(N+L)$ time and $O(N+L)$ memory for convolutional representations (with $N$ sequence length and $L$ state dimension), and $O(N)$ time for the recurrent case. Empirically, S4 sets new state-of-the-art on all tasks in the challenging Long-Range Arena benchmark (including uniquely solving the Path-X task with sequences of length 16k), achieves 91% accuracy on sequential CIFAR-10 with no augmentation (matching larger 2-D ResNets), reaches 98.3% accuracy in raw speech classification, and is competitive with or superior to Transformers and other domain-specialized architectures in both performance and speedup to $60\times$ faster in generative settings. Ablation studies validate the essential role of the HiPPO initialization in S4's performance. While acknowledging that S4 does not universally outperform all specialized models (e.g., a slight deficit to Transformers in language modeling), the authors argue for the versatility of S4 and propose future directions including hybrid architectures, broader applications to other modalities, and further hardware optimization."
44,"S. Al-Dabooni and D. Wunsch, ""The Boundedness Conditions for Model-Free HDP(λ),"" IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 7, pp. 1928-1942, July 2019. [Online]. Available: https://ieeexplore.ieee.org/document/8528554",This paper provides the stability analysis for a model-free action-dependent heuristic dynamic programming (HDP) approach with an eligibility ...
45,"Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen, Kay Chen Tan, ""A Survey on Evolutionary Neural Architecture Search,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 2, pp. 550-570, Feb. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/9863147","Deep neural networks (DNNs) have achieved great success in various applications, with their architectures playing a pivotal role in performance; yet, manual design of DNN architectures is labor-intensive and requires significant expertise. Neural architecture search (NAS) automates this design process, and among its approaches, evolutionary computation (EC) methods have recently gained significant attention and success. This article reviews over 200 recent EC-based NAS methods, systematically discussing their core components, design principles, and justifications. Additionally, it addresses current challenges and issues, proposing directions for future research in the field."
46,"Qi Liu and Wujie Wen, ""Model Compression Hardens Deep Neural Networks: A New Perspective to Prevent Adversarial Attacks,"" IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 1, pp. 3-14, Jan. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/9742065","This paper proposes the strategic use of hash compression to construct a low-cost defensive hash classifier that acts as the first layer of defense against adversarial attacks on deep neural networks. The method fundamentally strengthens models by crafting a more robust decision boundary and ensuring smaller gradients, which together impede the effectiveness of adversarial perturbations. Extensive experimental studies demonstrate that this approach significantly reduces the attack success rate across multiple adversarial attack types and datasets. The discussion underscores the critical importance of both the defensive decision boundary and the attenuation of gradient magnitudes for enhanced model protection. The authors conclude that model compression via hash compression offers a powerful and practical means to harden deep neural networks against adversarial threats."
47,"W. Ju, F. Zheng, Y. Gu, Z. Liu, Q. Long, Z. Qiao, Y. Qin, J. Shen, F. Sun, Z. Xiao, J. Yang, J. Yuan, Y. Zhao, Y. Wang, X. Luo, and M. Zhang, “A comprehensive survey on deep graph representation learning,” Neural Networks, vol. 173, pp. 1–36, Feb. 2024. [Online]. Available: https://doi.org/10.1016/j.neunet.2024.106207","Graph representation learning seeks to encode high-dimensional graph-structured data as compact vectors, enabling effective analysis across domains like social networks, biology, recommender systems, and traffic analysis. Classic shallow embedding methods focus on maintaining proximity between connected node vectors, but face challenges of limited capacity, lack of alignment with advanced learning paradigms, and insufficient coupling with downstream tasks. Deep graph representation learning, principally via Graph Neural Networks (GNNs), has emerged as a powerful paradigm, offering better scalability and expressiveness. The survey organizes deep graph learning into taxonomies based on major GNN architecturesspectral and spatial graph convolutions (e.g., GCN, GraphSAGE, GAT, ChebNet), graph kernel neural networks, pooling methods (gPool, SAGPool, DiffPool, MinCutPool), and graph transformers (using attention modifications, Laplacian/eigenvector encodings, and advanced bias mechanisms). Learning paradigms are categorized as supervised/semi-supervised, self-supervised (contrastive or generative, e.g., InfoNCE, autoencoders), and graph structure learning (adaptive adjacency, metric/model-based, or directly learnable with constraints). Table summarizing key method categories:

\[
\begin{tabular}{l|l|l}
\textbf{Architecture}      & \textbf{Paradigm}         & \textbf{Key Features} \\
\hline
GCN, GraphSAGE, GAT       & Supervised, Semi-supervised   & Spectral/spatial convolution, scalability \\
Graph Kernels             & Supervised, Unsupervised      & Isomorphism tests, domain-specificity     \\
Pooling (gPool, SAGPool)  & Any                           & Hierarchical/global summarization         \\
Transformer-based         & Any                           & Attention, expressiveness, scalability    \\
\end{tabular}
\]

Applications include social analytics, molecular property prediction/generation, recommendation (user-item, session-based), and traffic forecasting. The survey discusses theoretical and practical challenges: scalability, over-smoothing, computational cost, robustness, interpretability, fairness, and generalization to out-of-distribution and large-scale graphs. New research frontiers emphasize advancing fairness, robustness, adversarial adaptation, causal and interpretable inference, and theoretical groundwork (including spectral theory and manifold learning). For practitioners and researchers, the survey serves as both a methodological map and a guide to emerging application challenges and directions in deep graph representation learning."
48,"S. Tian, L. Li, W. Li, H. Ran, X. Ning, and P. Tiwari, “A survey on few-shot class-incremental learning,” Neural Networks, vol. 173, pp. 49–77, Jan. 2024. [Online]. Available: https://doi.org/10.1016/j.neunet.2023.10.039","This paper presents a comprehensive survey of Few-Shot Class-Incremental Learning (FSCIL), addressing the challenge where deep neural networks must learn new classes from very limited labeled samples incrementally, without forgetting previously learned informationa problem prone to catastrophic forgetting and overfitting. The authors define FSCIL as a process where models are initially trained on a base session with ample data, then must adapt to novel classes in later incremental sessions with few labeled samples, typically with little or no access to old-class data. The review synthesizes over 30 theoretical and 20 applied research studies, categorizing theoretical advances into five subfields: traditional machine learning, meta-learning-based, feature-space-based, replay-based, and dynamic network structure-based methods. Application domains covered include image classification, object detection, NLP, and graph tasks, with performance evaluated on benchmarks like CIFAR100, Mini-ImageNet, and CUB200 using metrics such as average incremental accuracy and forgetting rate. The discussion identifies main challenges such as catastrophic forgetting, overfitting, data and task design, and scalability, and highlights future directions including deeper theoretical analysis, broader applications across domains, and new experimental setups with sparser or non-i.i.d. data. The paper ultimately emphasizes the practical and theoretical importance of FSCIL in making deep learning models more adaptable and data-efficient."
49,"Z. Hu, K. K. Shukla, G. E. Karniadakis, and K. Kawaguchi, “Tackling the curse of dimensionality with physics-informed neural networks,” Neural Networks, vol. 175, pp. 155–176, May 2024. [Online]. Available: https://doi.org/10.1016/j.neunet.2024.106369","This paper addresses the long-standing curse of dimensionality (CoD) problem in numerically solving high-dimensional partial differential equations (PDEs), which makes conventional methods infeasible due to exponential growth in computational cost. The authors introduce Stochastic Dimension Gradient Descent (SDGD), a novel algorithm for scaling up Physics-Informed Neural Networks (PINNs) to arbitrary dimensions by decomposing the gradient of the PDE residual into dimension-specific components and stochastically sampling subsets at each training step. Theoretical analysis demonstrates SDGDs unbiasedness and convergence properties, and empirical results show PINNs with SDGD can solve complex, nonlinear PDEs with anisotropic and non-separable solutions, including HamiltonJacobiBellman and Schrodinger equations with up to 100,000 dimensions on a single GPU, far surpassing state-of-the-art methods. Critical implementation details, such as efficient memory use and gradient accumulation, as well as rigorous ablation studies and comparisons to contemporary alternatives (e.g., Hutchinson estimators and Score-PINN), are provided. While extremely high dimension may slow convergence due to gradient variance, SDGD is mesh-free, generally applicable, and marks a significant advance in tackling high-dimensional PDEs with deep learning, offering theoretical and practical improvements for the field."
50,"S. Kurogi, M. Toidani, R. Shigematsu, and K. Matsuo, ""Performance improvement via bagging in probabilistic prediction of chaotic time series using similarity of attractors and LOOCV predictable horizon,"" Neural Computing and Applications, vol. 29, no. 9, pp. 341–349, May 2018. doi: 10.1007/s00521-017-3149-7 URL: https://link.springer.com/article/10.1007/s00521-017-3149-7","This paper presents an improved approach for probabilistic prediction of chaotic time series by employing bagging (bootstrap aggregating) with competitive associative nets (CAN2) as strong learners. The method first generates predictions with CAN2 using iterated one-step-ahead (IOS) strategies and then selects plausible predictions based on the similarity of attractors between training and predicted sequences. Instead of relying on the ensemble mean, the representative predictionchosen for its longest estimated predictable horizonis identified using leave-one-out cross-validation (LOOCV). Numerical experiments on the Lorenz system ($dx_c/dt_c = \sigma(y_c - x_c)$, $dy_c/dt_c = -x_c z_c + r x_c - y_c$, $dz_c/dt_c = x_c y_c - b z_c$ with $\sigma=10$, $r=28$, $b=8/3$) demonstrate that bagging CAN2 achieves longer and more robust predictable horizons compared to single CAN2s. However, selection reliability is impacted by occasional negative correlations between LOOCV-estimated and actual predictable horizons, underlying a persistent challenge when selecting the optimal prediction. The authors suggest further refinement of the prediction selection method and improvement of learning machine performance to address these limitations."
51,"A. Karamchandani, A. Mozo, S. Gómez-Canaval, and A. Pastor, ""A methodological framework for optimizing the energy consumption of deep neural networks: a case study of a cyber threat detector,"" Neural Computing and Applications, vol. 36, pp. 10297–10338, Jun. 2024. doi: 10.1007/s00521-024-09588-z URL: https://link.springer.com/article/10.1007/s00521-024-09588-z","This paper introduces an open-source methodological framework designed to systematically evaluate and optimize the energy efficiency of deep neural network (DNN) systems in production, while maintaining high performance. The framework streamlines experimentation by automating model training, the application of multiple state-of-the-art optimization techniques (organized into configurable optimization strategies), exporting to production formats, and evaluating both energy consumption and inference performance across various batch sizes. Key innovations include optimization strategies for systematic assessment and optimization profiles that balance energy efficiency against performance preferences, enabling tailored selection of the optimal configuration for a given application. Validated through deployment on a DNN-based cyber threat detector, the framework achieves up to $82\%$ (\textit{specifically}, $82.3\%$) reduction in inference energy consumption with negligible accuracy loss, as confirmed by empirical results that analyze resource usage and trade-offs for different batch sizes and optimization profiles. The approach addresses challenges such as managing energy versus accuracy, automation of combinatorial technique selection, and applicability to evolving production scenarios (including edge and cloud deployments). Extensive appendices provide tabular analyses (CPU, memory, disk usage; e.g., $\begin{tabular}{lccc} \text{Batch size} & \text{CPU (W)} & \text{Mem (GB)} & \text{Disk (MB/s)} \\ \hline 1 & 1.3 & 0.5 & 2.0 \\ 8 & 1.6 & 0.7 & 2.1 \\ \end{tabular}$), and limitations and future work are discussed regarding support for different architectures, hardware accelerators, and alternative optimization search approaches. All code and data are openly available at GitHub (github.com/amitkbatra/EnergyNet), positioning the framework as a sustainable, extensible resource for the AI community to advance energy-efficient DNN deployment."
52,"B. Mesta, O. B. Akgun, and E. Kentel, ""Improving precipitation estimates for Turkey with multimodel ensemble: a comparison of nonlinear artificial neural network method with linear methods,"" Neural Computing and Applications, vol. 36, pp. 10219–10238, Jun. 2024. doi: 10.1007/s00521-024-09598-x URL: https://link.springer.com/article/10.1007/s00521-024-09598-x","This study evaluates the improvement in monthly precipitation estimates for south and southwestern Turkey achieved via ensemble analysis of regional climate models (RCMs) versus single-model outputs. Eight historical RCMs from the CORDEX EUR-11 domain were compared to ground observations using statistical metrics (correlation, RMSD, PBIAS) and Taylor diagrams. Ensemble methodologies compared include simple averaging (SAM), multiple linear regression superensemble (SE), artificial neural networks (ANN), and fuzzy inference systems (FIS). Results show ensembles outperform individual RCMs in terms of higher correlation and lower RMSD, especially when all modelsnot just the three bestare included. Nonlinear methods (ANN, FIS) further improve performance relative to linear ensembles, but ensemble series tend to diminish variability and under-represent precipitation extremes, potentially hindering their application in hydrologic design for rare events. Bias correction improves when log-transformation is omitted from preprocessing; however, challenges remain for representing extreme events and model selection, and single-RCM projections remain highly uncertain. The study recommends nonlinear ensembling for climate impact analyses but advocates inclusion of all single-model outputs when extremes are critical. While these findings are specific to Turkey's complex climate, future comparative research in other regions is advised to generalize results and refine ensemble strategies for extremes."
53,"Boris Hanin, ""Random Fully Connected Neural Networks as Perturbatively Solvable Hierarchies,"" Journal of Machine Learning Research, vol. 25, no. 267, pp. 1–58, 2024. [Online]. Available: https://www.jmlr.org/papers/v25/23-0643.html","We study the distribution of fully connected neural networks with Gaussian random weights and biases and $L$ hidden layers, each of width proportional to a large parameter $n$. For polynomially bounded nonlinearities, we give sharp estimates in powers of $1/n$ for the joint cumulants of the network output and its derivatives. We further show that network cumulants form a perturbatively solvable hierarchy in powers of $1/n$; specifically, the $k$-th order cumulants in each layer are determined to leading order in $1/n$ by cumulants of order at most $k$ computed at the previous layer. By explicitly deriving and solving several such recursions, we find that the depth-to-width ratio $L/n$ serves as an effective network depth, controlling both the distance to Gaussianity and the size of inter-neuron correlations."
54,"Sjoerd Dirksen, Martin Genzel, Laurent Jacques, and Alexander Stollenwerk, ""The Separation Capacity of Random Neural Networks,"" Journal of Machine Learning Research, vol. 23, no. 309, pp. 1–47, 2022. [Online]. Available: https://www.jmlr.org/papers/v23/21-1079.html","Neural networks with random weights, commonly used for initialization in deep learning and as a computationally efficient alternative to fully trained networks, are analyzed in this paper through the data separation problem: determining conditions under which a random neural network can render two classes $\mathcal{X}^-, \mathcal{X}^+ \subset \mathbb{R}^d$ (with positive distance) linearly separable. The authors show that a sufficiently large two-layer ReLU network with standard Gaussian weights and uniform biases can, with high probability, achieve linear separability between these sets. Crucially, the required number of neurons is explicitly tied to geometric properties and mutual arrangement of $\mathcal{X}^-$ and $\mathcal{X}^+$. Adopting an instance-specific view, the authors overcome the curse of dimensionalitytypically requiring exponentially wide layerswhen the data has low-complexity structure. They introduce a concept of mutual complexity based on a localized Gaussian mean width, facilitating informative, data-dependent separation guarantees and connecting these results to research on neural network approximation, memorization, and generalization."
55,"Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, and Pieter-Jan Kindermans, ""iNNvestigate Neural Networks!"" Journal of Machine Learning Research, vol. 20, no. 93, pp. 1–8, 2019. [Online]. Available: https://www.jmlr.org/papers/v20/18-540.html","iNNvestigate is an open-source Python library designed to facilitate the analysis and comparison of neural network explanations by providing a unified and modular interface for a broad range of state-of-the-art analysis techniques, such as Saliency Map, SmoothGrad, IntegratedGradients, Deconvnet, GuidedBackprop, PatternNet, PatternAttribution, DeepTaylor, and Layer-wise Relevance Propagation (LRP, including its variants like LRP-Z, LRP-Epsilon, and LRP-AlphaBeta). It supports Keras-based models and allows users to easily pass trained neural networks to analyzer objects for qualitative and quantitative investigations; the latter is aided by an implementation of perturbation analysis, also known as PixelFlipping, which quantitatively measures how perturbing identified important regions affects classification. The librarys modular design, built on base classes for layer-wise forward and backward computations, simplifies adding new methods and supports extensions such as combining analyzers with smoothing wrappers (e.g., creating SmoothGrad by augmenting gradient-based analyzers with noise). iNNvestigate includes the first reference implementations for PatternNet and PatternAttribution, and is demonstrated on numerous state-of-the-art architectures such as VGG16/19, InceptionV3, ResNet50, DenseNet, and NASNet. The software can efficiently handle large datasets (like ImageNet) and leverages GPU acceleration for training. Published under the MIT license and installable as a standard Python package (requiring TensorFlow as the backend), the library encourages community contributions for continual expansion as new interpretability methods emerge. By addressing the challenge of disparate implementations and easing systematic comparison, iNNvestigate is positioned to advance both the research and application of transparent neural networks in areas including autonomous driving, medical image analysis, and drug design."
56,"Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljačić, T. Y. Hou, and M. Tegmark, ""KAN: Kolmogorov-Arnold Networks,"" International Conference on Learning Representations (ICLR), arXiv preprint arXiv:2404.19756, 2025. [Online]. Available: https://arxiv.org/abs/2404.19756","Inspired by the Kolmogorov-Arnold representation theorem, the authors introduce Kolmogorov-Arnold Networks (KANs) as an alternative to Multi-Layer Perceptrons (MLPs), replacing fixed activation functions at nodes with learnable univariate functions (parameterized as B-splines) on edges and eliminating linear weights. Theoretically, KANs can approximate a wide class of multivariate functions maintain compositionality, benefiting from favorable scaling lawsempirically, KANs saturate an RMSE $\propto N^{-4}$ scaling on toy data and outperform MLPs in accuracy, parameter efficiency, and interpretability on function fitting, PDE solving, regression, and science tasks including the Feynman equation set. KANs avoid catastrophic forgetting in continual learning due to their local adaptation properties and facilitate symbolic regression: after automatic pruning and simplification, learned spline activations can be converted to interpretable formulas, letting users (re)discover known equations in mathematics and physics. Though current KAN implementations are slower to train due to architectural constraints, they provide a transparent, interactive modeling approach, making them promising foundational tools for scientific discovery, regression, and integration into modern ML frameworks. The paper provides theoretical guarantees, empirical benchmarks, use cases, and practical recipes for using KANs, highlighting their advantages and outlining avenues for future engineering and mathematical development."
57,"J. H. Tanis, C. Giannella, and A. V. Mariano, ""Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers,"" arXiv preprint arXiv:2412.19419, 2024. [Online]. Available: https://arxiv.org/abs/2412.19419","This survey provides a comprehensive introduction to graph neural networks (GNNs), framing them within the encoder-decoder paradigm and discussing how node and edge attributes in graphs can be effectively leveraged for predictive tasks where traditional machine learning methods fall short. The paper compares shallow embedding techniques to modern GNN architectures such as GCN, GraphSAGE, and GATv2, demonstrating through targeted experiments on 13 datasets that high-homophily graphs benefit from deeper, more rigid models (e.g., GCN), while low-homophily graphs require designs with fewer message-passing layers and robust pre/post feature processing due to issues with aggregating conflicting information and low signal-to-noise ratios (SNR). Experimental results, managed with PyTorch Geometric and GraphGym, reveal that optimal hyperparameters and architectural flexibility are highly dataset-dependent, and even advanced aggregation or skip connections cannot always offset the fundamental challenges posed by data with low homophily or SNR. The discussion emphasizes the persistent hurdles in adapting GNNs to challenging real-world data, and the conclusion notes the importance of the encoder-decoder framework in understanding and advancing the field. Future work should focus on strategies for improved robustness and inductive generalization, highlighting a continued need for research on architectures adept at handling both graph structure complexity and feature noise. Appendices offer practical guidance on open-source GNN libraries and granular benchmarking results."
58,"M. Yang and X.-J. Xu, ""Recent Advances in Hypergraph Neural Networks,"" arXiv preprint arXiv:2503.07959, 2025. [Online]. Available: https://arxiv.org/abs/2503.07959","This paper presents a comprehensive review of recent advances in hypergraph neural networks (HGNNs), motivated by the need to model higher-order relationships beyond simple pairwise connections found in standard graphs. HGNN architectures are systematically categorized into five primary types: Hypergraph Convolutional Networks (HGCNs  including spectral approaches, which are based on Laplacian eigen-decomposition and Chebyshev polynomials, and spatial approaches utilizing localized message passing), Hypergraph Attention Networks (HGATs, leveraging attention mechanisms to differentially weight neighbors), Hypergraph Autoencoders (HGAEs for unsupervised reconstruction tasks), Hypergraph Recurrent Networks (HGRNs for temporal/sequential data, typically built atop LSTM/GRU backbones), and Deep Hypergraph Generative Models (DHGGMs, such as VHGAEs for probabilistic modeling, HGGANs for adversarial learning, and HGGDMs employing diffusion principles). The paper highlights the mathematical underpinningssuch as incidence matrices, Laplacians, and the general HGNN pipeline of hypergraph construction, message passing, output prediction, and iterative refinement. Key challenges identified include the lack of unified hypergraph construction methods, limited scalability of models to large datasets, inadequate explainability, and difficulties with aggregation in heterogeneous hypergraphs. The authors conclude that while HGNNs have achieved strong results in fields like computer vision and bioinformatics, progress is needed in standardized hypergraph construction, scalable architectures, interpretable models, and robust handling of heterogeneous hypergraph data, anticipating that continued research will further expand the impact and applicability of HGNNs to complex real-world challenges."
59,"N. Mhatre and D. Cooley, “Transformed-Linear Models for Time Series Extremes,” Journal of Time Series Analysis, vol. 45, no. 5, pp. 671–690, Sep. 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12570","This paper introduces non-negative, regularly-varying time series models built using transformed-linear (copula-based) operations that mimic the classical ARMA framework while capturing extremal (upper tail) dependence crucial for modeling phenomena such as wind extremes relevant to wildfire risk. Central to the approach is the concept of weak tail stationarity and the use of the tail pairwise dependence function (TPDF) to quantify extremal dependencies; the authors rigorously show that these dependence structures are mathematically consistent across different dimensions and that the class of transformed-linear moving average processes forms an inner product space. Theoretical results prove the existence, stationarity, and regular variation of these processes, with closed-form or recursive formulas provided for the TPDFs of various model types. Fitting these models to hourly windspeed data from the Santa Ana region, the authors demonstrate that their methods substantially outperform classical ARMA models in estimating upper-tail behaviora key for extreme risk analysisas evidenced by empirical TPDF matches, diagnostic plots, and summary tables. The discussion emphasizes that transformed-linear time series bridge the gap between classical and extreme value theory, offering interpretable, tractable, and risk-relevant modeling tools for natural hazard extremes, while the appendices provide mathematical proofs and simulation details supporting the main results."
60,"Y. Lin and Q. Zhu, “On Vector Linear Double Autoregression,” Journal of Time Series Analysis, vol. 45, no. 3, pp. 376–397, May 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12551","This article introduces a vector linear double autoregressive (VLDAR) model with a constant conditional correlation structure designed to jointly model the co-movement, conditional means, and volatilities of multiple time series. The authors explore the strict stationarity conditions of the model and propose a self-weighted Gaussian quasi-maximum likelihood estimator (SQMLE), with a block coordinate descent (BCD) algorithm to enhance computational efficiency for high-dimensional data. Additional contributions include a Bayesian information criterion for selecting model order and a multivariate mixed portmanteau test for checking model adequacy. Notably, all asymptotic propertiespertaining to estimation, model selection, and diagnostic testingare proven without needing any moment conditions, broadening the applicability of the approach to heavy-tailed data. Performance is evaluated through simulation studies, and an empirical analysis of S&P 500 sector indices demonstrates the utility of the proposed model over existing alternatives."
61,"H.-F. Zhang, “Additive Autoregressive Models for Matrix Valued Time Series,” Journal of Time Series Analysis, vol. 45, no. 3, pp. 398–420, May 2024. [Online]. Available: https://onlinelibrary.wiley.com/doi/10.1111/jtsa.12552","In this article, additive autoregressive models (AddARM) are developed for time series data with matrix-valued predictors, with an emphasis on modeling separable row, column, and lag effects to enhance interpretability over existing bilinear matrix autoregressive models. The authors leverage Gershgorin's circle theorem to impose conditions ensuring strict stationarity of the underlying process. An alternating least squares method is introduced for solving equality-constrained optimization problems associated with parameter estimation, and the asymptotic distributions of the estimators are derived. Additionally, hypothesis tests are employed for parameter matrix diagnostics, and the utility of the proposed methods is demonstrated via simulations and real data analysis."
62,"Harrison Katz, Kai Thomas Brusch, and Robert E. Weiss, ""A Bayesian Dirichlet auto-regressive moving average model for forecasting lead times,"" International Journal of Forecasting, vol. 40, no. 4, pp. 1556-1567, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207024000049","This paper introduces the Bayesian Dirichlet Auto-Regressive Moving Average (B-DARMA) model as a new Bayesian framework for forecasting and analyzing compositional time series, where each observation is a vector of proportions (e.g., proportions of fees recognized in future monthly intervals in hospitality). Each days data is modeled as Dirichlet-distributed, with the mean vector evolving via a Vector Auto-Regressive Moving Average (VARMA) process that incorporates past compositions and covariates such as seasonality. Scale parameters are modeled with log-linear regression; prior choices (normal or horseshoe) offer flexibility and enable informative shrinkage. Simulation studies show that B-DARMA achieves equal or better root mean squared error and coverage than transformation-based or frequentist alternatives, exhibiting superior parameter recovery and forecastingespecially in high-dimensional settings ($J=3$, $T=540$, 400 replications per scenario). Analysis of five years of Airbnb booking data demonstrates effective forecasting of future fee allocations, with the Normal Full B-DAR(1) model balancing predictive power and parsimony. Model selection is performed via leave-future-out expected log pointwise predictive density, and the Bayesian formulation is implemented efficiently with Stan. Compared to frequentist methods, B-DARMA is robust to model fitting failures, interpretable, and computationally tractable. Limitations include the current inability to handle zeros in composition vectors; possible extensions cover hierarchical and zero-inflated models. The authors provide reproducible Stan code publicly, enabling adoption of B-DARMA for compositional forecasting across business and scientific applications."
63,"Gianluca Cubadda, Stefano Grassi, and Barbara Guardabascio, ""The time-varying Multivariate Autoregressive Index model,"" International Journal of Forecasting, vol. 41, no. 1, pp. 175-190, 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0169207024000384","Many economic variables exhibit changes in both their conditional mean and volatility, prompting the use of time-varying Vector Autoregressive Models to manage this complexity; however, such models face estimation and interpretation challenges as the number of time series increases. This paper presents a Multivariate Autoregressive Index model featuring time-varying mean and volatility, introducing a novel estimation methodology that combines switching algorithms with the forgetting factors approach of Koop and Korobilis (2012). This methodology significantly lowers computational demands and enables real-time selection or weighting of the number of common components and other data characteristics without incurring extra computational costs. The approach is validated through a forecasting application using US macroeconomic data, demonstrating the practicality and advantages of the proposed model."
64,"Francisco Cribari-Neto and Vinícius F. de Almeida, ""Beta autoregressive moving average model selection with application to modeling and forecasting stored hydroelectric energy,"" International Journal of Forecasting, vol. 39, no. 1, pp. 98-109, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/abs/pii/S0169207021001485","We build upon the class of beta regressions introduced by Ferrari and Cribari-Neto (J. Appl. Stat. 31:799815, 2004) to propose a dynamic model for continuous random variates that assume values in the standard unit interval (0,1). The proposed ARMA model includes both autoregressive and moving average dynamics, and also includes a set of regressors. We discuss parameter estimation, hypothesis testing, goodness-of-fit assessment and forecasting. In particular, we give closed-form expressions for the score function and for Fishers information matrix. An application that uses real data is presented and discussed. (SpringerLink, 2009)"
65,"C. Hill, A. A. Nielsen, and A. K. Sahu, ""The tensor auto‐regressive model,"" Journal of Forecasting, vol. 40, no. 4, pp. 636–652, 2021. [Online]. Available: https://doi.org/10.1002/for.2735","We introduce the tensor auto-regressive (TAR) model for time series data, emphasizing its robustness to model misspecification, seasonality, and nonlinear trends. The parameter estimation leverages the $t$-product for modeling a three-dimensional block of parameters, while the fast Fourier transform is used to enable efficient, parallelizable computation. Extensive validation through simulations and empirical applications demonstrates the TAR models performance in handling seasonal and geometric trends, analyzing misspecification, and employing bootstrapping to compute standard errors; model selection and benchmarking confirm that TAR is robust, computationally efficient, and compares favorably to existing forecasting methods."
66,"C. Berninger, M. Stips, and C. Dewes, ""A Bayesian time‐varying autoregressive model for improved short‐term and long‐term prediction,"" Journal of Forecasting, vol. 41, no. 1, pp. 181–200, 2022. [Online]. Available: https://doi.org/10.1002/for.2802","Motivated by applications to German interest rates, the authors propose a time-varying autoregressive model designed for time series exhibiting temporary nonstationary behavior but assumed to mean revert in the long run. They adopt a Bayesian framework to encode prior mean-reverting assumptions, thus regularizing far-future predictions. MCMC inference is conducted using derived full conditional distributions and a Metropolis-Hastings within Gibbs sampler to draw from the posterior predictive distribution. The approach achieves competitive short-term forecasts and sensible long-term projections by blending data-driven short-term estimates with long-term distributional assumptions. Empirical evaluation using interest rate data compares this model's forecasts to both a 2-Additive-Factor Gaussian model and the dynamic Nelson-Siegel model, demonstrating its efficacy."
67,"A. Taiebnia and S. Mohammadi, ""Forecast accuracy of the linear and nonlinear autoregressive models in macroeconomic modeling,"" Journal of Forecasting, vol. 42, no. 8, pp. 2045–2062, 2023. [Online]. Available: https://doi.org/10.1002/for.3002","Most nonlinear vector autoregressive methods in the econometric literature are based on specific functional forms, such as the smooth transition autoregressive model. This study proposes a general form of the nonlinear vector autoregressive model based on global approximators, such as neural networks, Volterra, and Weiner series. The simulation results of 20 linear and nonlinear multivariate time series processes indicate that nonlinear vector autoregressive methods, especially multioutput neural networks, are more accurate based on the root mean square error and model confidence set criteria. Applying the global approximator approach to a smallscale macroeconometric model reveals that the new approach can improve forecast accuracy compared to linear and other nonlinear vector error correction models. In addition, forecasting the relevant variables in a typical exchange rate and monetary policy models based on nonlinear specifications gives more successful results than in the linear case."
68,"J. Gibson, ""Entropy Power, Autoregressive Models, and Mutual Information,"" Entropy, vol. 20, no. 10, p. 750, 2018. [Online]. Available: https://www.mdpi.com/1099-4300/20/10/750","This paper introduces the log ratio of entropy powers as a novel quantity to analyze changes in differential entropy and mutual information when increasing the predictor order in autoregressive (AR) models, a concept applicable to fields such as speech, seismic, and biological signal processing. The log ratio, defined as $\log\left(\frac{N_2}{N_1}\right)$ where $N_1$ and $N_2$ are the entropy powers of two processes, equates to the difference in their differential entropies. For many AR processesespecially when Gaussian or Laplacian distributedthe entropy power can be substituted with the minimum mean squared prediction error (MSPE), simplifying calculations and making it pragmatic for real-world signal coding or model selection. Experimental results on speech frames show how mutual information increases, and bits/sample gains can be explicitly computed as AR model order increases, complementing traditional MSPE approaches with information-theoretic insights. The method enhances understanding of coding gain, helps in optimal model order selection, and has been applied or proposed for applications including speech waveform coding, CELP analysis, ECG and EEG classification, and geophysical exploration. The framework enables decomposing the mutual information in codecs and provides a theoretically grounded but practically implementable metric for AR model analysis, though challenges remain for non-Gaussian cases where the MSPE substitution is only approximate. The author also highlights future work exploring these metrics in a broader range of experimental settings."
69,"Lizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang, Junjie Gao, Yixuan Zhang, Wanxiang Che, Timothy Baldwin, Xudong Han, and Haonan Li, ""Against The Achilles' Heel: A Survey on Red Teaming for Generative Models,"" Journal of Artificial Intelligence Research, vol. 82, pp. 1–63, 2025. [Online]. Available: https://www.jair.org/index.php/jair/article/view/17654","Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safe use as various vulnerabilities are exposed. In light of this, the field of red teaming is undergoing fast-paced growth, highlighting the need for a comprehensive survey covering the entire pipeline and addressing emerging topics. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework to unify various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around LLM-based agents, overkill of harmless queries, and the balance between harmlessness and helpfulness."
70,"Giorgio Franceschelli and Mirco Musolesi, ""Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges,"" Journal of Artificial Intelligence Research, vol. 79, pp. 1351–1412, 2024. [Online]. Available: https://jair.org/index.php/jair/article/view/15278","This paper surveys the intersection of Generative Artificial Intelligence (AI) and Reinforcement Learning (RL), highlighting how RL provides a flexible framework for improving generative models beyond traditional differentiable objectives. It outlines three primary applications: (1) using RL as a direct generative approach in domains where loss functions are non-differentiable (e.g., sequential tasks like text or painting), (2) combining RL with generative models to maximize quantifiable objectives, such as BLEU or ROUGE scores in text generation or chemical properties in molecule design, and (3) applying RL, especially Reward Modeling and RL from Human Feedback (RLHF), to imbue generative models with characteristics that are difficult to quantify algorithmically, such as alignment with human values or preferences. The survey discusses technical underpinnings, like policy/value function learning, exploration versus exploitation trade-offs, and integration with techniques including inverse RL and hierarchical RL. While RL broadens the adoption of generative modeling by allowing the use of non-differentiable or user-defined objectives, it introduces challenges, notably the risk of reward hacking (Goodharts Law), biases in human feedback, difficulties with large action spaces, reliance on pre-trained models, and potential vulnerabilities (e.g., model jailbreaks). The authors conclude that despite substantial advancesespecially in aligning large-scale language models with human values using RLHFrobustness, bias mitigation, and integration of advanced RL techniques remain open research frontiers in this rapidly evolving field."
71,"Genta Indra Winata, Hanyang Zhao, Anirban Das, Wenpin Tang, David D. Yao, Shi-Xiong Zhang, and Sambit Sahu, ""Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey,"" Journal of Artificial Intelligence Research, vol. 82, pp. 522–601, 2025. [Online]. Available: https://www.jair.org/index.php/jair/article/view/17541","Preference tuning is essential for aligning deep generative models with human preferences, and this survey provides a comprehensive review of contemporary advancements in the field, particularly focusing on preference tuning and human feedback integration. The paper is structured into three segments: first, it introduces reinforcement learning frameworks, preference tuning tasks, models, datasets, and policy approaches across modalities such as language, speech, and vision; second, it offers a detailed analysis of various preference tuning methods; and third, it discusses application domains, presents evaluation strategies for different modalities, and envisions future research paths. The work aims to update researchers and practitioners on the latest techniques for model alignment and preference tuning, fostering deeper understanding and encouraging continued innovation, with supplementary resources made available at https://github.com/hanyang1999/Preference-Tuning-with-Human-Feedback."
72,"S. Bond-Taylor, A. Leach, Y. Long, C. G. Willcocks, T. Al-Maadeed, N. Moutari, T. Breckon, ""Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 11, pp. 12764-12793, Nov. 2023. Available: https://ieeexplore.ieee.org/document/9555209","Deep generative models utilize deep neural networks to learn the distribution of training data, and have diverged into several interconnected families, each offering different trade-offs such as efficiency, diversity in generation, and architectural constraints. This paper surveys energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, and normalizing flows, as well as hybrid forms, detailing the underlying principles of each, their interconnections, and highlighting the latest improvements and implementations in the field."
73,"F. Hong, J. Zhang, L. Chen, S. Yang, Z. Xu, ""DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 5, pp. 2997-3012, May 2024. Available: https://ieeexplore.ieee.org/document/10345691","This paper presents DaGAN++, a novel talking head generation framework that advances prior methods by incorporating self-supervised learning of dense 3D facial geometry (depth) from videos without reliance on 3D annotations or camera parameter input. The model introduces an uncertainty-guided depth estimation network, geometry-guided facial keypoint detector, and a multi-layer cross-modal attention mechanism that fuses appearance and depth features to guide generation at every stage. This pipeline, optimized by perceptual, equivariance, and diversity losses, enables highly accurate face reenactment that preserves source identity while transferring expressions and head movement from a driving video. Evaluated on challenging benchmarks (VoxCeleb1, VoxCeleb2, HDTF) using SSIM, PSNR, LPIPS, and the Temporal Consistency Metric (TCM), DaGAN++ achieves state-of-the-art image quality, temporal coherence, and generalization to out-of-domain examples such as paintings and cartoons. Ablation studies confirm the benefits of depth guidance, uncertainty weighting, and multi-stage geometry fusion. The paper concludes that leveraging dense 3D geometric cues is key for high-fidelity talking head synthesis and outlines future work toward handling more dynamic backgrounds and expanding generalization."
74,"J. Pan, J. Dong, Y. Liu, X. Zhang, M. Wang, Y. Tai, Y. Tang, K. Lu, Y. Zuo, L. Zhang, ""Physics-Based Generative Adversarial Models for Image Restoration and Beyond,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 7, pp. 2449-2462, Jul. 2021. Available: https://ieeexplore.ieee.org/document/8968618","The paper introduces an algorithm to address highly ill-posed image restoration problemssuch as deblurring, dehazing, and derainingby integrating physics-based constraints into the generative adversarial network (GAN) framework. Traditional methods for these tasks rely on heuristic priors, but the authors propose that combining GANs with explicit consistency checks under the image degradation physics yields superior results. Their model utilizes two discriminative networks: one to ensure that generated images approximate the true data distribution, and a second to verify the compatibility of generated outputs with observed degraded images via the physical degradation process. This system is trained end-to-end and, as validated by extensive experiments, outperforms several state-of-the-art algorithms in qualitative benchmarks for various restoration tasks."
75,"Yinchuan Li, Xinyu Shao, Jianping Zhang, Haozhi Wang, Leo Maxime Brunswic, Kaiwen Zhou, Jiqian Dong, Kaiyang Guo, Xiu Li, Zhitang Chen, Jun Wang, and Jianye Hao, ""Generative Models in Decision Making: A Survey,"" arXiv preprint arXiv:2502.17100, 2025. [Online]. Available: https://arxiv.org/abs/2502.17100","In recent years, generative models have garnered significant attention for their role in decision-making due to their capacity to model complex data distributions and produce trajectories that guide agents toward high-reward states or intermediate sub-goals. This paper provides a comprehensive review of how generative models are applied in decision-making tasks, categorizing seven core types: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. These models are discussed in the context of three functional rolescontrollers, modelers, and optimizersand their contributions to decision-making processes are analyzed across five key real-world scenarios. The review also highlights the advantages and limitations of current approaches and proposes three directions for future research in generative directive models: the development of high-performance algorithms, the creation of large-scale generalized decision-making models, and the pursuit of self-evolving and adaptive systems."
76,"Peng Sun, Yi Jiang, and Tao Lin, ""Unified Continuous Generative Models,"" arXiv preprint arXiv:2505.07447, 2025. [Online]. Available: https://arxiv.org/abs/2505.07447","This paper introduces UCGM, a unified framework for training and sampling in continuous generative models (including diffusion, flow-matching, and consistency paradigms), addressing fragmentation in current approaches by proposing the Unified Continuous Generative Models Trainer (UCGM-T) and Sampler (UCGM-S). UCGM-T employs a novel training objective parameterized by a consistency ratio $\lambda \in [0,1]$, seamlessly interpolating between multi-step and few-step inference regimes and accommodating diverse architectures, noise schedules, and VAEs. UCGM-S is a flexible sampling algorithm that accelerates inference and improves FID for both newly trained and pre-existing models. Empirically, UCGM achieves state-of-the-art (SOTA) results on ImageNet 256$\times$256 (1.30 FID in 20 steps self-trained, and optimized to 1.06 FID in 40 steps using prior models at no extra cost) and rivals or surpasses SOTA in both few-step and multi-step settings across various datasets and architectures, as confirmed by comprehensive ablations on parameters $\lambda$, $\zeta$, and $\kappa$. Theoretical and experimental analysis shows the unified approach bridges multi- and few-step paradigms, with enhanced target scoring and self-boosting decoupling performance from classifier-free guidance. The paper also addresses numerical instabilities as $\lambda \to 1$ with second-order difference methods and clipping, though exploration of more advanced training acceleration (e.g., REPA) and downstream tasks is left for future work. Extensive appendices cover convergence proofs and error bounds, and open-source code is available at https://github.com/LINs-lab/UCGM."
77,"Maria Nareklishvili, Nick Polson, and Vadim Sokolov, ""Generative Modeling: A Review,"" arXiv preprint arXiv:2501.05458, 2025. [Online]. Available: https://arxiv.org/abs/2501.05458","This paper reviews generative models that enable simulation-based Bayesian inference by learning mappings from observed data to latent parameters through deep neural networks, effectively reframing posterior estimation as a supervised learning task and circumventing traditional sampling techniques like Markov Chain Monte Carlo. It introduces and compares methods such as Approximate Bayesian Computation, Variational Autoencoders, Independent Component Analysis, Normalizing Flows, Generative Adversarial Networks, diffusion models, and deep fiducial inference, noting their particular strengths for high-dimensional and likelihood-intractable settings. The Generative Bayesian Computation (GBC) framework formalizes inference as learning deterministic (often quantile-based or normalizing flow-based) mappings from data and auxiliary noise to parameter samples. An empirical demonstration applies these ideas to an agent-based stochastic Ebola epidemic model, using a quantile neural network to reconstruct predictive distributions over epidemic trajectories; holdout tests show these methods closely recover empirical quantiles. The discussion highlights advantages in flexibility, scalability, and computational efficiency, while acknowledging ongoing challenges in architecture design, summary statistic selection, and uncertainty quantification. Future work aims to integrate utility-aware training, improve generalization and robustness for limited simulations, deepen theoretical understanding, and further expand the applicability of generative inference to diverse fields where Bayesian modeling of complex systems is critical."
78,"N. Ahad, M. A. Davenport, and Y. Xie, ""Data-adaptive symmetric CUSUM for sequential change detection,"" Sequential Analysis, vol. 43, no. 1, pp. 1–27, 2024. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/07474946.2023.2272908","Detecting change points sequentially in streaming settings where both the mean and variance can change presents challenges, primarily in setting suitable detection thresholds, especially as signals may switch between multiple distributions with either increases or decreases in mean or variance. Traditional log-likelihood ratio-based methods like CUSUM and GLR are not symmetric when both parameters change, making it hard to use a single detection threshold. The authors propose Data-Adaptive Symmetric CUSUM (DAS-CUSUM), a modification of CUSUM that enables symmetric detection for changes between distributions, allowing sequential detection with a fixed threshold. They derive theoretical results for expected detection delay and average run length, supported by simulations and real-world data experiments, showing DAS-CUSUM outperforms CUSUM and GLR for this application."
79,"H. Wang and Y. Tian, ""Sequential Point Clouds: A Survey,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 8, pp. 9276-9297, Aug. 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10436356","Point cloud data has gained significant attention due to its application in areas like autonomous driving and robotic manipulation, where sequential (four-dimensional) point clouds capture dynamic scenes beyond static data capabilities. This paper provides an extensive review of deep learning-based approaches for sequential point cloud tasks, including dynamic flow estimation, object detection and tracking, segmentation, and forecasting. It summarizes and compares quantitative results across public benchmark datasets, discusses current research challenges in sequential point clouds, and highlights promising directions for future work."
80,"F. J. Király and H. Oberhauser, ""Kernels for Sequentially Ordered Data,"" Journal of Machine Learning Research, vol. 20, no. 31, pp. 1-45, 2019. [Online]. Available: https://jmlr.org/papers/volume20/16-314/16-314.pdf","We present a novel framework for learning with sequential data of any kind, such as multivariate time series, strings, or sequences of graphs. The main result is a 'sequentialization' that transforms any kernel on a given domain into a kernel for sequences in that domain. This procedure preserves properties such as positive definiteness, the associated kernel feature map is an ordered variant of sample (cross-)moments, and this sequentialized kernel is consistent in the sense that it converges to a kernel for paths if sequences converge to paths (by discretization). Further, classical kernels for sequences arise as special cases of this method. We use dynamic programming and low-rank techniques for tensors to provide efficient algorithms to compute this sequentialized kernel."
81,"C. Shi and Y. Chen, ""An Online Sequential Test for Qualitative Treatment Effects,"" Journal of Machine Learning Research, vol. 22, no. 75, pp. 1-43, 2021. [Online]. Available: https://www.jmlr.org/papers/volume22/21-0383/21-0383.pdf","This paper introduces a general framework for sequentially testing qualitative treatment effects in online studies, leveraging a recursive implementation of the intersection-union test (IUT). The method controls type I error at any stopping time and is robust to complex study designs, including covariate-adaptive randomization, time-varying covariates, and non-i.i.d. outcomes. At each sequential time point, the test updates and computes p-values for constituent null hypotheses (e.g., for one-sided tests assessing superiority or safety), and globally rejects the null if all component nulls are rejected, thus applying the IUT principle. Theoretical justifications include finite-sample error bounds and exact significance calculations, and simulations confirm the method's validity. Application to mobile health trials using reinforcement learning demonstrates strong detection performance for adaptive, personalized interventions. The framework addresses technical challenges of adaptation, dependence, and computational feasibility in modern online experimentation, and lays a foundation for further advancements in hierarchical testing, contextual bandits, and dynamic treatment regimes."
82,"Yuxin He, Ping Huang, Weihang Hong, Qin Luo, Lishuai Li, and Kwok-Leung Tsui, ""In-Depth Insights into the Application of Recurrent Neural Networks (RNNs) in Traffic Prediction: A Comprehensive Review,"" Algorithms, vol. 17, no. 9, p. 398, 2024. [Online]. Available: https://doi.org/10.3390/a17090398","Traffic prediction plays a crucial role in transportation management and user convenience, with deep learningespecially Recurrent Neural Networks (RNNs)being widely adopted for this purpose. This paper provides a comprehensive review of RNN-based models in traffic prediction, tracing the evolution of relevant methods and summarizing contemporary techniques. It examines the unique features of traffic data, typical input representations, and formalizes the abstract structure of traffic prediction problems. The review organizes models by RNN architecture for traffic prediction and details seven main types of deep learning applications in the area. RNNs are compared with other leading methods, highlighting both their strengths and the challenges they face, and proposals are made to address these issues. The review offers valuable insights and strategies for researchers aiming to design improved traffic prediction models."
83,"Mehak Usmani, Zulfiqar Ali Memon, Adil Zulfiqar, and Rizwan Qureshi, ""Preptimize: Automation of Time Series Data Preprocessing and Forecasting,"" Algorithms, vol. 17, no. 8, p. 332, 2024. [Online]. Available: https://doi.org/10.3390/a17080332","Time series analysis is essential for informed business and financial decisions, particularly with the proliferation of IoT data, yet faces significant challenges including missing values, heteroscedasticity, seasonality, outliers, and noiseeach requiring tailored preprocessing strategies depending on whether the data is univariate, multivariate, Gaussian, or non-Gaussian, as well as stationary or non-stationary. The proposed automated system, Preptimize, combines statistical and machine learning techniques to recommend optimized prediction model blueprints, making forecasting workflows accessible even for non-experts by sampling from large datasets and suggesting the most appropriate preprocessing and modeling strategies. Experimental evaluations on diverse datasets, including stock prices, cryptocurrencies, and power consumption, showed Preptimize delivers prediction models that are comparable to or outperform standard benchmarks, affirming its broad utility for automating time series forecasting across various domains."
84,"Christie I. Ezeife and Hemmi Karlapalepu, ""A Survey of Sequential Pattern Based E-Commerce Recommendation Systems,"" Algorithms, vol. 16, no. 10, p. 467, 2023. [Online]. Available: https://doi.org/10.3390/a16100467","E-commerce recommendation systems must handle extensive customer sequential datasets such as historical purchases or click streams, and their accuracy can be enhanced by learning complex sequential user behavior patterns. By integrating these sequential patterns into the useritem rating matrix used for collaborative filtering, recommendation systems benefit from improved accuracy, reduced sparsity in useritem ratings, greater recommendation novelty, and better scalability. This review presents a thorough comparative analysis of current sequential pattern-based recommendation algorithms, outlining their methodologies, achievements, limitations, and future potential to address key challenges in the field."
85,"A. Klenitskiy, A. Volodkevich, A. Pembek, and A. Vasilev, ""Does It Look Sequential? An Analysis of Datasets for Evaluation of Sequential Recommendations,"" arXiv preprint arXiv:2408.12008, Aug. 2024. [Online]. Available: https://arxiv.org/abs/2408.12008","This paper investigates the adequacy of widely-used datasets for evaluating Sequential Recommender Systems (SRSs) by analyzing their inherent sequential structure. The authors propose and thoroughly apply three assessment approaches: (1) a model-agnostic sequential rule mining technique that compares the number of sequential association rules before and after shuffling user interaction sequences, (2) model-based analysis by measuring performance drops ($\Delta$NDCG@10, $\Delta$HitRate@10) with SRSs like SASRec and GRU4Rec when test sequences are randomly shuffled, and (3) a Rank List Sensitivity method using top-K Jaccard scores to quantify similarity in recommendations before and after shuffling. Across 15 datasets frequently used in sequential recommendation benchmarks, results show that manysuch as Foursquare, Gowalla, RetailRocket, Steam, and Yelpexhibit weak or negligible sequential structure, calling into question their appropriateness for SRS evaluation. Datasets like ML-20m, 30Music, MegaMarket, and Zvuk display strong sequential patterns, with relative decreases of over $-90\%$ in rule counts and significant drops in recommendation metrics upon shuffling. Consistency between approaches was observed, with a Spearman correlation of approximately 0.9 in performance drops between SASRec and GRU4Rec, though rule-based and model-based assessments may occasionally diverge, especially on data with short histories or few sequential patterns. Preprocessing choices like filtering and deduplication significantly affect the apparent sequential structure. The study concludes that careful dataset selectionensuring alignment between data properties and evaluation objectivesis crucial, and urges further research into standardized criteria for SRS dataset suitability and into the real-world implications of benchmarking on strongly sequential vs. weakly sequential datasets."
86,"J. Wang and B. Boukai, ""Bayesian sequential analysis of adverse events with binary data,"" arXiv preprint arXiv:2504.02959, Apr. 2025. [Online]. Available: https://arxiv.org/abs/2504.02959","This paper introduces Bayesian sequential procedures for testing hypotheses about the relative risk ($\gamma = \theta_A / \theta_B$) between two treatments in two-arm clinical trials with binary outcomes. Building on Wang (2024) and the Stopping Rule Principle, the authors derive both standard and modified Bayesian tests employing Beta priors, calculating posterior probabilities and Bayes factors to generate decision rules. The modified approach, following Berger and Sellke (1997), includes a 'no-decision' region for ambiguous evidence and enables straightforward computation of Type I and II error probabilities, both marginal and conditional. Using H1N1 adverse event data from Silva et al. (2020) as an illustrative case, they compare decisions across several priors (uniform, informative, Jeffrey's), reporting detailed results in tabular form for each patient group, including Bayes factors, decisions, and HPD intervals for $\gamma$. Their results show that Bayesian sequential monitoring may reach termination earlier or later compared to classical frequentist methods, with posterior-based error rates providing greater interpretive value. The advantages include no penalty for optional stopping, transparent error assessment, and practical computation, especially in real-time or interim analyses of clinical trials. Challenges such as prior selection and interpreting borderline Bayes factors are discussed, with future work aimed at broadening the method to more arms and outcome types. The connection with Uniformly Most Powerful Bayesian Tests (UMPBT) is delineated, offering a bridge to frequentist interpretations. Extensive appendices provide full computational details for all priors and testing scenarios."
87,"D. Botache, K. Dingel, R. Huhnstock, A. Ehresmann, and B. Sick, ""Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis,"" arXiv preprint arXiv:2307.14294, Jul. 2023. [Online]. Available: https://arxiv.org/abs/2307.14294","Splitting sequential data, such as videos and time series, is an essential step in various data analysis tasks like object tracking and anomaly detection, but presents challenges affecting accuracy and reliability. This concept article discusses issues including data acquisition, representation, split ratio selection, establishing quality criteria, and choosing appropriate selection strategies, illustrating these points through practical examples in motor test benches and particle tracking in liquids."
